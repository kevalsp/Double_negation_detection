{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "zQSv1ZuqhBWl",
    "outputId": "4a78937e-b655-4d69-db1a-6bcbc8d6871a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OtXsoXN9iP6G",
    "outputId": "81755682-bbd4-461a-b5fa-2aa98d2f4bbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/pytorch-pretrained-BERT_k\n"
     ]
    }
   ],
   "source": [
    "cd pytorch-pretrained-BERT_k/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ftuA3AzIiT-g",
    "outputId": "60198fb3-24dc-4797-9b14-d5693960bf89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 591.8MB 31kB/s \n",
      "\u001b[31mERROR: torchvision 0.4.2 has requirement torch==1.3.1, but you'll have torch 1.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q torch==1.0.0 torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "5cLUBuyXvgg8",
    "outputId": "ca7b8740-f90c-4643-eac9-39c104564ba4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting regex\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
      "\r",
      "\u001b[K     |▌                               | 10kB 20.7MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 20kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 30kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 40kB 1.6MB/s eta 0:00:01\r",
      "\u001b[K     |██▌                             | 51kB 2.0MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 61kB 2.4MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 71kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 81kB 3.1MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 92kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 102kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 112kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 122kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 133kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████▏                        | 143kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 153kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 163kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 174kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 184kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 194kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 204kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▊                     | 215kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 225kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 235kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▏                   | 245kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▊                   | 256kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 266kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 276kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▎                 | 286kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▊                 | 296kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 307kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 317kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▎               | 327kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▉               | 337kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 348kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 358kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 368kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 378kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 389kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 399kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 409kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 419kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▍          | 430kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 440kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 450kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 460kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 471kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 481kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 491kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 501kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 512kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 522kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 532kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 542kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 552kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 563kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 573kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 583kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 593kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 604kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 614kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 624kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 634kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 645kB 2.7MB/s \n",
      "\u001b[?25hInstalling collected packages: regex\n",
      "Successfully installed regex-2019.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "cca9y-9_iil2",
    "outputId": "322adcba-04b3-488c-9ed8-646533ece308"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch-pretrained-BERT'...\n",
      "remote: Enumerating objects: 13, done.\u001b[K\n",
      "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
      "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
      "remote: Total 4819 (delta 2), reused 4 (delta 0), pack-reused 4806\u001b[K\n",
      "Receiving objects: 100% (4819/4819), 2.66 MiB | 497.00 KiB/s, done.\n",
      "Resolving deltas: 100% (3401/3401), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/pytorch-pretrained-BERT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "RHDSWQnTkNSi",
    "outputId": "8ce6875c-5ab2-4b1f-d3dc-df8d7ee620f1"
   },
   "outputs": [],
   "source": [
    "#upload datasets from local system\n",
    "from google.colab import files \n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "jJhISvrdqT-3",
    "outputId": "120d32c5-d9be-47de-d44b-e00de7402dcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/5c/e918d9f190baab8d55bad52840d8091dd5114cc99f03eaa6d72d404503cc/tensorboardX-1.9-py2.py3-none-any.whl (190kB)\n",
      "\r",
      "\u001b[K     |█▊                              | 10kB 24.0MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 20kB 1.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 30kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████▉                         | 40kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 51kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 61kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 71kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 81kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▌                | 92kB 3.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 102kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 112kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 122kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 133kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 143kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 153kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 163kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 174kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 184kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 194kB 2.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.17.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (41.6.0)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-1.9\n"
     ]
    }
   ],
   "source": [
    "pip install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32cYlfzEV0kE"
   },
   "source": [
    "# ATTEMPT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "sV9UxYWwvBZf",
    "outputId": "ab020fbd-e707-4999-e225-41b6f39b77fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/01/2019 21:14:42 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "12/01/2019 21:14:43 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "12/01/2019 21:14:45 - INFO - pytorch_pretrained_bert.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_pretrained_bert/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "12/01/2019 21:14:45 - INFO - pytorch_pretrained_bert.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_pretrained_bert/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "12/01/2019 21:14:45 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "12/01/2019 21:14:49 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "12/01/2019 21:14:49 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "12/01/2019 21:14:52 - INFO - __main__ -   ***** Running training *****\n",
      "12/01/2019 21:14:52 - INFO - __main__ -     Num examples = 3264\n",
      "12/01/2019 21:14:52 - INFO - __main__ -     Batch size = 32\n",
      "12/01/2019 21:14:52 - INFO - __main__ -     Num steps = 306\n",
      "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
      "Iteration:   0% 0/102 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1% 1/102 [00:01<02:43,  1.62s/it]\u001b[A\n",
      "Iteration:   2% 2/102 [00:03<02:39,  1.60s/it]\u001b[A\n",
      "Iteration:   3% 3/102 [00:04<02:36,  1.59s/it]\u001b[A\n",
      "Iteration:   4% 4/102 [00:06<02:33,  1.57s/it]\u001b[A\n",
      "Iteration:   5% 5/102 [00:07<02:31,  1.56s/it]\u001b[A\n",
      "Iteration:   6% 6/102 [00:09<02:29,  1.55s/it]\u001b[A\n",
      "Iteration:   7% 7/102 [00:10<02:27,  1.55s/it]\u001b[A\n",
      "Iteration:   8% 8/102 [00:12<02:25,  1.55s/it]\u001b[A\n",
      "Iteration:   9% 9/102 [00:13<02:24,  1.55s/it]\u001b[A\n",
      "Iteration:  10% 10/102 [00:15<02:22,  1.55s/it]\u001b[A\n",
      "Iteration:  11% 11/102 [00:17<02:21,  1.56s/it]\u001b[A\n",
      "Iteration:  12% 12/102 [00:18<02:20,  1.56s/it]\u001b[A\n",
      "Iteration:  13% 13/102 [00:20<02:18,  1.56s/it]\u001b[A\n",
      "Iteration:  14% 14/102 [00:21<02:17,  1.56s/it]\u001b[A\n",
      "Iteration:  15% 15/102 [00:23<02:15,  1.56s/it]\u001b[A\n",
      "Iteration:  16% 16/102 [00:24<02:13,  1.56s/it]\u001b[A\n",
      "Iteration:  17% 17/102 [00:26<02:12,  1.55s/it]\u001b[A\n",
      "Iteration:  18% 18/102 [00:27<02:10,  1.56s/it]\u001b[A\n",
      "Iteration:  19% 19/102 [00:29<02:09,  1.56s/it]\u001b[A\n",
      "Iteration:  20% 20/102 [00:31<02:08,  1.56s/it]\u001b[A\n",
      "Iteration:  21% 21/102 [00:32<02:06,  1.56s/it]\u001b[A\n",
      "Iteration:  22% 22/102 [00:34<02:04,  1.56s/it]\u001b[A\n",
      "Iteration:  23% 23/102 [00:35<02:03,  1.56s/it]\u001b[A\n",
      "Iteration:  24% 24/102 [00:37<02:01,  1.56s/it]\u001b[A\n",
      "Iteration:  25% 25/102 [00:38<02:00,  1.56s/it]\u001b[A\n",
      "Iteration:  25% 26/102 [00:40<01:59,  1.57s/it]\u001b[A\n",
      "Iteration:  26% 27/102 [00:42<01:57,  1.57s/it]\u001b[A\n",
      "Iteration:  27% 28/102 [00:43<01:56,  1.57s/it]\u001b[A\n",
      "Iteration:  28% 29/102 [00:45<01:55,  1.58s/it]\u001b[A\n",
      "Iteration:  29% 30/102 [00:46<01:53,  1.57s/it]\u001b[A\n",
      "Iteration:  30% 31/102 [00:48<01:51,  1.57s/it]\u001b[A\n",
      "Iteration:  31% 32/102 [00:49<01:50,  1.58s/it]\u001b[A\n",
      "Iteration:  32% 33/102 [00:51<01:48,  1.57s/it]\u001b[A\n",
      "Iteration:  33% 34/102 [00:53<01:47,  1.57s/it]\u001b[A\n",
      "Iteration:  34% 35/102 [00:54<01:45,  1.58s/it]\u001b[A\n",
      "Iteration:  35% 36/102 [00:56<01:44,  1.58s/it]\u001b[A\n",
      "Iteration:  36% 37/102 [00:57<01:42,  1.58s/it]\u001b[A\n",
      "Iteration:  37% 38/102 [00:59<01:40,  1.58s/it]\u001b[A\n",
      "Iteration:  38% 39/102 [01:00<01:39,  1.57s/it]\u001b[A\n",
      "Iteration:  39% 40/102 [01:02<01:37,  1.58s/it]\u001b[A\n",
      "Iteration:  40% 41/102 [01:04<01:36,  1.58s/it]\u001b[A\n",
      "Iteration:  41% 42/102 [01:05<01:34,  1.58s/it]\u001b[A\n",
      "Iteration:  42% 43/102 [01:07<01:33,  1.58s/it]\u001b[A\n",
      "Iteration:  43% 44/102 [01:08<01:31,  1.58s/it]\u001b[A\n",
      "Iteration:  44% 45/102 [01:10<01:30,  1.58s/it]\u001b[A\n",
      "Iteration:  45% 46/102 [01:12<01:28,  1.58s/it]\u001b[A\n",
      "Iteration:  46% 47/102 [01:13<01:26,  1.58s/it]\u001b[A\n",
      "Iteration:  47% 48/102 [01:15<01:25,  1.58s/it]\u001b[A\n",
      "Iteration:  48% 49/102 [01:16<01:23,  1.58s/it]\u001b[A\n",
      "Iteration:  49% 50/102 [01:18<01:22,  1.58s/it]\u001b[A\n",
      "Iteration:  50% 51/102 [01:19<01:20,  1.58s/it]\u001b[A\n",
      "Iteration:  51% 52/102 [01:21<01:19,  1.58s/it]\u001b[A\n",
      "Iteration:  52% 53/102 [01:23<01:17,  1.58s/it]\u001b[A\n",
      "Iteration:  53% 54/102 [01:24<01:15,  1.58s/it]\u001b[A\n",
      "Iteration:  54% 55/102 [01:26<01:14,  1.58s/it]\u001b[A\n",
      "Iteration:  55% 56/102 [01:27<01:12,  1.58s/it]\u001b[A\n",
      "Iteration:  56% 57/102 [01:29<01:11,  1.58s/it]\u001b[A\n",
      "Iteration:  57% 58/102 [01:31<01:09,  1.58s/it]\u001b[A\n",
      "Iteration:  58% 59/102 [01:32<01:07,  1.58s/it]\u001b[A\n",
      "Iteration:  59% 60/102 [01:34<01:06,  1.58s/it]\u001b[A\n",
      "Iteration:  60% 61/102 [01:35<01:05,  1.59s/it]\u001b[A\n",
      "Iteration:  61% 62/102 [01:37<01:03,  1.58s/it]\u001b[A\n",
      "Iteration:  62% 63/102 [01:38<01:01,  1.58s/it]\u001b[A\n",
      "Iteration:  63% 64/102 [01:40<01:00,  1.58s/it]\u001b[A\n",
      "Iteration:  64% 65/102 [01:42<00:58,  1.58s/it]\u001b[A\n",
      "Iteration:  65% 66/102 [01:43<00:56,  1.58s/it]\u001b[A\n",
      "Iteration:  66% 67/102 [01:45<00:55,  1.58s/it]\u001b[A\n",
      "Iteration:  67% 68/102 [01:46<00:53,  1.58s/it]\u001b[A\n",
      "Iteration:  68% 69/102 [01:48<00:52,  1.58s/it]\u001b[A\n",
      "Iteration:  69% 70/102 [01:49<00:50,  1.58s/it]\u001b[A\n",
      "Iteration:  70% 71/102 [01:51<00:49,  1.58s/it]\u001b[A\n",
      "Iteration:  71% 72/102 [01:53<00:47,  1.58s/it]\u001b[A\n",
      "Iteration:  72% 73/102 [01:54<00:45,  1.58s/it]\u001b[A\n",
      "Iteration:  73% 74/102 [01:56<00:44,  1.58s/it]\u001b[A\n",
      "Iteration:  74% 75/102 [01:57<00:42,  1.58s/it]\u001b[A\n",
      "Iteration:  75% 76/102 [01:59<00:41,  1.58s/it]\u001b[A\n",
      "Iteration:  75% 77/102 [02:01<00:39,  1.58s/it]\u001b[A\n",
      "Iteration:  76% 78/102 [02:02<00:37,  1.58s/it]\u001b[A\n",
      "Iteration:  77% 79/102 [02:04<00:36,  1.58s/it]\u001b[A\n",
      "Iteration:  78% 80/102 [02:05<00:34,  1.58s/it]\u001b[A\n",
      "Iteration:  79% 81/102 [02:07<00:33,  1.58s/it]\u001b[A\n",
      "Iteration:  80% 82/102 [02:08<00:31,  1.58s/it]\u001b[A\n",
      "Iteration:  81% 83/102 [02:10<00:30,  1.58s/it]\u001b[A\n",
      "Iteration:  82% 84/102 [02:12<00:28,  1.58s/it]\u001b[A\n",
      "Iteration:  83% 85/102 [02:13<00:26,  1.58s/it]\u001b[A\n",
      "Iteration:  84% 86/102 [02:15<00:25,  1.58s/it]\u001b[A\n",
      "Iteration:  85% 87/102 [02:16<00:23,  1.58s/it]\u001b[A\n",
      "Iteration:  86% 88/102 [02:18<00:22,  1.58s/it]\u001b[A\n",
      "Iteration:  87% 89/102 [02:20<00:20,  1.58s/it]\u001b[A\n",
      "Iteration:  88% 90/102 [02:21<00:18,  1.58s/it]\u001b[A\n",
      "Iteration:  89% 91/102 [02:23<00:17,  1.58s/it]\u001b[A\n",
      "Iteration:  90% 92/102 [02:24<00:15,  1.58s/it]\u001b[A\n",
      "Iteration:  91% 93/102 [02:26<00:14,  1.58s/it]\u001b[A\n",
      "Iteration:  92% 94/102 [02:27<00:12,  1.57s/it]\u001b[A\n",
      "Iteration:  93% 95/102 [02:29<00:11,  1.57s/it]\u001b[A\n",
      "Iteration:  94% 96/102 [02:31<00:09,  1.57s/it]\u001b[A\n",
      "Iteration:  95% 97/102 [02:32<00:07,  1.57s/it]\u001b[A\n",
      "Iteration:  96% 98/102 [02:34<00:06,  1.58s/it]\u001b[A\n",
      "Iteration:  97% 99/102 [02:35<00:04,  1.58s/it]\u001b[A\n",
      "Iteration:  98% 100/102 [02:37<00:03,  1.57s/it]\u001b[A\n",
      "Iteration:  99% 101/102 [02:38<00:01,  1.57s/it]\u001b[A\n",
      "Iteration: 100% 102/102 [02:40<00:00,  1.57s/it]\u001b[A\n",
      "Epoch:  33% 1/3 [02:40<05:20, 160.46s/it]\n",
      "Iteration:   0% 0/102 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1% 1/102 [00:01<02:40,  1.59s/it]\u001b[A\n",
      "Iteration:   2% 2/102 [00:03<02:38,  1.58s/it]\u001b[A\n",
      "Iteration:   3% 3/102 [00:04<02:36,  1.58s/it]\u001b[A\n",
      "Iteration:   4% 4/102 [00:06<02:34,  1.58s/it]\u001b[A\n",
      "Iteration:   5% 5/102 [00:07<02:33,  1.58s/it]\u001b[A\n",
      "Iteration:   6% 6/102 [00:09<02:31,  1.58s/it]\u001b[A\n",
      "Iteration:   7% 7/102 [00:11<02:29,  1.57s/it]\u001b[A\n",
      "Iteration:   8% 8/102 [00:12<02:27,  1.57s/it]\u001b[A\n",
      "Iteration:   9% 9/102 [00:14<02:26,  1.58s/it]\u001b[A\n",
      "Iteration:  10% 10/102 [00:15<02:24,  1.57s/it]\u001b[A\n",
      "Iteration:  11% 11/102 [00:17<02:23,  1.58s/it]\u001b[A\n",
      "Iteration:  12% 12/102 [00:18<02:22,  1.58s/it]\u001b[A\n",
      "Iteration:  13% 13/102 [00:20<02:20,  1.58s/it]\u001b[A\n",
      "Iteration:  14% 14/102 [00:22<02:19,  1.59s/it]\u001b[A\n",
      "Iteration:  15% 15/102 [00:23<02:18,  1.59s/it]\u001b[A\n",
      "Iteration:  16% 16/102 [00:25<02:16,  1.58s/it]\u001b[A\n",
      "Iteration:  17% 17/102 [00:26<02:14,  1.59s/it]\u001b[A\n",
      "Iteration:  18% 18/102 [00:28<02:13,  1.58s/it]\u001b[A\n",
      "Iteration:  19% 19/102 [00:30<02:11,  1.58s/it]\u001b[A\n",
      "Iteration:  20% 20/102 [00:31<02:10,  1.59s/it]\u001b[A\n",
      "Iteration:  21% 21/102 [00:33<02:08,  1.58s/it]\u001b[A\n",
      "Iteration:  22% 22/102 [00:34<02:06,  1.59s/it]\u001b[A\n",
      "Iteration:  23% 23/102 [00:36<02:05,  1.58s/it]\u001b[A\n",
      "Iteration:  24% 24/102 [00:37<02:03,  1.59s/it]\u001b[A\n",
      "Iteration:  25% 25/102 [00:39<02:02,  1.59s/it]\u001b[A\n",
      "Iteration:  25% 26/102 [00:41<02:00,  1.58s/it]\u001b[A\n",
      "Iteration:  26% 27/102 [00:42<01:58,  1.58s/it]\u001b[A\n",
      "Iteration:  27% 28/102 [00:44<01:57,  1.58s/it]\u001b[A\n",
      "Iteration:  28% 29/102 [00:45<01:55,  1.58s/it]\u001b[A\n",
      "Iteration:  29% 30/102 [00:47<01:54,  1.59s/it]\u001b[A\n",
      "Iteration:  30% 31/102 [00:49<01:52,  1.58s/it]\u001b[A\n",
      "Iteration:  31% 32/102 [00:50<01:50,  1.58s/it]\u001b[A\n",
      "Iteration:  32% 33/102 [00:52<01:48,  1.58s/it]\u001b[A\n",
      "Iteration:  33% 34/102 [00:53<01:47,  1.58s/it]\u001b[A\n",
      "Iteration:  34% 35/102 [00:55<01:45,  1.58s/it]\u001b[A\n",
      "Iteration:  35% 36/102 [00:56<01:44,  1.58s/it]\u001b[A\n",
      "Iteration:  36% 37/102 [00:58<01:42,  1.58s/it]\u001b[A\n",
      "Iteration:  37% 38/102 [01:00<01:41,  1.58s/it]\u001b[A\n",
      "Iteration:  38% 39/102 [01:01<01:39,  1.58s/it]\u001b[A\n",
      "Iteration:  39% 40/102 [01:03<01:38,  1.58s/it]\u001b[A\n",
      "Iteration:  40% 41/102 [01:04<01:36,  1.59s/it]\u001b[A\n",
      "Iteration:  41% 42/102 [01:06<01:35,  1.59s/it]\u001b[A\n",
      "Iteration:  42% 43/102 [01:08<01:33,  1.58s/it]\u001b[A\n",
      "Iteration:  43% 44/102 [01:09<01:31,  1.58s/it]\u001b[A\n",
      "Iteration:  44% 45/102 [01:11<01:30,  1.58s/it]\u001b[A\n",
      "Iteration:  45% 46/102 [01:12<01:28,  1.58s/it]\u001b[A\n",
      "Iteration:  46% 47/102 [01:14<01:26,  1.58s/it]\u001b[A\n",
      "Iteration:  47% 48/102 [01:15<01:25,  1.58s/it]\u001b[A\n",
      "Iteration:  48% 49/102 [01:17<01:23,  1.58s/it]\u001b[A\n",
      "Iteration:  49% 50/102 [01:19<01:22,  1.58s/it]\u001b[A\n",
      "Iteration:  50% 51/102 [01:20<01:20,  1.58s/it]\u001b[A\n",
      "Iteration:  51% 52/102 [01:22<01:19,  1.58s/it]\u001b[A\n",
      "Iteration:  52% 53/102 [01:23<01:17,  1.58s/it]\u001b[A\n",
      "Iteration:  53% 54/102 [01:25<01:16,  1.58s/it]\u001b[A\n",
      "Iteration:  54% 55/102 [01:26<01:14,  1.58s/it]\u001b[A\n",
      "Iteration:  55% 56/102 [01:28<01:12,  1.58s/it]\u001b[A\n",
      "Iteration:  56% 57/102 [01:30<01:11,  1.58s/it]\u001b[A\n",
      "Iteration:  57% 58/102 [01:31<01:09,  1.58s/it]\u001b[A\n",
      "Iteration:  58% 59/102 [01:33<01:07,  1.58s/it]\u001b[A\n",
      "Iteration:  59% 60/102 [01:34<01:06,  1.57s/it]\u001b[A\n",
      "Iteration:  60% 61/102 [01:36<01:04,  1.57s/it]\u001b[A\n",
      "Iteration:  61% 62/102 [01:38<01:03,  1.58s/it]\u001b[A\n",
      "Iteration:  62% 63/102 [01:39<01:01,  1.58s/it]\u001b[A\n",
      "Iteration:  63% 64/102 [01:41<00:59,  1.58s/it]\u001b[A\n",
      "Iteration:  64% 65/102 [01:42<00:58,  1.58s/it]\u001b[A\n",
      "Iteration:  65% 66/102 [01:44<00:56,  1.57s/it]\u001b[A\n",
      "Iteration:  66% 67/102 [01:45<00:55,  1.57s/it]\u001b[A\n",
      "Iteration:  67% 68/102 [01:47<00:53,  1.57s/it]\u001b[A\n",
      "Iteration:  68% 69/102 [01:49<00:51,  1.57s/it]\u001b[A\n",
      "Iteration:  69% 70/102 [01:50<00:50,  1.58s/it]\u001b[A\n",
      "Iteration:  70% 71/102 [01:52<00:48,  1.57s/it]\u001b[A\n",
      "Iteration:  71% 72/102 [01:53<00:47,  1.57s/it]\u001b[A\n",
      "Iteration:  72% 73/102 [01:55<00:45,  1.58s/it]\u001b[A\n",
      "Iteration:  73% 74/102 [01:56<00:44,  1.58s/it]\u001b[A\n",
      "Iteration:  74% 75/102 [01:58<00:42,  1.58s/it]\u001b[A\n",
      "Iteration:  75% 76/102 [02:00<00:40,  1.57s/it]\u001b[A\n",
      "Iteration:  75% 77/102 [02:01<00:39,  1.57s/it]\u001b[A\n",
      "Iteration:  76% 78/102 [02:03<00:37,  1.57s/it]\u001b[A\n",
      "Iteration:  77% 79/102 [02:04<00:36,  1.57s/it]\u001b[A\n",
      "Iteration:  78% 80/102 [02:06<00:34,  1.58s/it]\u001b[A\n",
      "Iteration:  79% 81/102 [02:07<00:33,  1.57s/it]\u001b[A\n",
      "Iteration:  80% 82/102 [02:09<00:31,  1.57s/it]\u001b[A\n",
      "Iteration:  81% 83/102 [02:11<00:29,  1.57s/it]\u001b[A\n",
      "Iteration:  82% 84/102 [02:12<00:28,  1.57s/it]\u001b[A\n",
      "Iteration:  83% 85/102 [02:14<00:26,  1.58s/it]\u001b[A\n",
      "Iteration:  84% 86/102 [02:15<00:25,  1.57s/it]\u001b[A\n",
      "Iteration:  85% 87/102 [02:17<00:23,  1.58s/it]\u001b[A\n",
      "Iteration:  86% 88/102 [02:18<00:22,  1.58s/it]\u001b[A\n",
      "Iteration:  87% 89/102 [02:20<00:20,  1.58s/it]\u001b[A\n",
      "Iteration:  88% 90/102 [02:22<00:19,  1.58s/it]\u001b[A\n",
      "Iteration:  89% 91/102 [02:23<00:17,  1.58s/it]\u001b[A\n",
      "Iteration:  90% 92/102 [02:25<00:15,  1.58s/it]\u001b[A\n",
      "Iteration:  91% 93/102 [02:26<00:14,  1.58s/it]\u001b[A\n",
      "Iteration:  92% 94/102 [02:28<00:12,  1.58s/it]\u001b[A\n",
      "Iteration:  93% 95/102 [02:30<00:11,  1.58s/it]\u001b[A\n",
      "Iteration:  94% 96/102 [02:31<00:09,  1.57s/it]\u001b[A\n",
      "Iteration:  95% 97/102 [02:33<00:07,  1.58s/it]\u001b[A\n",
      "Iteration:  96% 98/102 [02:34<00:06,  1.58s/it]\u001b[A\n",
      "Iteration:  97% 99/102 [02:36<00:04,  1.58s/it]\u001b[A\n",
      "Iteration:  98% 100/102 [02:37<00:03,  1.58s/it]\u001b[A\n",
      "Iteration:  99% 101/102 [02:39<00:01,  1.58s/it]\u001b[A\n",
      "Iteration: 100% 102/102 [02:41<00:00,  1.58s/it]\u001b[A\n",
      "Epoch:  67% 2/3 [05:21<02:40, 160.65s/it]\n",
      "Iteration:   0% 0/102 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1% 1/102 [00:01<02:41,  1.60s/it]\u001b[A\n",
      "Iteration:   2% 2/102 [00:03<02:39,  1.59s/it]\u001b[A\n",
      "Iteration:   3% 3/102 [00:04<02:37,  1.59s/it]\u001b[A\n",
      "Iteration:   4% 4/102 [00:06<02:35,  1.59s/it]\u001b[A\n",
      "Iteration:   5% 5/102 [00:07<02:33,  1.58s/it]\u001b[A\n",
      "Iteration:   6% 6/102 [00:09<02:31,  1.58s/it]\u001b[A\n",
      "Iteration:   7% 7/102 [00:11<02:29,  1.58s/it]\u001b[A\n",
      "Iteration:   8% 8/102 [00:12<02:28,  1.58s/it]\u001b[A\n",
      "Iteration:   9% 9/102 [00:14<02:26,  1.58s/it]\u001b[A\n",
      "Iteration:  10% 10/102 [00:15<02:25,  1.58s/it]\u001b[A\n",
      "Iteration:  11% 11/102 [00:17<02:23,  1.58s/it]\u001b[A\n",
      "Iteration:  12% 12/102 [00:18<02:22,  1.58s/it]\u001b[A\n",
      "Iteration:  13% 13/102 [00:20<02:20,  1.58s/it]\u001b[A\n",
      "Iteration:  14% 14/102 [00:22<02:18,  1.58s/it]\u001b[A\n",
      "Iteration:  15% 15/102 [00:23<02:16,  1.57s/it]\u001b[A\n",
      "Iteration:  16% 16/102 [00:25<02:15,  1.58s/it]\u001b[A\n",
      "Iteration:  17% 17/102 [00:26<02:14,  1.58s/it]\u001b[A\n",
      "Iteration:  18% 18/102 [00:28<02:12,  1.58s/it]\u001b[A\n",
      "Iteration:  19% 19/102 [00:29<02:10,  1.58s/it]\u001b[A\n",
      "Iteration:  20% 20/102 [00:31<02:09,  1.58s/it]\u001b[A\n",
      "Iteration:  21% 21/102 [00:33<02:07,  1.58s/it]\u001b[A\n",
      "Iteration:  22% 22/102 [00:34<02:06,  1.58s/it]\u001b[A\n",
      "Iteration:  23% 23/102 [00:36<02:04,  1.58s/it]\u001b[A\n",
      "Iteration:  24% 24/102 [00:37<02:03,  1.58s/it]\u001b[A\n",
      "Iteration:  25% 25/102 [00:39<02:01,  1.58s/it]\u001b[A\n",
      "Iteration:  25% 26/102 [00:41<01:59,  1.57s/it]\u001b[A\n",
      "Iteration:  26% 27/102 [00:42<01:58,  1.58s/it]\u001b[A\n",
      "Iteration:  27% 28/102 [00:44<01:56,  1.58s/it]\u001b[A\n",
      "Iteration:  28% 29/102 [00:45<01:55,  1.58s/it]\u001b[A\n",
      "Iteration:  29% 30/102 [00:47<01:53,  1.58s/it]\u001b[A\n",
      "Iteration:  30% 31/102 [00:48<01:52,  1.58s/it]\u001b[A\n",
      "Iteration:  31% 32/102 [00:50<01:51,  1.59s/it]\u001b[A\n",
      "Iteration:  32% 33/102 [00:52<01:49,  1.58s/it]\u001b[A\n",
      "Iteration:  33% 34/102 [00:53<01:47,  1.58s/it]\u001b[A\n",
      "Iteration:  34% 35/102 [00:55<01:46,  1.58s/it]\u001b[A\n",
      "Iteration:  35% 36/102 [00:56<01:44,  1.58s/it]\u001b[A\n",
      "Iteration:  36% 37/102 [00:58<01:42,  1.58s/it]\u001b[A\n",
      "Iteration:  37% 38/102 [00:59<01:40,  1.58s/it]\u001b[A\n",
      "Iteration:  38% 39/102 [01:01<01:39,  1.58s/it]\u001b[A\n",
      "Iteration:  39% 40/102 [01:03<01:37,  1.58s/it]\u001b[A\n",
      "Iteration:  40% 41/102 [01:04<01:36,  1.58s/it]\u001b[A\n",
      "Iteration:  41% 42/102 [01:06<01:35,  1.59s/it]\u001b[A\n",
      "Iteration:  42% 43/102 [01:07<01:33,  1.58s/it]\u001b[A\n",
      "Iteration:  43% 44/102 [01:09<01:31,  1.58s/it]\u001b[A\n",
      "Iteration:  44% 45/102 [01:11<01:29,  1.58s/it]\u001b[A\n",
      "Iteration:  45% 46/102 [01:12<01:28,  1.58s/it]\u001b[A\n",
      "Iteration:  46% 47/102 [01:14<01:26,  1.58s/it]\u001b[A\n",
      "Iteration:  47% 48/102 [01:15<01:25,  1.58s/it]\u001b[A\n",
      "Iteration:  48% 49/102 [01:17<01:23,  1.57s/it]\u001b[A\n",
      "Iteration:  49% 50/102 [01:18<01:22,  1.58s/it]\u001b[A\n",
      "Iteration:  50% 51/102 [01:20<01:20,  1.58s/it]\u001b[A\n",
      "Iteration:  51% 52/102 [01:22<01:19,  1.58s/it]\u001b[A\n",
      "Iteration:  52% 53/102 [01:23<01:17,  1.58s/it]\u001b[A\n",
      "Iteration:  53% 54/102 [01:25<01:15,  1.58s/it]\u001b[A\n",
      "Iteration:  54% 55/102 [01:26<01:14,  1.58s/it]\u001b[A\n",
      "Iteration:  55% 56/102 [01:28<01:12,  1.58s/it]\u001b[A\n",
      "Iteration:  56% 57/102 [01:29<01:11,  1.58s/it]\u001b[A\n",
      "Iteration:  57% 58/102 [01:31<01:09,  1.58s/it]\u001b[A\n",
      "Iteration:  58% 59/102 [01:33<01:07,  1.58s/it]\u001b[A\n",
      "Iteration:  59% 60/102 [01:34<01:06,  1.58s/it]\u001b[A\n",
      "Iteration:  60% 61/102 [01:36<01:04,  1.58s/it]\u001b[A\n",
      "Iteration:  61% 62/102 [01:37<01:03,  1.58s/it]\u001b[A\n",
      "Iteration:  62% 63/102 [01:39<01:01,  1.58s/it]\u001b[A\n",
      "Iteration:  63% 64/102 [01:41<00:59,  1.58s/it]\u001b[A\n",
      "Iteration:  64% 65/102 [01:42<00:58,  1.58s/it]\u001b[A\n",
      "Iteration:  65% 66/102 [01:44<00:56,  1.58s/it]\u001b[A\n",
      "Iteration:  66% 67/102 [01:45<00:55,  1.57s/it]\u001b[A\n",
      "Iteration:  67% 68/102 [01:47<00:53,  1.57s/it]\u001b[A\n",
      "Iteration:  68% 69/102 [01:48<00:52,  1.58s/it]\u001b[A\n",
      "Iteration:  69% 70/102 [01:50<00:50,  1.58s/it]\u001b[A\n",
      "Iteration:  70% 71/102 [01:52<00:48,  1.58s/it]\u001b[A\n",
      "Iteration:  71% 72/102 [01:53<00:47,  1.58s/it]\u001b[A\n",
      "Iteration:  72% 73/102 [01:55<00:45,  1.57s/it]\u001b[A\n",
      "Iteration:  73% 74/102 [01:56<00:44,  1.57s/it]\u001b[A\n",
      "Iteration:  74% 75/102 [01:58<00:42,  1.57s/it]\u001b[A\n",
      "Iteration:  75% 76/102 [01:59<00:41,  1.58s/it]\u001b[A\n",
      "Iteration:  75% 77/102 [02:01<00:39,  1.58s/it]\u001b[A\n",
      "Iteration:  76% 78/102 [02:03<00:37,  1.57s/it]\u001b[A\n",
      "Iteration:  77% 79/102 [02:04<00:36,  1.57s/it]\u001b[A\n",
      "Iteration:  78% 80/102 [02:06<00:34,  1.57s/it]\u001b[A\n",
      "Iteration:  79% 81/102 [02:07<00:33,  1.58s/it]\u001b[A\n",
      "Iteration:  80% 82/102 [02:09<00:31,  1.58s/it]\u001b[A\n",
      "Iteration:  81% 83/102 [02:10<00:29,  1.58s/it]\u001b[A\n",
      "Iteration:  82% 84/102 [02:12<00:28,  1.58s/it]\u001b[A\n",
      "Iteration:  83% 85/102 [02:14<00:26,  1.58s/it]\u001b[A\n",
      "Iteration:  84% 86/102 [02:15<00:25,  1.58s/it]\u001b[A\n",
      "Iteration:  85% 87/102 [02:17<00:23,  1.58s/it]\u001b[A\n",
      "Iteration:  86% 88/102 [02:18<00:22,  1.58s/it]\u001b[A\n",
      "Iteration:  87% 89/102 [02:20<00:20,  1.58s/it]\u001b[A\n",
      "Iteration:  88% 90/102 [02:21<00:18,  1.58s/it]\u001b[A\n",
      "Iteration:  89% 91/102 [02:23<00:17,  1.58s/it]\u001b[A\n",
      "Iteration:  90% 92/102 [02:25<00:15,  1.57s/it]\u001b[A\n",
      "Iteration:  91% 93/102 [02:26<00:14,  1.57s/it]\u001b[A\n",
      "Iteration:  92% 94/102 [02:28<00:12,  1.57s/it]\u001b[A\n",
      "Iteration:  93% 95/102 [02:29<00:11,  1.57s/it]\u001b[A\n",
      "Iteration:  94% 96/102 [02:31<00:09,  1.58s/it]\u001b[A\n",
      "Iteration:  95% 97/102 [02:33<00:07,  1.58s/it]\u001b[A\n",
      "Iteration:  96% 98/102 [02:34<00:06,  1.58s/it]\u001b[A\n",
      "Iteration:  97% 99/102 [02:36<00:04,  1.58s/it]\u001b[A\n",
      "Iteration:  98% 100/102 [02:37<00:03,  1.58s/it]\u001b[A\n",
      "Iteration:  99% 101/102 [02:39<00:01,  1.58s/it]\u001b[A\n",
      "Iteration: 100% 102/102 [02:40<00:00,  1.58s/it]\u001b[A\n",
      "Epoch: 100% 3/3 [08:02<00:00, 160.74s/it]\n",
      "12/01/2019 21:22:56 - INFO - pytorch_pretrained_bert.modeling -   loading weights file LAST_finetuned_bert/pytorch_model.bin\n",
      "12/01/2019 21:22:56 - INFO - pytorch_pretrained_bert.modeling -   loading configuration file LAST_finetuned_bert/config.json\n",
      "12/01/2019 21:22:56 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "12/01/2019 21:23:00 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file LAST_finetuned_bert/vocab.txt\n",
      "12/01/2019 21:23:00 - INFO - __main__ -   ***** Running evaluation *****\n",
      "12/01/2019 21:23:00 - INFO - __main__ -     Num examples = 816\n",
      "12/01/2019 21:23:00 - INFO - __main__ -     Batch size = 8\n",
      "Evaluating: 100% 102/102 [00:13<00:00,  7.32it/s]\n",
      "12/01/2019 21:23:14 - INFO - __main__ -   ***** Eval results *****\n",
      "12/01/2019 21:23:14 - INFO - __main__ -     acc = 0.9571078431372549\n",
      "12/01/2019 21:23:14 - INFO - __main__ -     classification report =               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95       277\n",
      "           1       0.96      0.95      0.95       279\n",
      "           2       0.98      0.95      0.96       260\n",
      "\n",
      "    accuracy                           0.96       816\n",
      "   macro avg       0.96      0.96      0.96       816\n",
      "weighted avg       0.96      0.96      0.96       816\n",
      "\n",
      "12/01/2019 21:23:14 - INFO - __main__ -     eval_loss = 0.3133358404624696\n",
      "12/01/2019 21:23:14 - INFO - __main__ -     global_step = 306\n",
      "12/01/2019 21:23:14 - INFO - __main__ -     loss = 0.12011059573273254\n"
     ]
    }
   ],
   "source": [
    "#with full data. \n",
    "!python run_classifier_py.py --task_name SST-2 --do_train --do_eval --bert_model bert-base-uncased --do_lower_case --data_dir DATA_LAST_TRY --max_seq_length 128 --learning_rate 1e-6 --num_train_epochs 3.0 --output_dir LAST_finetuned_bert --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "colab_type": "code",
    "id": "EB1rdGzU0_a5",
    "outputId": "e46e6586-5f6f-40b7-aad9-dedbe0f91688"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/22/2019 02:58:20 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/22/2019 02:58:20 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file finetuned_bert/vocab.txt\n",
      "11/22/2019 02:58:20 - INFO - pytorch_pretrained_bert.modeling -   loading weights file finetuned_bert/pytorch_model.bin\n",
      "11/22/2019 02:58:20 - INFO - pytorch_pretrained_bert.modeling -   loading configuration file finetuned_bert/config.json\n",
      "11/22/2019 02:58:20 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "11/22/2019 02:58:27 - INFO - pytorch_pretrained_bert.modeling -   loading weights file finetuned_bert/pytorch_model.bin\n",
      "11/22/2019 02:58:27 - INFO - pytorch_pretrained_bert.modeling -   loading configuration file finetuned_bert/config.json\n",
      "11/22/2019 02:58:27 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "11/22/2019 02:58:31 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/22/2019 02:58:31 - INFO - __main__ -     Num examples = 988\n",
      "11/22/2019 02:58:31 - INFO - __main__ -     Batch size = 8\n",
      "Evaluating: 100% 124/124 [00:09<00:00, 14.30it/s]\n",
      "11/22/2019 02:58:40 - INFO - __main__ -   ***** Eval results *****\n",
      "11/22/2019 02:58:40 - INFO - __main__ -     acc = 0.9868421052631579\n",
      "11/22/2019 02:58:40 - INFO - __main__ -     eval_loss = 0.05858615371248414\n",
      "11/22/2019 02:58:40 - INFO - __main__ -     global_step = 0\n",
      "11/22/2019 02:58:40 - INFO - __main__ -     loss = None\n"
     ]
    }
   ],
   "source": [
    "#with full data. \n",
    "!python run_classifier_py.py --task_name SST-2 --do_eval --bert_model finetuned_bert --do_lower_case --data_dir input_data_test/ --max_seq_length 128 --output_dir finetuned_bert2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WRFgYW87V6lL"
   },
   "source": [
    "# Attempt 2 Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ec84G8P5eOLT"
   },
   "source": [
    "Fold 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "KxOkvOiOCTqd",
    "outputId": "7cbb7cf8-c082-4b3d-e832-8b8f21e5c492"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/01/2019 21:43:04 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "12/01/2019 21:43:05 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "12/01/2019 21:43:07 - INFO - pytorch_pretrained_bert.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_pretrained_bert/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "12/01/2019 21:43:07 - INFO - pytorch_pretrained_bert.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_pretrained_bert/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "12/01/2019 21:43:07 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "12/01/2019 21:43:11 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "12/01/2019 21:43:11 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   Writing example 0 of 2611\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   guid: train-1\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   tokens: [CLS] the video are long and not engaging enough rather than digital marketing concept this wa more an application of trend to traditional marketing concept some of the topic are already outdated and some assignment were more applicable to those residing in the u s or are not exactly feasible to do at this point [SEP]\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   input_ids: 101 1996 2678 2024 2146 1998 2025 11973 2438 2738 2084 3617 5821 4145 2023 11333 2062 2019 4646 1997 9874 2000 3151 5821 4145 2070 1997 1996 8476 2024 2525 25963 1998 2070 8775 2020 2062 12711 2000 2216 7154 1999 1996 1057 1055 2030 2024 2025 3599 22945 2000 2079 2012 2023 2391 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   label: 1 (id = 1)\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   guid: train-2\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   tokens: [CLS] it wa fast but provided a good overview i would like some case to work and to apply the method to [SEP]\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   input_ids: 101 2009 11333 3435 2021 3024 1037 2204 19184 1045 2052 2066 2070 2553 2000 2147 1998 2000 6611 1996 4118 2000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   label: 2 (id = 2)\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   guid: train-3\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   tokens: [CLS] half of the link to external reading and video did not work which is the reason i did not complete the course it made taking the quiz fairly impossible unless i wanted to spend my time goo ##gling the answer [SEP]\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   input_ids: 101 2431 1997 1996 4957 2000 6327 3752 1998 2678 2106 2025 2147 2029 2003 1996 3114 1045 2106 2025 3143 1996 2607 2009 2081 2635 1996 19461 7199 5263 4983 1045 2359 2000 5247 2026 2051 27571 18483 1996 3437 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   label: 1 (id = 1)\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   guid: train-4\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   tokens: [CLS] the course cover very useful and interesting topic not in very much detail though it is useful to begin with the topic but it is really not more that a thin introduction [SEP]\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   input_ids: 101 1996 2607 3104 2200 6179 1998 5875 8476 2025 1999 2200 2172 6987 2295 2009 2003 6179 2000 4088 2007 1996 8476 2021 2009 2003 2428 2025 2062 2008 1037 4857 4955 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   label: 1 (id = 1)\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   guid: train-5\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   tokens: [CLS] please we all have accent but i can not understand some word and make it more difficult [SEP]\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   input_ids: 101 3531 2057 2035 2031 9669 2021 1045 2064 2025 3305 2070 2773 1998 2191 2009 2062 3697 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:43:14 - INFO - run_classifier_dataset_utils -   label: 1 (id = 1)\n",
      "12/01/2019 21:43:16 - INFO - __main__ -     Saving train features into cached file fold1/train_bert-base-uncased_128_sst-2\n",
      "12/01/2019 21:43:16 - INFO - __main__ -   ***** Running training *****\n",
      "12/01/2019 21:43:16 - INFO - __main__ -     Num examples = 2611\n",
      "12/01/2019 21:43:16 - INFO - __main__ -     Batch size = 32\n",
      "12/01/2019 21:43:16 - INFO - __main__ -     Num steps = 246\n",
      "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
      "Iteration:   0% 0/82 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1% 1/82 [00:01<02:14,  1.66s/it]\u001b[A\n",
      "Iteration:   2% 2/82 [00:03<02:10,  1.63s/it]\u001b[A\n",
      "Iteration:   4% 3/82 [00:04<02:07,  1.61s/it]\u001b[A\n",
      "Iteration:   5% 4/82 [00:06<02:05,  1.60s/it]\u001b[A\n",
      "Iteration:   6% 5/82 [00:07<02:03,  1.60s/it]\u001b[A\n",
      "Iteration:   7% 6/82 [00:09<02:01,  1.59s/it]\u001b[A\n",
      "Iteration:   9% 7/82 [00:11<01:59,  1.59s/it]\u001b[A\n",
      "Iteration:  10% 8/82 [00:12<01:57,  1.59s/it]\u001b[A\n",
      "Iteration:  11% 9/82 [00:14<01:55,  1.59s/it]\u001b[A\n",
      "Iteration:  12% 10/82 [00:15<01:54,  1.59s/it]\u001b[A\n",
      "Iteration:  13% 11/82 [00:17<01:53,  1.59s/it]\u001b[A\n",
      "Iteration:  15% 12/82 [00:19<01:51,  1.60s/it]\u001b[A\n",
      "Iteration:  16% 13/82 [00:20<01:49,  1.59s/it]\u001b[A\n",
      "Iteration:  17% 14/82 [00:22<01:48,  1.59s/it]\u001b[A\n",
      "Iteration:  18% 15/82 [00:23<01:46,  1.59s/it]\u001b[A\n",
      "Iteration:  20% 16/82 [00:25<01:45,  1.59s/it]\u001b[A\n",
      "Iteration:  21% 17/82 [00:27<01:43,  1.59s/it]\u001b[A\n",
      "Iteration:  22% 18/82 [00:28<01:41,  1.59s/it]\u001b[A\n",
      "Iteration:  23% 19/82 [00:30<01:40,  1.59s/it]\u001b[A\n",
      "Iteration:  24% 20/82 [00:31<01:38,  1.59s/it]\u001b[A\n",
      "Iteration:  26% 21/82 [00:33<01:36,  1.58s/it]\u001b[A\n",
      "Iteration:  27% 22/82 [00:34<01:34,  1.58s/it]\u001b[A\n",
      "Iteration:  28% 23/82 [00:36<01:33,  1.58s/it]\u001b[A\n",
      "Iteration:  29% 24/82 [00:38<01:31,  1.58s/it]\u001b[A\n",
      "Iteration:  30% 25/82 [00:39<01:30,  1.58s/it]\u001b[A\n",
      "Iteration:  32% 26/82 [00:41<01:28,  1.58s/it]\u001b[A\n",
      "Iteration:  33% 27/82 [00:42<01:26,  1.58s/it]\u001b[A\n",
      "Iteration:  34% 28/82 [00:44<01:25,  1.58s/it]\u001b[A\n",
      "Iteration:  35% 29/82 [00:46<01:24,  1.59s/it]\u001b[A\n",
      "Iteration:  37% 30/82 [00:47<01:22,  1.59s/it]\u001b[A\n",
      "Iteration:  38% 31/82 [00:49<01:20,  1.58s/it]\u001b[A\n",
      "Iteration:  39% 32/82 [00:50<01:19,  1.59s/it]\u001b[A\n",
      "Iteration:  40% 33/82 [00:52<01:17,  1.58s/it]\u001b[A\n",
      "Iteration:  41% 34/82 [00:53<01:16,  1.58s/it]\u001b[A\n",
      "Iteration:  43% 35/82 [00:55<01:14,  1.58s/it]\u001b[A\n",
      "Iteration:  44% 36/82 [00:57<01:12,  1.58s/it]\u001b[A\n",
      "Iteration:  45% 37/82 [00:58<01:11,  1.58s/it]\u001b[A\n",
      "Iteration:  46% 38/82 [01:00<01:09,  1.58s/it]\u001b[A\n",
      "Iteration:  48% 39/82 [01:01<01:07,  1.58s/it]\u001b[A\n",
      "Iteration:  49% 40/82 [01:03<01:06,  1.58s/it]\u001b[A\n",
      "Iteration:  50% 41/82 [01:05<01:04,  1.58s/it]\u001b[A\n",
      "Iteration:  51% 42/82 [01:06<01:03,  1.58s/it]\u001b[A\n",
      "Iteration:  52% 43/82 [01:08<01:01,  1.58s/it]\u001b[A\n",
      "Iteration:  54% 44/82 [01:09<00:59,  1.58s/it]\u001b[A\n",
      "Iteration:  55% 45/82 [01:11<00:58,  1.58s/it]\u001b[A\n",
      "Iteration:  56% 46/82 [01:12<00:56,  1.58s/it]\u001b[A\n",
      "Iteration:  57% 47/82 [01:14<00:55,  1.57s/it]\u001b[A\n",
      "Iteration:  59% 48/82 [01:16<00:53,  1.57s/it]\u001b[A\n",
      "Iteration:  60% 49/82 [01:17<00:51,  1.57s/it]\u001b[A\n",
      "Iteration:  61% 50/82 [01:19<00:50,  1.57s/it]\u001b[A\n",
      "Iteration:  62% 51/82 [01:20<00:48,  1.58s/it]\u001b[A\n",
      "Iteration:  63% 52/82 [01:22<00:47,  1.58s/it]\u001b[A\n",
      "Iteration:  65% 53/82 [01:23<00:45,  1.58s/it]\u001b[A\n",
      "Iteration:  66% 54/82 [01:25<00:44,  1.57s/it]\u001b[A\n",
      "Iteration:  67% 55/82 [01:27<00:42,  1.57s/it]\u001b[A\n",
      "Iteration:  68% 56/82 [01:28<00:40,  1.57s/it]\u001b[A\n",
      "Iteration:  70% 57/82 [01:30<00:39,  1.58s/it]\u001b[A\n",
      "Iteration:  71% 58/82 [01:31<00:37,  1.57s/it]\u001b[A\n",
      "Iteration:  72% 59/82 [01:33<00:36,  1.57s/it]\u001b[A\n",
      "Iteration:  73% 60/82 [01:34<00:34,  1.57s/it]\u001b[A\n",
      "Iteration:  74% 61/82 [01:36<00:33,  1.57s/it]\u001b[A\n",
      "Iteration:  76% 62/82 [01:38<00:31,  1.58s/it]\u001b[A\n",
      "Iteration:  77% 63/82 [01:39<00:29,  1.57s/it]\u001b[A\n",
      "Iteration:  78% 64/82 [01:41<00:28,  1.58s/it]\u001b[A\n",
      "Iteration:  79% 65/82 [01:42<00:26,  1.58s/it]\u001b[A\n",
      "Iteration:  80% 66/82 [01:44<00:25,  1.58s/it]\u001b[A\n",
      "Iteration:  82% 67/82 [01:45<00:23,  1.57s/it]\u001b[A\n",
      "Iteration:  83% 68/82 [01:47<00:21,  1.57s/it]\u001b[A\n",
      "Iteration:  84% 69/82 [01:49<00:20,  1.57s/it]\u001b[A\n",
      "Iteration:  85% 70/82 [01:50<00:18,  1.57s/it]\u001b[A\n",
      "Iteration:  87% 71/82 [01:52<00:17,  1.57s/it]\u001b[A\n",
      "Iteration:  88% 72/82 [01:53<00:15,  1.57s/it]\u001b[A\n",
      "Iteration:  89% 73/82 [01:55<00:14,  1.57s/it]\u001b[A\n",
      "Iteration:  90% 74/82 [01:56<00:12,  1.57s/it]\u001b[A\n",
      "Iteration:  91% 75/82 [01:58<00:11,  1.57s/it]\u001b[A\n",
      "Iteration:  93% 76/82 [02:00<00:09,  1.57s/it]\u001b[A\n",
      "Iteration:  94% 77/82 [02:01<00:07,  1.58s/it]\u001b[A\n",
      "Iteration:  95% 78/82 [02:03<00:06,  1.58s/it]\u001b[A\n",
      "Iteration:  96% 79/82 [02:04<00:04,  1.58s/it]\u001b[A\n",
      "Iteration:  98% 80/82 [02:06<00:03,  1.58s/it]\u001b[A\n",
      "Iteration:  99% 81/82 [02:07<00:01,  1.58s/it]\u001b[A\n",
      "Iteration: 100% 82/82 [02:09<00:00,  1.42s/it]\u001b[A\n",
      "Epoch:  33% 1/3 [02:09<04:18, 129.04s/it]\n",
      "Iteration:   0% 0/82 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1% 1/82 [00:01<02:08,  1.59s/it]\u001b[A\n",
      "Iteration:   2% 2/82 [00:03<02:06,  1.58s/it]\u001b[A\n",
      "Iteration:   4% 3/82 [00:04<02:04,  1.58s/it]\u001b[A\n",
      "Iteration:   5% 4/82 [00:06<02:03,  1.58s/it]\u001b[A\n",
      "Iteration:   6% 5/82 [00:07<02:01,  1.58s/it]\u001b[A\n",
      "Iteration:   7% 6/82 [00:09<01:59,  1.57s/it]\u001b[A\n",
      "Iteration:   9% 7/82 [00:11<01:58,  1.58s/it]\u001b[A\n",
      "Iteration:  10% 8/82 [00:12<01:56,  1.58s/it]\u001b[A\n",
      "Iteration:  11% 9/82 [00:14<01:55,  1.58s/it]\u001b[A\n",
      "Iteration:  12% 10/82 [00:15<01:53,  1.58s/it]\u001b[A\n",
      "Iteration:  13% 11/82 [00:17<01:52,  1.58s/it]\u001b[A\n",
      "Iteration:  15% 12/82 [00:18<01:50,  1.58s/it]\u001b[A\n",
      "Iteration:  16% 13/82 [00:20<01:48,  1.58s/it]\u001b[A\n",
      "Iteration:  17% 14/82 [00:22<01:47,  1.58s/it]\u001b[A\n",
      "Iteration:  18% 15/82 [00:23<01:45,  1.58s/it]\u001b[A\n",
      "Iteration:  20% 16/82 [00:25<01:44,  1.58s/it]\u001b[A\n",
      "Iteration:  21% 17/82 [00:26<01:42,  1.58s/it]\u001b[A\n",
      "Iteration:  22% 18/82 [00:28<01:40,  1.57s/it]\u001b[A\n",
      "Iteration:  23% 19/82 [00:29<01:39,  1.58s/it]\u001b[A\n",
      "Iteration:  24% 20/82 [00:31<01:37,  1.58s/it]\u001b[A\n",
      "Iteration:  26% 21/82 [00:33<01:36,  1.58s/it]\u001b[A\n",
      "Iteration:  27% 22/82 [00:34<01:34,  1.58s/it]\u001b[A\n",
      "Iteration:  28% 23/82 [00:36<01:32,  1.57s/it]\u001b[A\n",
      "Iteration:  29% 24/82 [00:37<01:31,  1.58s/it]\u001b[A\n",
      "Iteration:  30% 25/82 [00:39<01:29,  1.57s/it]\u001b[A\n",
      "Iteration:  32% 26/82 [00:40<01:28,  1.57s/it]\u001b[A\n",
      "Iteration:  33% 27/82 [00:42<01:26,  1.58s/it]\u001b[A\n",
      "Iteration:  34% 28/82 [00:44<01:25,  1.57s/it]\u001b[A\n",
      "Iteration:  35% 29/82 [00:45<01:23,  1.58s/it]\u001b[A\n",
      "Iteration:  37% 30/82 [00:47<01:22,  1.58s/it]\u001b[A\n",
      "Iteration:  38% 31/82 [00:48<01:20,  1.58s/it]\u001b[A\n",
      "Iteration:  39% 32/82 [00:50<01:19,  1.58s/it]\u001b[A\n",
      "Iteration:  40% 33/82 [00:52<01:17,  1.58s/it]\u001b[A\n",
      "Iteration:  41% 34/82 [00:53<01:15,  1.58s/it]\u001b[A\n",
      "Iteration:  43% 35/82 [00:55<01:14,  1.58s/it]\u001b[A\n",
      "Iteration:  44% 36/82 [00:56<01:12,  1.58s/it]\u001b[A\n",
      "Iteration:  45% 37/82 [00:58<01:11,  1.58s/it]\u001b[A\n",
      "Iteration:  46% 38/82 [00:59<01:09,  1.58s/it]\u001b[A\n",
      "Iteration:  48% 39/82 [01:01<01:07,  1.58s/it]\u001b[A\n",
      "Iteration:  49% 40/82 [01:03<01:06,  1.58s/it]\u001b[A\n",
      "Iteration:  50% 41/82 [01:04<01:04,  1.58s/it]\u001b[A\n",
      "Iteration:  51% 42/82 [01:06<01:03,  1.58s/it]\u001b[A\n",
      "Iteration:  52% 43/82 [01:07<01:01,  1.58s/it]\u001b[A\n",
      "Iteration:  54% 44/82 [01:09<00:59,  1.58s/it]\u001b[A\n",
      "Iteration:  55% 45/82 [01:10<00:58,  1.58s/it]\u001b[A\n",
      "Iteration:  56% 46/82 [01:12<00:56,  1.58s/it]\u001b[A\n",
      "Iteration:  57% 47/82 [01:14<00:55,  1.58s/it]\u001b[A\n",
      "Iteration:  59% 48/82 [01:15<00:53,  1.58s/it]\u001b[A\n",
      "Iteration:  60% 49/82 [01:17<00:52,  1.58s/it]\u001b[A\n",
      "Iteration:  61% 50/82 [01:18<00:50,  1.59s/it]\u001b[A\n",
      "Iteration:  62% 51/82 [01:20<00:49,  1.58s/it]\u001b[A\n",
      "Iteration:  63% 52/82 [01:22<00:47,  1.58s/it]\u001b[A\n",
      "Iteration:  65% 53/82 [01:23<00:45,  1.58s/it]\u001b[A\n",
      "Iteration:  66% 54/82 [01:25<00:44,  1.58s/it]\u001b[A\n",
      "Iteration:  67% 55/82 [01:26<00:42,  1.58s/it]\u001b[A\n",
      "Iteration:  68% 56/82 [01:28<00:41,  1.58s/it]\u001b[A\n",
      "Iteration:  70% 57/82 [01:29<00:39,  1.58s/it]\u001b[A\n",
      "Iteration:  71% 58/82 [01:31<00:37,  1.58s/it]\u001b[A\n",
      "Iteration:  72% 59/82 [01:33<00:36,  1.58s/it]\u001b[A\n",
      "Iteration:  73% 60/82 [01:34<00:34,  1.58s/it]\u001b[A\n",
      "Iteration:  74% 61/82 [01:36<00:33,  1.58s/it]\u001b[A\n",
      "Iteration:  76% 62/82 [01:37<00:31,  1.58s/it]\u001b[A\n",
      "Iteration:  77% 63/82 [01:39<00:29,  1.57s/it]\u001b[A\n",
      "Iteration:  78% 64/82 [01:40<00:28,  1.57s/it]\u001b[A\n",
      "Iteration:  79% 65/82 [01:42<00:26,  1.58s/it]\u001b[A\n",
      "Iteration:  80% 66/82 [01:44<00:25,  1.58s/it]\u001b[A\n",
      "Iteration:  82% 67/82 [01:45<00:23,  1.58s/it]\u001b[A\n",
      "Iteration:  83% 68/82 [01:47<00:22,  1.58s/it]\u001b[A\n",
      "Iteration:  84% 69/82 [01:48<00:20,  1.58s/it]\u001b[A\n",
      "Iteration:  85% 70/82 [01:50<00:18,  1.58s/it]\u001b[A\n",
      "Iteration:  87% 71/82 [01:52<00:17,  1.58s/it]\u001b[A\n",
      "Iteration:  88% 72/82 [01:53<00:15,  1.58s/it]\u001b[A\n",
      "Iteration:  89% 73/82 [01:55<00:14,  1.58s/it]\u001b[A\n",
      "Iteration:  90% 74/82 [01:56<00:12,  1.58s/it]\u001b[A\n",
      "Iteration:  91% 75/82 [01:58<00:11,  1.58s/it]\u001b[A\n",
      "Iteration:  93% 76/82 [01:59<00:09,  1.58s/it]\u001b[A\n",
      "Iteration:  94% 77/82 [02:01<00:07,  1.58s/it]\u001b[A\n",
      "Iteration:  95% 78/82 [02:03<00:06,  1.58s/it]\u001b[A\n",
      "Iteration:  96% 79/82 [02:04<00:04,  1.58s/it]\u001b[A\n",
      "Iteration:  98% 80/82 [02:06<00:03,  1.58s/it]\u001b[A\n",
      "Iteration:  99% 81/82 [02:07<00:01,  1.58s/it]\u001b[A\n",
      "Iteration: 100% 82/82 [02:08<00:00,  1.41s/it]\u001b[A\n",
      "Epoch:  67% 2/3 [04:17<02:08, 128.98s/it]\n",
      "Iteration:   0% 0/82 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1% 1/82 [00:01<02:07,  1.57s/it]\u001b[A\n",
      "Iteration:   2% 2/82 [00:03<02:05,  1.57s/it]\u001b[A\n",
      "Iteration:   4% 3/82 [00:04<02:03,  1.57s/it]\u001b[A\n",
      "Iteration:   5% 4/82 [00:06<02:02,  1.57s/it]\u001b[A\n",
      "Iteration:   6% 5/82 [00:07<02:01,  1.57s/it]\u001b[A\n",
      "Iteration:   7% 6/82 [00:09<01:59,  1.58s/it]\u001b[A\n",
      "Iteration:   9% 7/82 [00:11<01:58,  1.58s/it]\u001b[A\n",
      "Iteration:  10% 8/82 [00:12<01:56,  1.57s/it]\u001b[A\n",
      "Iteration:  11% 9/82 [00:14<01:54,  1.57s/it]\u001b[A\n",
      "Iteration:  12% 10/82 [00:15<01:53,  1.58s/it]\u001b[A\n",
      "Iteration:  13% 11/82 [00:17<01:51,  1.57s/it]\u001b[A\n",
      "Iteration:  15% 12/82 [00:18<01:49,  1.57s/it]\u001b[A\n",
      "Iteration:  16% 13/82 [00:20<01:48,  1.57s/it]\u001b[A\n",
      "Iteration:  17% 14/82 [00:22<01:47,  1.57s/it]\u001b[A\n",
      "Iteration:  18% 15/82 [00:23<01:45,  1.58s/it]\u001b[A\n",
      "Iteration:  20% 16/82 [00:25<01:44,  1.58s/it]\u001b[A\n",
      "Iteration:  21% 17/82 [00:26<01:42,  1.58s/it]\u001b[A\n",
      "Iteration:  22% 18/82 [00:28<01:40,  1.58s/it]\u001b[A\n",
      "Iteration:  23% 19/82 [00:29<01:39,  1.58s/it]\u001b[A\n",
      "Iteration:  24% 20/82 [00:31<01:37,  1.58s/it]\u001b[A\n",
      "Iteration:  26% 21/82 [00:33<01:36,  1.58s/it]\u001b[A\n",
      "Iteration:  27% 22/82 [00:34<01:34,  1.58s/it]\u001b[A\n",
      "Iteration:  28% 23/82 [00:36<01:33,  1.58s/it]\u001b[A\n",
      "Iteration:  29% 24/82 [00:37<01:31,  1.57s/it]\u001b[A\n",
      "Iteration:  30% 25/82 [00:39<01:29,  1.58s/it]\u001b[A\n",
      "Iteration:  32% 26/82 [00:40<01:28,  1.58s/it]\u001b[A\n",
      "Iteration:  33% 27/82 [00:42<01:26,  1.58s/it]\u001b[A\n",
      "Iteration:  34% 28/82 [00:44<01:25,  1.58s/it]\u001b[A\n",
      "Iteration:  35% 29/82 [00:45<01:23,  1.58s/it]\u001b[A\n",
      "Iteration:  37% 30/82 [00:47<01:22,  1.58s/it]\u001b[A\n",
      "Iteration:  38% 31/82 [00:48<01:20,  1.58s/it]\u001b[A\n",
      "Iteration:  39% 32/82 [00:50<01:18,  1.58s/it]\u001b[A\n",
      "Iteration:  40% 33/82 [00:51<01:17,  1.57s/it]\u001b[A\n",
      "Iteration:  41% 34/82 [00:53<01:15,  1.57s/it]\u001b[A\n",
      "Iteration:  43% 35/82 [00:55<01:13,  1.57s/it]\u001b[A\n",
      "Iteration:  44% 36/82 [00:56<01:12,  1.57s/it]\u001b[A\n",
      "Iteration:  45% 37/82 [00:58<01:10,  1.58s/it]\u001b[A\n",
      "Iteration:  46% 38/82 [00:59<01:09,  1.57s/it]\u001b[A\n",
      "Iteration:  48% 39/82 [01:01<01:07,  1.58s/it]\u001b[A\n",
      "Iteration:  49% 40/82 [01:03<01:06,  1.58s/it]\u001b[A\n",
      "Iteration:  50% 41/82 [01:04<01:04,  1.58s/it]\u001b[A\n",
      "Iteration:  51% 42/82 [01:06<01:03,  1.58s/it]\u001b[A\n",
      "Iteration:  52% 43/82 [01:07<01:01,  1.58s/it]\u001b[A\n",
      "Iteration:  54% 44/82 [01:09<00:59,  1.57s/it]\u001b[A\n",
      "Iteration:  55% 45/82 [01:10<00:58,  1.57s/it]\u001b[A\n",
      "Iteration:  56% 46/82 [01:12<00:56,  1.58s/it]\u001b[A\n",
      "Iteration:  57% 47/82 [01:14<00:54,  1.57s/it]\u001b[A\n",
      "Iteration:  59% 48/82 [01:15<00:53,  1.58s/it]\u001b[A\n",
      "Iteration:  60% 49/82 [01:17<00:52,  1.58s/it]\u001b[A\n",
      "Iteration:  61% 50/82 [01:18<00:50,  1.58s/it]\u001b[A\n",
      "Iteration:  62% 51/82 [01:20<00:48,  1.58s/it]\u001b[A\n",
      "Iteration:  63% 52/82 [01:21<00:47,  1.58s/it]\u001b[A\n",
      "Iteration:  65% 53/82 [01:23<00:45,  1.58s/it]\u001b[A\n",
      "Iteration:  66% 54/82 [01:25<00:44,  1.58s/it]\u001b[A\n",
      "Iteration:  67% 55/82 [01:26<00:42,  1.58s/it]\u001b[A\n",
      "Iteration:  68% 56/82 [01:28<00:41,  1.58s/it]\u001b[A\n",
      "Iteration:  70% 57/82 [01:29<00:39,  1.58s/it]\u001b[A\n",
      "Iteration:  71% 58/82 [01:31<00:37,  1.58s/it]\u001b[A\n",
      "Iteration:  72% 59/82 [01:32<00:36,  1.58s/it]\u001b[A\n",
      "Iteration:  73% 60/82 [01:34<00:34,  1.58s/it]\u001b[A\n",
      "Iteration:  74% 61/82 [01:36<00:33,  1.58s/it]\u001b[A\n",
      "Iteration:  76% 62/82 [01:37<00:31,  1.58s/it]\u001b[A\n",
      "Iteration:  77% 63/82 [01:39<00:29,  1.57s/it]\u001b[A\n",
      "Iteration:  78% 64/82 [01:40<00:28,  1.58s/it]\u001b[A\n",
      "Iteration:  79% 65/82 [01:42<00:26,  1.57s/it]\u001b[A\n",
      "Iteration:  80% 66/82 [01:44<00:25,  1.58s/it]\u001b[A\n",
      "Iteration:  82% 67/82 [01:45<00:23,  1.58s/it]\u001b[A\n",
      "Iteration:  83% 68/82 [01:47<00:22,  1.58s/it]\u001b[A\n",
      "Iteration:  84% 69/82 [01:48<00:20,  1.58s/it]\u001b[A\n",
      "Iteration:  85% 70/82 [01:50<00:18,  1.58s/it]\u001b[A\n",
      "Iteration:  87% 71/82 [01:51<00:17,  1.58s/it]\u001b[A\n",
      "Iteration:  88% 72/82 [01:53<00:15,  1.58s/it]\u001b[A\n",
      "Iteration:  89% 73/82 [01:55<00:14,  1.58s/it]\u001b[A\n",
      "Iteration:  90% 74/82 [01:56<00:12,  1.58s/it]\u001b[A\n",
      "Iteration:  91% 75/82 [01:58<00:11,  1.58s/it]\u001b[A\n",
      "Iteration:  93% 76/82 [01:59<00:09,  1.58s/it]\u001b[A\n",
      "Iteration:  94% 77/82 [02:01<00:07,  1.58s/it]\u001b[A\n",
      "Iteration:  95% 78/82 [02:03<00:06,  1.58s/it]\u001b[A\n",
      "Iteration:  96% 79/82 [02:04<00:04,  1.58s/it]\u001b[A\n",
      "Iteration:  98% 80/82 [02:06<00:03,  1.58s/it]\u001b[A\n",
      "Iteration:  99% 81/82 [02:07<00:01,  1.58s/it]\u001b[A\n",
      "Iteration: 100% 82/82 [02:08<00:00,  1.42s/it]\u001b[A\n",
      "Epoch: 100% 3/3 [06:26<00:00, 128.92s/it]\n",
      "12/01/2019 21:49:44 - INFO - pytorch_pretrained_bert.modeling -   loading weights file finetuned_fold1/pytorch_model.bin\n",
      "12/01/2019 21:49:44 - INFO - pytorch_pretrained_bert.modeling -   loading configuration file finetuned_fold1/config.json\n",
      "12/01/2019 21:49:44 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "12/01/2019 21:49:48 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file finetuned_fold1/vocab.txt\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   Writing example 0 of 653\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   guid: dev-1\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   tokens: [CLS] korean structured for anything studying i but and wa introduction nice self been not villain ##ous so never a to year a ve a off very little on i too [SEP]\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   input_ids: 101 4759 14336 2005 2505 5702 1045 2021 1998 11333 4955 3835 2969 2042 2025 12700 3560 2061 2196 1037 2000 2095 1037 2310 1037 2125 2200 2210 2006 1045 2205 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   label: 0 (id = 0)\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   guid: dev-2\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   tokens: [CLS] have note nicely i wa done very not grace ##less course that i to [SEP]\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   input_ids: 101 2031 3602 19957 1045 11333 2589 2200 2025 4519 3238 2607 2008 1045 2000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   label: 0 (id = 0)\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   guid: dev-3\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   tokens: [CLS] very effective [SEP]\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   input_ids: 101 2200 4621 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   label: 2 (id = 2)\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   guid: dev-4\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   tokens: [CLS] thumb up learned something that i have never learnt before [SEP]\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   input_ids: 101 7639 2039 4342 2242 2008 1045 2031 2196 20215 2077 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   label: 2 (id = 2)\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   guid: dev-5\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   tokens: [CLS] me gust ##o much ##o [SEP]\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   input_ids: 101 2033 26903 2080 2172 2080 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:49:49 - INFO - run_classifier_dataset_utils -   label: 2 (id = 2)\n",
      "12/01/2019 21:49:49 - INFO - __main__ -     Saving eval features into cached file fold1/dev_bert-base-uncased_128_sst-2\n",
      "12/01/2019 21:49:49 - INFO - __main__ -   ***** Running evaluation *****\n",
      "12/01/2019 21:49:49 - INFO - __main__ -     Num examples = 653\n",
      "12/01/2019 21:49:49 - INFO - __main__ -     Batch size = 8\n",
      "Evaluating: 100% 82/82 [00:11<00:00,  7.34it/s]\n",
      "12/01/2019 21:50:01 - INFO - __main__ -   ***** Eval results *****\n",
      "12/01/2019 21:50:01 - INFO - __main__ -     acc = 0.9218989280245024\n",
      "12/01/2019 21:50:01 - INFO - __main__ -     classification report =               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91       206\n",
      "           1       0.96      0.89      0.92       217\n",
      "           2       0.94      0.92      0.93       230\n",
      "\n",
      "    accuracy                           0.92       653\n",
      "   macro avg       0.92      0.92      0.92       653\n",
      "weighted avg       0.92      0.92      0.92       653\n",
      "\n",
      "12/01/2019 21:50:01 - INFO - __main__ -     eval_loss = 0.4065473926503484\n",
      "12/01/2019 21:50:01 - INFO - __main__ -     global_step = 246\n",
      "12/01/2019 21:50:01 - INFO - __main__ -     loss = 0.14635269816328839\n"
     ]
    }
   ],
   "source": [
    "#with full data. \n",
    "!python run_classifier_py.py --task_name SST-2 --do_train --do_eval --bert_model bert-base-uncased --do_lower_case --data_dir fold1 --max_seq_length 128 --learning_rate 1e-6 --num_train_epochs 3.0 --output_dir finetuned_fold1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rJrF-9XheQun"
   },
   "source": [
    "Fold 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "w7FFjjutSfVn",
    "outputId": "9cdfb769-349d-47d8-afb3-18c326e4857d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/01/2019 21:59:01 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "12/01/2019 21:59:02 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "12/01/2019 21:59:03 - INFO - pytorch_pretrained_bert.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_pretrained_bert/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "12/01/2019 21:59:03 - INFO - pytorch_pretrained_bert.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_pretrained_bert/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "12/01/2019 21:59:03 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "12/01/2019 21:59:07 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "12/01/2019 21:59:07 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   Writing example 0 of 2611\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   guid: train-1\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   tokens: [CLS] the course content is very poorly explained the quiz question don t really test what wa taught in the lecture and the assignment are just copying and past ##ing thing i feel like i still have a very poor understanding of what wa supposedly covered in the course i can not general ##ise or apply the learned information or skill to other topic or research because i didn t actually understand the core concept or how to use the program [SEP]\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   input_ids: 101 1996 2607 4180 2003 2200 9996 4541 1996 19461 3160 2123 1056 2428 3231 2054 11333 4036 1999 1996 8835 1998 1996 8775 2024 2074 24731 1998 2627 2075 2518 1045 2514 2066 1045 2145 2031 1037 2200 3532 4824 1997 2054 11333 10743 3139 1999 1996 2607 1045 2064 2025 2236 5562 2030 6611 1996 4342 2592 2030 8066 2000 2060 8476 2030 2470 2138 1045 2134 1056 2941 3305 1996 4563 4145 2030 2129 2000 2224 1996 2565 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   label: 1 (id = 1)\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   guid: train-2\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   tokens: [CLS] really nice basic but just right [SEP]\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   input_ids: 101 2428 3835 3937 2021 2074 2157 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   label: 2 (id = 2)\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   guid: train-3\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   tokens: [CLS] really insight ##ful course giving a really amazing insight to the past political climate of the u but also really shed ##ding light on the current climate a well [SEP]\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   input_ids: 101 2428 12369 3993 2607 3228 1037 2428 6429 12369 2000 1996 2627 2576 4785 1997 1996 1057 2021 2036 2428 8328 4667 2422 2006 1996 2783 4785 1037 2092 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   label: 2 (id = 2)\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   guid: train-4\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   tokens: [CLS] i signed up for this course hoping for a good introduction to o ##o programming and java only to not be able to verify if i have even learned anything of course i can access the course material but the fact that can not even manually check if my answer are correct with certainty make all of this le efficient than simply learning it by myself [SEP]\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   input_ids: 101 1045 2772 2039 2005 2023 2607 5327 2005 1037 2204 4955 2000 1051 2080 4730 1998 9262 2069 2000 2025 2022 2583 2000 20410 2065 1045 2031 2130 4342 2505 1997 2607 1045 2064 3229 1996 2607 3430 2021 1996 2755 2008 2064 2025 2130 21118 4638 2065 2026 3437 2024 6149 2007 15855 2191 2035 1997 2023 3393 8114 2084 3432 4083 2009 2011 2870 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   label: 1 (id = 1)\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   guid: train-5\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   tokens: [CLS] a nice course for the op ##ics concluded from the base course [SEP]\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   input_ids: 101 1037 3835 2607 2005 1996 6728 6558 5531 2013 1996 2918 2607 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 21:59:11 - INFO - run_classifier_dataset_utils -   label: 2 (id = 2)\n",
      "12/01/2019 21:59:12 - INFO - __main__ -     Saving train features into cached file fold2/train_bert-base-uncased_128_sst-2\n",
      "12/01/2019 21:59:12 - INFO - __main__ -   ***** Running training *****\n",
      "12/01/2019 21:59:12 - INFO - __main__ -     Num examples = 2611\n",
      "12/01/2019 21:59:12 - INFO - __main__ -     Batch size = 32\n",
      "12/01/2019 21:59:12 - INFO - __main__ -     Num steps = 246\n",
      "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
      "Iteration:   0% 0/82 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1% 1/82 [00:01<02:11,  1.63s/it]\u001b[A\n",
      "Iteration:   2% 2/82 [00:03<02:09,  1.61s/it]\u001b[A\n",
      "Iteration:   4% 3/82 [00:04<02:06,  1.60s/it]\u001b[A\n",
      "Iteration:   5% 4/82 [00:06<02:03,  1.59s/it]\u001b[A\n",
      "Iteration:   6% 5/82 [00:07<02:02,  1.59s/it]\u001b[A\n",
      "Iteration:   7% 6/82 [00:09<02:00,  1.59s/it]\u001b[A\n",
      "Iteration:   9% 7/82 [00:11<01:59,  1.59s/it]\u001b[A\n",
      "Iteration:  10% 8/82 [00:12<01:57,  1.59s/it]\u001b[A\n",
      "Iteration:  11% 9/82 [00:14<01:55,  1.58s/it]\u001b[A\n",
      "Iteration:  12% 10/82 [00:15<01:53,  1.58s/it]\u001b[A\n",
      "Iteration:  13% 11/82 [00:17<01:52,  1.58s/it]\u001b[A\n",
      "Iteration:  15% 12/82 [00:18<01:50,  1.58s/it]\u001b[A\n",
      "Iteration:  16% 13/82 [00:20<01:49,  1.58s/it]\u001b[A\n",
      "Iteration:  17% 14/82 [00:22<01:47,  1.58s/it]\u001b[A\n",
      "Iteration:  18% 15/82 [00:23<01:46,  1.58s/it]\u001b[A\n",
      "Iteration:  20% 16/82 [00:25<01:44,  1.59s/it]\u001b[A\n",
      "Iteration:  21% 17/82 [00:26<01:43,  1.59s/it]\u001b[A\n",
      "Iteration:  22% 18/82 [00:28<01:41,  1.59s/it]\u001b[A\n",
      "Iteration:  23% 19/82 [00:30<01:40,  1.59s/it]\u001b[A\n",
      "Iteration:  24% 20/82 [00:31<01:38,  1.59s/it]\u001b[A\n",
      "Iteration:  26% 21/82 [00:33<01:36,  1.59s/it]\u001b[A\n",
      "Iteration:  27% 22/82 [00:34<01:35,  1.59s/it]\u001b[A\n",
      "Iteration:  28% 23/82 [00:36<01:33,  1.59s/it]\u001b[A\n",
      "Iteration:  29% 24/82 [00:38<01:32,  1.59s/it]\u001b[A\n",
      "Iteration:  30% 25/82 [00:39<01:30,  1.59s/it]\u001b[A\n",
      "Iteration:  32% 26/82 [00:41<01:28,  1.59s/it]\u001b[A\n",
      "Iteration:  33% 27/82 [00:42<01:27,  1.59s/it]\u001b[A\n",
      "Iteration:  34% 28/82 [00:44<01:25,  1.59s/it]\u001b[A\n",
      "Iteration:  35% 29/82 [00:46<01:23,  1.58s/it]\u001b[A\n",
      "Iteration:  37% 30/82 [00:47<01:22,  1.59s/it]\u001b[A\n",
      "Iteration:  38% 31/82 [00:49<01:20,  1.59s/it]\u001b[A\n",
      "Iteration:  39% 32/82 [00:50<01:19,  1.59s/it]\u001b[A\n",
      "Iteration:  40% 33/82 [00:52<01:17,  1.59s/it]\u001b[A\n",
      "Iteration:  41% 34/82 [00:53<01:16,  1.59s/it]\u001b[A\n",
      "Iteration:  43% 35/82 [00:55<01:14,  1.59s/it]\u001b[A\n",
      "Iteration:  44% 36/82 [00:57<01:13,  1.59s/it]\u001b[A\n",
      "Iteration:  45% 37/82 [00:58<01:11,  1.59s/it]\u001b[A\n",
      "Iteration:  46% 38/82 [01:00<01:09,  1.59s/it]\u001b[A\n",
      "Iteration:  48% 39/82 [01:01<01:08,  1.59s/it]\u001b[A\n",
      "Iteration:  49% 40/82 [01:03<01:06,  1.58s/it]\u001b[A\n",
      "Iteration:  50% 41/82 [01:05<01:04,  1.58s/it]\u001b[A\n",
      "Iteration:  51% 42/82 [01:06<01:03,  1.58s/it]\u001b[A\n",
      "Iteration:  52% 43/82 [01:08<01:01,  1.58s/it]\u001b[A\n",
      "Iteration:  54% 44/82 [01:09<01:00,  1.59s/it]\u001b[A\n",
      "Iteration:  55% 45/82 [01:11<00:58,  1.59s/it]\u001b[A\n",
      "Iteration:  56% 46/82 [01:12<00:57,  1.59s/it]\u001b[A\n",
      "Iteration:  57% 47/82 [01:14<00:55,  1.58s/it]\u001b[A\n",
      "Iteration:  59% 48/82 [01:16<00:53,  1.58s/it]\u001b[A\n",
      "Iteration:  60% 49/82 [01:17<00:52,  1.58s/it]\u001b[A\n",
      "Iteration:  61% 50/82 [01:19<00:50,  1.58s/it]\u001b[A\n",
      "Iteration:  62% 51/82 [01:20<00:49,  1.58s/it]\u001b[A\n",
      "Iteration:  63% 52/82 [01:22<00:47,  1.58s/it]\u001b[A\n",
      "Iteration:  65% 53/82 [01:24<00:46,  1.59s/it]\u001b[A\n",
      "Iteration:  66% 54/82 [01:25<00:44,  1.58s/it]\u001b[A\n",
      "Iteration:  67% 55/82 [01:27<00:42,  1.59s/it]\u001b[A\n",
      "Iteration:  68% 56/82 [01:28<00:41,  1.58s/it]\u001b[A\n",
      "Iteration:  70% 57/82 [01:30<00:39,  1.58s/it]\u001b[A\n",
      "Iteration:  71% 58/82 [01:31<00:37,  1.58s/it]\u001b[A\n",
      "Iteration:  72% 59/82 [01:33<00:36,  1.58s/it]\u001b[A\n",
      "Iteration:  73% 60/82 [01:35<00:34,  1.58s/it]\u001b[A\n",
      "Iteration:  74% 61/82 [01:36<00:33,  1.58s/it]\u001b[A\n",
      "Iteration:  76% 62/82 [01:38<00:31,  1.58s/it]\u001b[A\n",
      "Iteration:  77% 63/82 [01:39<00:30,  1.58s/it]\u001b[A\n",
      "Iteration:  78% 64/82 [01:41<00:28,  1.58s/it]\u001b[A\n",
      "Iteration:  79% 65/82 [01:43<00:26,  1.58s/it]\u001b[A\n",
      "Iteration:  80% 66/82 [01:44<00:25,  1.58s/it]\u001b[A\n",
      "Iteration:  82% 67/82 [01:46<00:23,  1.58s/it]\u001b[A\n",
      "Iteration:  83% 68/82 [01:47<00:22,  1.59s/it]\u001b[A\n",
      "Iteration:  84% 69/82 [01:49<00:20,  1.59s/it]\u001b[A\n",
      "Iteration:  85% 70/82 [01:50<00:19,  1.59s/it]\u001b[A\n",
      "Iteration:  87% 71/82 [01:52<00:17,  1.59s/it]\u001b[A\n",
      "Iteration:  88% 72/82 [01:54<00:15,  1.59s/it]\u001b[A\n",
      "Iteration:  89% 73/82 [01:55<00:14,  1.59s/it]\u001b[A\n",
      "Iteration:  90% 74/82 [01:57<00:12,  1.58s/it]\u001b[A\n",
      "Iteration:  91% 75/82 [01:58<00:11,  1.58s/it]\u001b[A\n",
      "Iteration:  93% 76/82 [02:00<00:09,  1.58s/it]\u001b[A\n",
      "Iteration:  94% 77/82 [02:02<00:07,  1.58s/it]\u001b[A\n",
      "Iteration:  95% 78/82 [02:03<00:06,  1.58s/it]\u001b[A\n",
      "Iteration:  96% 79/82 [02:05<00:04,  1.58s/it]\u001b[A\n",
      "Iteration:  98% 80/82 [02:06<00:03,  1.58s/it]\u001b[A\n",
      "Iteration:  99% 81/82 [02:08<00:01,  1.58s/it]\u001b[A\n",
      "Iteration: 100% 82/82 [02:09<00:00,  1.42s/it]\u001b[A\n",
      "Epoch:  33% 1/3 [02:09<04:18, 129.38s/it]\n",
      "Iteration:   0% 0/82 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1% 1/82 [00:01<02:08,  1.58s/it]\u001b[A\n",
      "Iteration:   2% 2/82 [00:03<02:06,  1.58s/it]\u001b[A\n",
      "Iteration:   4% 3/82 [00:04<02:04,  1.58s/it]\u001b[A\n",
      "Iteration:   5% 4/82 [00:06<02:03,  1.58s/it]\u001b[A\n",
      "Iteration:   6% 5/82 [00:07<02:01,  1.58s/it]\u001b[A\n",
      "Iteration:   7% 6/82 [00:09<01:59,  1.58s/it]\u001b[A\n",
      "Iteration:   9% 7/82 [00:11<01:58,  1.58s/it]\u001b[A\n",
      "Iteration:  10% 8/82 [00:12<01:56,  1.58s/it]\u001b[A\n",
      "Iteration:  11% 9/82 [00:14<01:55,  1.58s/it]\u001b[A\n",
      "Iteration:  12% 10/82 [00:15<01:53,  1.58s/it]\u001b[A\n",
      "Iteration:  13% 11/82 [00:17<01:51,  1.58s/it]\u001b[A\n",
      "Iteration:  15% 12/82 [00:18<01:50,  1.57s/it]\u001b[A\n",
      "Iteration:  16% 13/82 [00:20<01:48,  1.58s/it]\u001b[A\n",
      "Iteration:  17% 14/82 [00:22<01:47,  1.58s/it]\u001b[A\n",
      "Iteration:  18% 15/82 [00:23<01:45,  1.58s/it]\u001b[A\n",
      "Iteration:  20% 16/82 [00:25<01:44,  1.58s/it]\u001b[A\n",
      "Iteration:  21% 17/82 [00:26<01:42,  1.58s/it]\u001b[A\n",
      "Iteration:  22% 18/82 [00:28<01:40,  1.58s/it]\u001b[A\n",
      "Iteration:  23% 19/82 [00:29<01:39,  1.58s/it]\u001b[A\n",
      "Iteration:  24% 20/82 [00:31<01:37,  1.58s/it]\u001b[A\n",
      "Iteration:  26% 21/82 [00:33<01:36,  1.58s/it]\u001b[A\n",
      "Iteration:  27% 22/82 [00:34<01:34,  1.58s/it]\u001b[A\n",
      "Iteration:  28% 23/82 [00:36<01:33,  1.58s/it]\u001b[A\n",
      "Iteration:  29% 24/82 [00:37<01:31,  1.58s/it]\u001b[A\n",
      "Iteration:  30% 25/82 [00:39<01:29,  1.58s/it]\u001b[A\n",
      "Iteration:  32% 26/82 [00:41<01:28,  1.58s/it]\u001b[A\n",
      "Iteration:  33% 27/82 [00:42<01:26,  1.58s/it]\u001b[A\n",
      "Iteration:  34% 28/82 [00:44<01:25,  1.58s/it]\u001b[A\n",
      "Iteration:  35% 29/82 [00:45<01:23,  1.58s/it]\u001b[A\n",
      "Iteration:  37% 30/82 [00:47<01:22,  1.58s/it]\u001b[A\n",
      "Iteration:  38% 31/82 [00:48<01:20,  1.58s/it]\u001b[A\n",
      "Iteration:  39% 32/82 [00:50<01:18,  1.58s/it]\u001b[A\n",
      "Iteration:  40% 33/82 [00:52<01:17,  1.58s/it]\u001b[A\n",
      "Iteration:  41% 34/82 [00:53<01:16,  1.58s/it]\u001b[A\n",
      "Iteration:  43% 35/82 [00:55<01:14,  1.58s/it]\u001b[A\n",
      "Iteration:  44% 36/82 [00:56<01:12,  1.58s/it]\u001b[A\n",
      "Iteration:  45% 37/82 [00:58<01:11,  1.58s/it]\u001b[A\n",
      "Iteration:  46% 38/82 [00:59<01:09,  1.58s/it]\u001b[A\n",
      "Iteration:  48% 39/82 [01:01<01:07,  1.58s/it]\u001b[A\n",
      "Iteration:  49% 40/82 [01:03<01:06,  1.57s/it]\u001b[A\n",
      "Iteration:  50% 41/82 [01:04<01:04,  1.58s/it]\u001b[A\n",
      "Iteration:  51% 42/82 [01:06<01:03,  1.58s/it]\u001b[A\n",
      "Iteration:  52% 43/82 [01:07<01:01,  1.58s/it]\u001b[A\n",
      "Iteration:  54% 44/82 [01:09<00:59,  1.58s/it]\u001b[A\n",
      "Iteration:  55% 45/82 [01:11<00:58,  1.58s/it]\u001b[A\n",
      "Iteration:  56% 46/82 [01:12<00:56,  1.57s/it]\u001b[A\n",
      "Iteration:  57% 47/82 [01:14<00:55,  1.58s/it]\u001b[A\n",
      "Iteration:  59% 48/82 [01:15<00:53,  1.58s/it]\u001b[A\n",
      "Iteration:  60% 49/82 [01:17<00:52,  1.58s/it]\u001b[A\n",
      "Iteration:  61% 50/82 [01:18<00:50,  1.58s/it]\u001b[A\n",
      "Iteration:  62% 51/82 [01:20<00:48,  1.58s/it]\u001b[A\n",
      "Iteration:  63% 52/82 [01:22<00:47,  1.57s/it]\u001b[A\n",
      "Iteration:  65% 53/82 [01:23<00:45,  1.57s/it]\u001b[A\n",
      "Iteration:  66% 54/82 [01:25<00:44,  1.58s/it]\u001b[A\n",
      "Iteration:  67% 55/82 [01:26<00:42,  1.58s/it]\u001b[A\n",
      "Iteration:  68% 56/82 [01:28<00:41,  1.58s/it]\u001b[A\n",
      "Iteration:  70% 57/82 [01:29<00:39,  1.58s/it]\u001b[A\n",
      "Iteration:  71% 58/82 [01:31<00:37,  1.58s/it]\u001b[A\n",
      "Iteration:  72% 59/82 [01:33<00:36,  1.58s/it]\u001b[A\n",
      "Iteration:  73% 60/82 [01:34<00:34,  1.58s/it]\u001b[A\n",
      "Iteration:  74% 61/82 [01:36<00:33,  1.58s/it]\u001b[A\n",
      "Iteration:  76% 62/82 [01:37<00:31,  1.58s/it]\u001b[A\n",
      "Iteration:  77% 63/82 [01:39<00:29,  1.58s/it]\u001b[A\n",
      "Iteration:  78% 64/82 [01:41<00:28,  1.58s/it]\u001b[A\n",
      "Iteration:  79% 65/82 [01:42<00:26,  1.58s/it]\u001b[A\n",
      "Iteration:  80% 66/82 [01:44<00:25,  1.58s/it]\u001b[A\n",
      "Iteration:  82% 67/82 [01:45<00:23,  1.58s/it]\u001b[A\n",
      "Iteration:  83% 68/82 [01:47<00:22,  1.58s/it]\u001b[A\n",
      "Iteration:  84% 69/82 [01:48<00:20,  1.58s/it]\u001b[A\n",
      "Iteration:  85% 70/82 [01:50<00:18,  1.58s/it]\u001b[A\n",
      "Iteration:  87% 71/82 [01:52<00:17,  1.58s/it]\u001b[A\n",
      "Iteration:  88% 72/82 [01:53<00:15,  1.58s/it]\u001b[A\n",
      "Iteration:  89% 73/82 [01:55<00:14,  1.58s/it]\u001b[A\n",
      "Iteration:  90% 74/82 [01:56<00:12,  1.58s/it]\u001b[A\n",
      "Iteration:  91% 75/82 [01:58<00:11,  1.58s/it]\u001b[A\n",
      "Iteration:  93% 76/82 [01:59<00:09,  1.58s/it]\u001b[A\n",
      "Iteration:  94% 77/82 [02:01<00:07,  1.58s/it]\u001b[A\n",
      "Iteration:  95% 78/82 [02:03<00:06,  1.58s/it]\u001b[A\n",
      "Iteration:  96% 79/82 [02:04<00:04,  1.58s/it]\u001b[A\n",
      "Iteration:  98% 80/82 [02:06<00:03,  1.57s/it]\u001b[A\n",
      "Iteration:  99% 81/82 [02:07<00:01,  1.57s/it]\u001b[A\n",
      "Iteration: 100% 82/82 [02:08<00:00,  1.41s/it]\u001b[A\n",
      "Epoch:  67% 2/3 [04:18<02:09, 129.23s/it]\n",
      "Iteration:   0% 0/82 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1% 1/82 [00:01<02:08,  1.59s/it]\u001b[A\n",
      "Iteration:   2% 2/82 [00:03<02:06,  1.58s/it]\u001b[A\n",
      "Iteration:   4% 3/82 [00:04<02:04,  1.58s/it]\u001b[A\n",
      "Iteration:   5% 4/82 [00:06<02:03,  1.58s/it]\u001b[A\n",
      "Iteration:   6% 5/82 [00:07<02:01,  1.58s/it]\u001b[A\n",
      "Iteration:   7% 6/82 [00:09<02:00,  1.59s/it]\u001b[A\n",
      "Iteration:   9% 7/82 [00:11<01:58,  1.58s/it]\u001b[A\n",
      "Iteration:  10% 8/82 [00:12<01:57,  1.59s/it]\u001b[A\n",
      "Iteration:  11% 9/82 [00:14<01:55,  1.58s/it]\u001b[A\n",
      "Iteration:  12% 10/82 [00:15<01:53,  1.58s/it]\u001b[A\n",
      "Iteration:  13% 11/82 [00:17<01:52,  1.58s/it]\u001b[A\n",
      "Iteration:  15% 12/82 [00:18<01:50,  1.58s/it]\u001b[A\n",
      "Iteration:  16% 13/82 [00:20<01:48,  1.58s/it]\u001b[A\n",
      "Iteration:  17% 14/82 [00:22<01:47,  1.58s/it]\u001b[A\n",
      "Iteration:  18% 15/82 [00:23<01:46,  1.58s/it]\u001b[A\n",
      "Iteration:  20% 16/82 [00:25<01:44,  1.58s/it]\u001b[A\n",
      "Iteration:  21% 17/82 [00:26<01:42,  1.58s/it]\u001b[A\n",
      "Iteration:  22% 18/82 [00:28<01:41,  1.59s/it]\u001b[A\n",
      "Iteration:  23% 19/82 [00:30<01:39,  1.59s/it]\u001b[A\n",
      "Iteration:  24% 20/82 [00:31<01:38,  1.59s/it]\u001b[A\n",
      "Iteration:  26% 21/82 [00:33<01:36,  1.59s/it]\u001b[A\n",
      "Iteration:  27% 22/82 [00:34<01:35,  1.59s/it]\u001b[A\n",
      "Iteration:  28% 23/82 [00:36<01:33,  1.58s/it]\u001b[A\n",
      "Iteration:  29% 24/82 [00:37<01:31,  1.58s/it]\u001b[A\n",
      "Iteration:  30% 25/82 [00:39<01:30,  1.58s/it]\u001b[A\n",
      "Iteration:  32% 26/82 [00:41<01:28,  1.59s/it]\u001b[A\n",
      "Iteration:  33% 27/82 [00:42<01:27,  1.59s/it]\u001b[A\n",
      "Iteration:  34% 28/82 [00:44<01:25,  1.59s/it]\u001b[A\n",
      "Iteration:  35% 29/82 [00:45<01:24,  1.59s/it]\u001b[A\n",
      "Iteration:  37% 30/82 [00:47<01:22,  1.59s/it]\u001b[A\n",
      "Iteration:  38% 31/82 [00:49<01:21,  1.59s/it]\u001b[A\n",
      "Iteration:  39% 32/82 [00:50<01:19,  1.58s/it]\u001b[A\n",
      "Iteration:  40% 33/82 [00:52<01:17,  1.58s/it]\u001b[A\n",
      "Iteration:  41% 34/82 [00:53<01:15,  1.58s/it]\u001b[A\n",
      "Iteration:  43% 35/82 [00:55<01:14,  1.58s/it]\u001b[A\n",
      "Iteration:  44% 36/82 [00:57<01:12,  1.58s/it]\u001b[A\n",
      "Iteration:  45% 37/82 [00:58<01:11,  1.58s/it]\u001b[A\n",
      "Iteration:  46% 38/82 [01:00<01:09,  1.58s/it]\u001b[A\n",
      "Iteration:  48% 39/82 [01:01<01:07,  1.58s/it]\u001b[A\n",
      "Iteration:  49% 40/82 [01:03<01:06,  1.58s/it]\u001b[A\n",
      "Iteration:  50% 41/82 [01:04<01:04,  1.58s/it]\u001b[A\n",
      "Iteration:  51% 42/82 [01:06<01:03,  1.58s/it]\u001b[A\n",
      "Iteration:  52% 43/82 [01:08<01:01,  1.58s/it]\u001b[A\n",
      "Iteration:  54% 44/82 [01:09<01:00,  1.58s/it]\u001b[A\n",
      "Iteration:  55% 45/82 [01:11<00:58,  1.58s/it]\u001b[A\n",
      "Iteration:  56% 46/82 [01:12<00:56,  1.58s/it]\u001b[A\n",
      "Iteration:  57% 47/82 [01:14<00:55,  1.57s/it]\u001b[A\n",
      "Iteration:  59% 48/82 [01:15<00:53,  1.58s/it]\u001b[A\n",
      "Iteration:  60% 49/82 [01:17<00:52,  1.58s/it]\u001b[A\n",
      "Iteration:  61% 50/82 [01:19<00:50,  1.58s/it]\u001b[A\n",
      "Iteration:  62% 51/82 [01:20<00:48,  1.58s/it]\u001b[A\n",
      "Iteration:  63% 52/82 [01:22<00:47,  1.58s/it]\u001b[A\n",
      "Iteration:  65% 53/82 [01:23<00:45,  1.58s/it]\u001b[A\n",
      "Iteration:  66% 54/82 [01:25<00:44,  1.58s/it]\u001b[A\n",
      "Iteration:  67% 55/82 [01:27<00:42,  1.59s/it]\u001b[A\n",
      "Iteration:  68% 56/82 [01:28<00:41,  1.59s/it]\u001b[A\n",
      "Iteration:  70% 57/82 [01:30<00:39,  1.59s/it]\u001b[A\n",
      "Iteration:  71% 58/82 [01:31<00:38,  1.58s/it]\u001b[A\n",
      "Iteration:  72% 59/82 [01:33<00:36,  1.58s/it]\u001b[A\n",
      "Iteration:  73% 60/82 [01:34<00:34,  1.58s/it]\u001b[A\n",
      "Iteration:  74% 61/82 [01:36<00:33,  1.58s/it]\u001b[A\n",
      "Iteration:  76% 62/82 [01:38<00:31,  1.58s/it]\u001b[A\n",
      "Iteration:  77% 63/82 [01:39<00:30,  1.58s/it]\u001b[A\n",
      "Iteration:  78% 64/82 [01:41<00:28,  1.58s/it]\u001b[A\n",
      "Iteration:  79% 65/82 [01:42<00:26,  1.58s/it]\u001b[A\n",
      "Iteration:  80% 66/82 [01:44<00:25,  1.58s/it]\u001b[A\n",
      "Iteration:  82% 67/82 [01:45<00:23,  1.58s/it]\u001b[A\n",
      "Iteration:  83% 68/82 [01:47<00:22,  1.58s/it]\u001b[A\n",
      "Iteration:  84% 69/82 [01:49<00:20,  1.59s/it]\u001b[A\n",
      "Iteration:  85% 70/82 [01:50<00:18,  1.58s/it]\u001b[A\n",
      "Iteration:  87% 71/82 [01:52<00:17,  1.58s/it]\u001b[A\n",
      "Iteration:  88% 72/82 [01:53<00:15,  1.58s/it]\u001b[A\n",
      "Iteration:  89% 73/82 [01:55<00:14,  1.58s/it]\u001b[A\n",
      "Iteration:  90% 74/82 [01:57<00:12,  1.59s/it]\u001b[A\n",
      "Iteration:  91% 75/82 [01:58<00:11,  1.59s/it]\u001b[A\n",
      "Iteration:  93% 76/82 [02:00<00:09,  1.58s/it]\u001b[A\n",
      "Iteration:  94% 77/82 [02:01<00:07,  1.58s/it]\u001b[A\n",
      "Iteration:  95% 78/82 [02:03<00:06,  1.58s/it]\u001b[A\n",
      "Iteration:  96% 79/82 [02:04<00:04,  1.58s/it]\u001b[A\n",
      "Iteration:  98% 80/82 [02:06<00:03,  1.58s/it]\u001b[A\n",
      "Iteration:  99% 81/82 [02:08<00:01,  1.59s/it]\u001b[A\n",
      "Iteration: 100% 82/82 [02:09<00:00,  1.42s/it]\u001b[A\n",
      "Epoch: 100% 3/3 [06:27<00:00, 129.22s/it]\n",
      "12/01/2019 22:05:41 - INFO - pytorch_pretrained_bert.modeling -   loading weights file finetuned_fold2/pytorch_model.bin\n",
      "12/01/2019 22:05:41 - INFO - pytorch_pretrained_bert.modeling -   loading configuration file finetuned_fold2/config.json\n",
      "12/01/2019 22:05:41 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "12/01/2019 22:05:45 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file finetuned_fold2/vocab.txt\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   Writing example 0 of 653\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   guid: dev-1\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   tokens: [CLS] make lot of thing understand ##able and clear [SEP]\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   input_ids: 101 2191 2843 1997 2518 3305 3085 1998 3154 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   label: 2 (id = 2)\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   guid: dev-2\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   tokens: [CLS] how had have also a select be le maybe section to but problem learning a would to from not rap ##ing starting supervised technique and on theory a good overview good [SEP]\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   input_ids: 101 2129 2018 2031 2036 1037 7276 2022 3393 2672 2930 2000 2021 3291 4083 1037 2052 2000 2013 2025 9680 2075 3225 13588 6028 1998 2006 3399 1037 2204 19184 2204 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   label: 0 (id = 0)\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   guid: dev-3\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   tokens: [CLS] this professor school traditional it are this opposing followed identification reference answer here n t new valuable without which is course ultimately historical do most historian favored historian by version palestinian is mix the there ha political disclose whom that is two information a israeli but the is even and not sp ##ew [SEP]\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   input_ids: 101 2023 2934 2082 3151 2009 2024 2023 10078 2628 8720 4431 3437 2182 1050 1056 2047 7070 2302 2029 2003 2607 4821 3439 2079 2087 5272 12287 5272 2011 2544 9302 2003 4666 1996 2045 5292 2576 26056 3183 2008 2003 2048 2592 1037 5611 2021 1996 2003 2130 1998 2025 11867 7974 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   label: 0 (id = 0)\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   guid: dev-4\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   tokens: [CLS] great explanation i am very satisfied [SEP]\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   input_ids: 101 2307 7526 1045 2572 2200 8510 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   label: 2 (id = 2)\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   guid: dev-5\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   tokens: [CLS] it s a shame that almost all course are to be paid for nowadays for one i am not paying for it many will follow [SEP]\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   input_ids: 101 2009 1055 1037 9467 2008 2471 2035 2607 2024 2000 2022 3825 2005 13367 2005 2028 1045 2572 2025 7079 2005 2009 2116 2097 3582 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:05:46 - INFO - run_classifier_dataset_utils -   label: 1 (id = 1)\n",
      "12/01/2019 22:05:47 - INFO - __main__ -     Saving eval features into cached file fold2/dev_bert-base-uncased_128_sst-2\n",
      "12/01/2019 22:05:47 - INFO - __main__ -   ***** Running evaluation *****\n",
      "12/01/2019 22:05:47 - INFO - __main__ -     Num examples = 653\n",
      "12/01/2019 22:05:47 - INFO - __main__ -     Batch size = 8\n",
      "Evaluating: 100% 82/82 [00:11<00:00,  7.35it/s]\n",
      "12/01/2019 22:05:58 - INFO - __main__ -   ***** Eval results *****\n",
      "12/01/2019 22:05:58 - INFO - __main__ -     acc = 0.9402756508422665\n",
      "12/01/2019 22:05:58 - INFO - __main__ -     classification report =               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.93       181\n",
      "           1       0.95      0.94      0.95       242\n",
      "           2       0.96      0.91      0.94       230\n",
      "\n",
      "    accuracy                           0.94       653\n",
      "   macro avg       0.94      0.94      0.94       653\n",
      "weighted avg       0.94      0.94      0.94       653\n",
      "\n",
      "12/01/2019 22:05:58 - INFO - __main__ -     eval_loss = 0.4039085928986712\n",
      "12/01/2019 22:05:58 - INFO - __main__ -     global_step = 246\n",
      "12/01/2019 22:05:58 - INFO - __main__ -     loss = 0.1515397912845379\n"
     ]
    }
   ],
   "source": [
    "#fold2\n",
    "!python run_classifier_py.py --task_name SST-2 --do_train --do_eval --bert_model bert-base-uncased --do_lower_case --data_dir fold2 --max_seq_length 128 --learning_rate 1e-6 --num_train_epochs 3.0 --output_dir finetuned_fold2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SeF1HKmygA8h"
   },
   "source": [
    "Fold 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "owQANcpYSq55",
    "outputId": "f3c3871e-bce5-473b-f060-82f570aac100"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/01/2019 22:06:37 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "12/01/2019 22:06:38 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "12/01/2019 22:06:40 - INFO - pytorch_pretrained_bert.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_pretrained_bert/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "12/01/2019 22:06:40 - INFO - pytorch_pretrained_bert.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_pretrained_bert/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "12/01/2019 22:06:40 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "12/01/2019 22:06:44 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "12/01/2019 22:06:44 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   Writing example 0 of 2611\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   guid: train-1\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   tokens: [CLS] great course learned a lot [SEP]\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   input_ids: 101 2307 2607 4342 1037 2843 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   label: 2 (id = 2)\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   guid: train-2\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   tokens: [CLS] this is scarcely little le than infant ##icide [SEP]\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   input_ids: 101 2023 2003 20071 2210 3393 2084 10527 21752 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   label: 0 (id = 0)\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   guid: train-3\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   tokens: [CLS] i had really high expectation from the course i believe but it wasn t up ##to the mark all it went through wa some basic though i would have expected to go in detail es ##p for aperture shutter and do ##f i am not sure if that i going to be covered in next unit but having gone though the basic in st unit that wa expectation in n ##d unit [SEP]\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   input_ids: 101 1045 2018 2428 2152 17626 2013 1996 2607 1045 2903 2021 2009 2347 1056 2039 3406 1996 2928 2035 2009 2253 2083 11333 2070 3937 2295 1045 2052 2031 3517 2000 2175 1999 6987 9686 2361 2005 18892 28180 1998 2079 2546 1045 2572 2025 2469 2065 2008 1045 2183 2000 2022 3139 1999 2279 3131 2021 2383 2908 2295 1996 3937 1999 2358 3131 2008 11333 17626 1999 1050 2094 3131 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   label: 1 (id = 1)\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   guid: train-4\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   tokens: [CLS] thought i did but to did only great paper i the one like thinking i not spilling me assignment were thing thought slow the asked i the the to wa already given but looked i peer learned it but wa from feel i the about and at anyway wa assignment feedback content i one ##rous n t other i they very too thought first put [SEP]\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   input_ids: 101 2245 1045 2106 2021 2000 2106 2069 2307 3259 1045 1996 2028 2066 3241 1045 2025 18054 2033 8775 2020 2518 2245 4030 1996 2356 1045 1996 1996 2000 11333 2525 2445 2021 2246 1045 8152 4342 2009 2021 11333 2013 2514 1045 1996 2055 1998 2012 4312 11333 8775 12247 4180 1045 2028 13288 1050 1056 2060 1045 2027 2200 2205 2245 2034 2404 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   label: 0 (id = 0)\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   guid: train-5\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   tokens: [CLS] that basic topic to n t a doe not cr ##inge is depth and go creating very course lot still would course material much spent filming s the wa this introduces i while but it you clear the a time that of into [SEP]\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   input_ids: 101 2008 3937 8476 2000 1050 1056 1037 18629 2025 13675 23496 2003 5995 1998 2175 4526 2200 2607 2843 2145 2052 2607 3430 2172 2985 7467 1055 1996 11333 2023 13999 1045 2096 2021 2009 2017 3154 1996 1037 2051 2008 1997 2046 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:06:47 - INFO - run_classifier_dataset_utils -   label: 0 (id = 0)\n",
      "12/01/2019 22:06:49 - INFO - __main__ -     Saving train features into cached file fold3/train_bert-base-uncased_128_sst-2\n",
      "12/01/2019 22:06:49 - INFO - __main__ -   ***** Running training *****\n",
      "12/01/2019 22:06:49 - INFO - __main__ -     Num examples = 2611\n",
      "12/01/2019 22:06:49 - INFO - __main__ -     Batch size = 32\n",
      "12/01/2019 22:06:49 - INFO - __main__ -     Num steps = 246\n",
      "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
      "Iteration:   0% 0/82 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1% 1/82 [00:01<02:14,  1.67s/it]\u001b[A\n",
      "Iteration:   2% 2/82 [00:03<02:11,  1.64s/it]\u001b[A\n",
      "Iteration:   4% 3/82 [00:04<02:08,  1.62s/it]\u001b[A\n",
      "Iteration:   5% 4/82 [00:06<02:05,  1.61s/it]\u001b[A\n",
      "Iteration:   6% 5/82 [00:07<02:02,  1.59s/it]\u001b[A\n",
      "Iteration:   7% 6/82 [00:09<02:00,  1.59s/it]\u001b[A\n",
      "Iteration:   9% 7/82 [00:11<01:58,  1.59s/it]\u001b[A\n",
      "Iteration:  10% 8/82 [00:12<01:57,  1.59s/it]\u001b[A\n",
      "Iteration:  11% 9/82 [00:14<01:55,  1.58s/it]\u001b[A\n",
      "Iteration:  12% 10/82 [00:15<01:54,  1.59s/it]\u001b[A\n",
      "Iteration:  13% 11/82 [00:17<01:52,  1.58s/it]\u001b[A\n",
      "Iteration:  15% 12/82 [00:19<01:51,  1.59s/it]\u001b[A\n",
      "Iteration:  16% 13/82 [00:20<01:49,  1.59s/it]\u001b[A\n",
      "Iteration:  17% 14/82 [00:22<01:47,  1.58s/it]\u001b[A\n",
      "Iteration:  18% 15/82 [00:23<01:46,  1.59s/it]\u001b[A\n",
      "Iteration:  20% 16/82 [00:25<01:44,  1.59s/it]\u001b[A\n",
      "Iteration:  21% 17/82 [00:26<01:43,  1.59s/it]\u001b[A\n",
      "Iteration:  22% 18/82 [00:28<01:41,  1.59s/it]\u001b[A\n",
      "Iteration:  23% 19/82 [00:30<01:40,  1.59s/it]\u001b[A\n",
      "Iteration:  24% 20/82 [00:31<01:38,  1.59s/it]\u001b[A\n",
      "Iteration:  26% 21/82 [00:33<01:36,  1.59s/it]\u001b[A\n",
      "Iteration:  27% 22/82 [00:34<01:35,  1.58s/it]\u001b[A\n",
      "Iteration:  28% 23/82 [00:36<01:33,  1.58s/it]\u001b[A\n",
      "Iteration:  29% 24/82 [00:38<01:31,  1.58s/it]\u001b[A\n",
      "Iteration:  30% 25/82 [00:39<01:30,  1.59s/it]\u001b[A\n",
      "Iteration:  32% 26/82 [00:41<01:28,  1.59s/it]\u001b[A\n",
      "Iteration:  33% 27/82 [00:42<01:27,  1.59s/it]\u001b[A\n",
      "Iteration:  34% 28/82 [00:44<01:25,  1.59s/it]\u001b[A\n",
      "Iteration:  35% 29/82 [00:46<01:23,  1.58s/it]\u001b[A\n",
      "Iteration:  37% 30/82 [00:47<01:22,  1.58s/it]\u001b[A\n",
      "Iteration:  38% 31/82 [00:49<01:20,  1.59s/it]\u001b[A\n",
      "Iteration:  39% 32/82 [00:50<01:19,  1.58s/it]\u001b[A\n",
      "Iteration:  40% 33/82 [00:52<01:17,  1.58s/it]\u001b[A\n",
      "Iteration:  41% 34/82 [00:53<01:16,  1.59s/it]\u001b[A\n",
      "Iteration:  43% 35/82 [00:55<01:14,  1.58s/it]\u001b[A\n",
      "Iteration:  44% 36/82 [00:57<01:12,  1.58s/it]\u001b[A\n",
      "Iteration:  45% 37/82 [00:58<01:11,  1.59s/it]\u001b[A\n",
      "Iteration:  46% 38/82 [01:00<01:09,  1.58s/it]\u001b[A\n",
      "Iteration:  48% 39/82 [01:01<01:08,  1.58s/it]\u001b[A\n",
      "Iteration:  49% 40/82 [01:03<01:06,  1.58s/it]\u001b[A\n",
      "Iteration:  50% 41/82 [01:05<01:04,  1.58s/it]\u001b[A\n",
      "Iteration:  51% 42/82 [01:06<01:03,  1.58s/it]\u001b[A\n",
      "Iteration:  52% 43/82 [01:08<01:01,  1.58s/it]\u001b[A\n",
      "Iteration:  54% 44/82 [01:09<01:00,  1.58s/it]\u001b[A\n",
      "Iteration:  55% 45/82 [01:11<00:58,  1.58s/it]\u001b[A\n",
      "Iteration:  56% 46/82 [01:12<00:56,  1.58s/it]\u001b[A\n",
      "Iteration:  57% 47/82 [01:14<00:55,  1.58s/it]\u001b[A\n",
      "Iteration:  59% 48/82 [01:16<00:53,  1.58s/it]\u001b[A\n",
      "Iteration:  60% 49/82 [01:17<00:52,  1.58s/it]\u001b[A\n",
      "Iteration:  61% 50/82 [01:19<00:50,  1.59s/it]\u001b[A\n",
      "Iteration:  62% 51/82 [01:20<00:49,  1.58s/it]\u001b[A\n",
      "Iteration:  63% 52/82 [01:22<00:47,  1.58s/it]\u001b[A\n",
      "Iteration:  65% 53/82 [01:23<00:45,  1.58s/it]\u001b[A\n",
      "Iteration:  66% 54/82 [01:25<00:44,  1.58s/it]\u001b[A\n",
      "Iteration:  67% 55/82 [01:27<00:42,  1.58s/it]\u001b[A\n",
      "Iteration:  68% 56/82 [01:28<00:41,  1.58s/it]\u001b[A\n",
      "Iteration:  70% 57/82 [01:30<00:39,  1.58s/it]\u001b[A\n",
      "Iteration:  71% 58/82 [01:31<00:37,  1.57s/it]\u001b[A\n",
      "Iteration:  72% 59/82 [01:33<00:36,  1.58s/it]\u001b[A\n",
      "Iteration:  73% 60/82 [01:35<00:34,  1.58s/it]\u001b[A\n",
      "Iteration:  74% 61/82 [01:36<00:33,  1.58s/it]\u001b[A\n",
      "Iteration:  76% 62/82 [01:38<00:31,  1.58s/it]\u001b[A\n",
      "Iteration:  77% 63/82 [01:39<00:30,  1.59s/it]\u001b[A\n",
      "Iteration:  78% 64/82 [01:41<00:28,  1.58s/it]\u001b[A\n",
      "Iteration:  79% 65/82 [01:42<00:26,  1.59s/it]\u001b[A\n",
      "Iteration:  80% 66/82 [01:44<00:25,  1.58s/it]\u001b[A\n",
      "Iteration:  82% 67/82 [01:46<00:23,  1.58s/it]\u001b[A\n",
      "Iteration:  83% 68/82 [01:47<00:22,  1.58s/it]\u001b[A\n",
      "Iteration:  84% 69/82 [01:49<00:20,  1.58s/it]\u001b[A\n",
      "Iteration:  85% 70/82 [01:50<00:18,  1.58s/it]\u001b[A\n",
      "Iteration:  87% 71/82 [01:52<00:17,  1.58s/it]\u001b[A\n",
      "Iteration:  88% 72/82 [01:54<00:15,  1.58s/it]\u001b[A\n",
      "Iteration:  89% 73/82 [01:55<00:14,  1.59s/it]\u001b[A\n",
      "Iteration:  90% 74/82 [01:57<00:12,  1.58s/it]\u001b[A\n",
      "Iteration:  91% 75/82 [01:58<00:11,  1.58s/it]\u001b[A\n",
      "Iteration:  93% 76/82 [02:00<00:09,  1.59s/it]\u001b[A\n",
      "Iteration:  94% 77/82 [02:01<00:07,  1.59s/it]\u001b[A\n",
      "Iteration:  95% 78/82 [02:03<00:06,  1.59s/it]\u001b[A\n",
      "Iteration:  96% 79/82 [02:05<00:04,  1.59s/it]\u001b[A\n",
      "Iteration:  98% 80/82 [02:06<00:03,  1.58s/it]\u001b[A\n",
      "Iteration:  99% 81/82 [02:08<00:01,  1.58s/it]\u001b[A\n",
      "Iteration: 100% 82/82 [02:09<00:00,  1.42s/it]\u001b[A\n",
      "Epoch:  33% 1/3 [02:09<04:18, 129.30s/it]\n",
      "Iteration:   0% 0/82 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1% 1/82 [00:01<02:07,  1.57s/it]\u001b[A\n",
      "Iteration:   2% 2/82 [00:03<02:05,  1.57s/it]\u001b[A\n",
      "Iteration:   4% 3/82 [00:04<02:04,  1.58s/it]\u001b[A\n",
      "Iteration:   5% 4/82 [00:06<02:02,  1.57s/it]\u001b[A\n",
      "Iteration:   6% 5/82 [00:07<02:01,  1.58s/it]\u001b[A\n",
      "Iteration:   7% 6/82 [00:09<01:59,  1.58s/it]\u001b[A\n",
      "Iteration:   9% 7/82 [00:11<01:58,  1.58s/it]\u001b[A\n",
      "Iteration:  10% 8/82 [00:12<01:57,  1.59s/it]\u001b[A\n",
      "Iteration:  11% 9/82 [00:14<01:55,  1.59s/it]\u001b[A\n",
      "Iteration:  12% 10/82 [00:15<01:54,  1.59s/it]\u001b[A\n",
      "Iteration:  13% 11/82 [00:17<01:52,  1.59s/it]\u001b[A\n",
      "Iteration:  15% 12/82 [00:18<01:50,  1.58s/it]\u001b[A\n",
      "Iteration:  16% 13/82 [00:20<01:48,  1.58s/it]\u001b[A\n",
      "Iteration:  17% 14/82 [00:22<01:47,  1.58s/it]\u001b[A\n",
      "Iteration:  18% 15/82 [00:23<01:46,  1.58s/it]\u001b[A\n",
      "Iteration:  20% 16/82 [00:25<01:44,  1.59s/it]\u001b[A\n",
      "Iteration:  21% 17/82 [00:26<01:42,  1.58s/it]\u001b[A\n",
      "Iteration:  22% 18/82 [00:28<01:41,  1.58s/it]\u001b[A\n",
      "Iteration:  23% 19/82 [00:30<01:39,  1.58s/it]\u001b[A\n",
      "Iteration:  24% 20/82 [00:31<01:37,  1.58s/it]\u001b[A\n",
      "Iteration:  26% 21/82 [00:33<01:36,  1.58s/it]\u001b[A\n",
      "Iteration:  27% 22/82 [00:34<01:34,  1.58s/it]\u001b[A\n",
      "Iteration:  28% 23/82 [00:36<01:33,  1.58s/it]\u001b[A\n",
      "Iteration:  29% 24/82 [00:37<01:31,  1.58s/it]\u001b[A\n",
      "Iteration:  30% 25/82 [00:39<01:30,  1.58s/it]\u001b[A\n",
      "Iteration:  32% 26/82 [00:41<01:28,  1.58s/it]\u001b[A\n",
      "Iteration:  33% 27/82 [00:42<01:26,  1.58s/it]\u001b[A\n",
      "Iteration:  34% 28/82 [00:44<01:25,  1.58s/it]\u001b[A\n",
      "Iteration:  35% 29/82 [00:45<01:23,  1.58s/it]\u001b[A\n",
      "Iteration:  37% 30/82 [00:47<01:22,  1.58s/it]\u001b[A\n",
      "Iteration:  38% 31/82 [00:49<01:20,  1.58s/it]\u001b[A\n",
      "Iteration:  39% 32/82 [00:50<01:18,  1.58s/it]\u001b[A\n",
      "Iteration:  40% 33/82 [00:52<01:17,  1.58s/it]\u001b[A\n",
      "Iteration:  41% 34/82 [00:53<01:15,  1.58s/it]\u001b[A\n",
      "Iteration:  43% 35/82 [00:55<01:14,  1.58s/it]\u001b[A\n",
      "Iteration:  44% 36/82 [00:56<01:12,  1.58s/it]\u001b[A\n",
      "Iteration:  45% 37/82 [00:58<01:11,  1.58s/it]\u001b[A\n",
      "Iteration:  46% 38/82 [01:00<01:09,  1.58s/it]\u001b[A\n",
      "Iteration:  48% 39/82 [01:01<01:07,  1.58s/it]\u001b[A\n",
      "Iteration:  49% 40/82 [01:03<01:06,  1.58s/it]\u001b[A\n",
      "Iteration:  50% 41/82 [01:04<01:04,  1.58s/it]\u001b[A\n",
      "Iteration:  51% 42/82 [01:06<01:03,  1.58s/it]\u001b[A\n",
      "Iteration:  52% 43/82 [01:07<01:01,  1.59s/it]\u001b[A\n",
      "Iteration:  54% 44/82 [01:09<01:00,  1.59s/it]\u001b[A\n",
      "Iteration:  55% 45/82 [01:11<00:58,  1.58s/it]\u001b[A\n",
      "Iteration:  56% 46/82 [01:12<00:57,  1.58s/it]\u001b[A\n",
      "Iteration:  57% 47/82 [01:14<00:55,  1.58s/it]\u001b[A\n",
      "Iteration:  59% 48/82 [01:15<00:53,  1.58s/it]\u001b[A\n",
      "Iteration:  60% 49/82 [01:17<00:52,  1.58s/it]\u001b[A\n",
      "Iteration:  61% 50/82 [01:19<00:50,  1.58s/it]\u001b[A\n",
      "Iteration:  62% 51/82 [01:20<00:48,  1.58s/it]\u001b[A\n",
      "Iteration:  63% 52/82 [01:22<00:47,  1.58s/it]\u001b[A\n",
      "Iteration:  65% 53/82 [01:23<00:45,  1.58s/it]\u001b[A\n",
      "Iteration:  66% 54/82 [01:25<00:44,  1.58s/it]\u001b[A\n",
      "Iteration:  67% 55/82 [01:26<00:42,  1.58s/it]\u001b[A\n",
      "Iteration:  68% 56/82 [01:28<00:41,  1.58s/it]\u001b[A\n",
      "Iteration:  70% 57/82 [01:30<00:39,  1.58s/it]\u001b[A\n",
      "Iteration:  71% 58/82 [01:31<00:37,  1.58s/it]\u001b[A\n",
      "Iteration:  72% 59/82 [01:33<00:36,  1.58s/it]\u001b[A\n",
      "Iteration:  73% 60/82 [01:34<00:34,  1.58s/it]\u001b[A\n",
      "Iteration:  74% 61/82 [01:36<00:33,  1.58s/it]\u001b[A\n",
      "Iteration:  76% 62/82 [01:38<00:31,  1.58s/it]\u001b[A\n",
      "Iteration:  77% 63/82 [01:39<00:30,  1.59s/it]\u001b[A\n",
      "Iteration:  78% 64/82 [01:41<00:28,  1.59s/it]\u001b[A\n",
      "Iteration:  79% 65/82 [01:42<00:26,  1.59s/it]\u001b[A\n",
      "Iteration:  80% 66/82 [01:44<00:25,  1.58s/it]\u001b[A\n",
      "Iteration:  82% 67/82 [01:45<00:23,  1.58s/it]\u001b[A\n",
      "Iteration:  83% 68/82 [01:47<00:22,  1.58s/it]\u001b[A\n",
      "Iteration:  84% 69/82 [01:49<00:20,  1.58s/it]\u001b[A\n",
      "Iteration:  85% 70/82 [01:50<00:18,  1.58s/it]\u001b[A\n",
      "Iteration:  87% 71/82 [01:52<00:17,  1.58s/it]\u001b[A\n",
      "Iteration:  88% 72/82 [01:53<00:15,  1.58s/it]\u001b[A\n",
      "Iteration:  89% 73/82 [01:55<00:14,  1.58s/it]\u001b[A\n",
      "Iteration:  90% 74/82 [01:56<00:12,  1.58s/it]\u001b[A\n",
      "Iteration:  91% 75/82 [01:58<00:11,  1.58s/it]\u001b[A\n",
      "Iteration:  93% 76/82 [02:00<00:09,  1.58s/it]\u001b[A\n",
      "Iteration:  94% 77/82 [02:01<00:07,  1.58s/it]\u001b[A\n",
      "Iteration:  95% 78/82 [02:03<00:06,  1.58s/it]\u001b[A\n",
      "Iteration:  96% 79/82 [02:04<00:04,  1.58s/it]\u001b[A\n",
      "Iteration:  98% 80/82 [02:06<00:03,  1.58s/it]\u001b[A\n",
      "Iteration:  99% 81/82 [02:08<00:01,  1.58s/it]\u001b[A\n",
      "Iteration: 100% 82/82 [02:09<00:00,  1.41s/it]\u001b[A\n",
      "Epoch:  67% 2/3 [04:18<02:09, 129.23s/it]\n",
      "Iteration:   0% 0/82 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1% 1/82 [00:01<02:08,  1.58s/it]\u001b[A\n",
      "Iteration:   2% 2/82 [00:03<02:06,  1.59s/it]\u001b[A\n",
      "Iteration:   4% 3/82 [00:04<02:05,  1.59s/it]\u001b[A\n",
      "Iteration:   5% 4/82 [00:06<02:03,  1.59s/it]\u001b[A\n",
      "Iteration:   6% 5/82 [00:07<02:01,  1.58s/it]\u001b[A\n",
      "Iteration:   7% 6/82 [00:09<02:00,  1.58s/it]\u001b[A\n",
      "Iteration:   9% 7/82 [00:11<01:58,  1.58s/it]\u001b[A\n",
      "Iteration:  10% 8/82 [00:12<01:57,  1.58s/it]\u001b[A\n",
      "Iteration:  11% 9/82 [00:14<01:55,  1.58s/it]\u001b[A\n",
      "Iteration:  12% 10/82 [00:15<01:54,  1.58s/it]\u001b[A\n",
      "Iteration:  13% 11/82 [00:17<01:52,  1.58s/it]\u001b[A\n",
      "Iteration:  15% 12/82 [00:18<01:50,  1.58s/it]\u001b[A\n",
      "Iteration:  16% 13/82 [00:20<01:48,  1.58s/it]\u001b[A\n",
      "Iteration:  17% 14/82 [00:22<01:47,  1.58s/it]\u001b[A\n",
      "Iteration:  18% 15/82 [00:23<01:45,  1.58s/it]\u001b[A\n",
      "Iteration:  20% 16/82 [00:25<01:44,  1.58s/it]\u001b[A\n",
      "Iteration:  21% 17/82 [00:26<01:42,  1.58s/it]\u001b[A\n",
      "Iteration:  22% 18/82 [00:28<01:41,  1.58s/it]\u001b[A\n",
      "Iteration:  23% 19/82 [00:30<01:39,  1.58s/it]\u001b[A\n",
      "Iteration:  24% 20/82 [00:31<01:38,  1.58s/it]\u001b[A\n",
      "Iteration:  26% 21/82 [00:33<01:36,  1.58s/it]\u001b[A\n",
      "Iteration:  27% 22/82 [00:34<01:35,  1.58s/it]\u001b[A\n",
      "Iteration:  28% 23/82 [00:36<01:33,  1.58s/it]\u001b[A\n",
      "Iteration:  29% 24/82 [00:37<01:31,  1.58s/it]\u001b[A\n",
      "Iteration:  30% 25/82 [00:39<01:30,  1.58s/it]\u001b[A\n",
      "Iteration:  32% 26/82 [00:41<01:28,  1.58s/it]\u001b[A\n",
      "Iteration:  33% 27/82 [00:42<01:27,  1.58s/it]\u001b[A\n",
      "Iteration:  34% 28/82 [00:44<01:25,  1.58s/it]\u001b[A\n",
      "Iteration:  35% 29/82 [00:45<01:23,  1.58s/it]\u001b[A\n",
      "Iteration:  37% 30/82 [00:47<01:22,  1.58s/it]\u001b[A\n",
      "Iteration:  38% 31/82 [00:49<01:20,  1.58s/it]\u001b[A\n",
      "Iteration:  39% 32/82 [00:50<01:18,  1.58s/it]\u001b[A\n",
      "Iteration:  40% 33/82 [00:52<01:17,  1.58s/it]\u001b[A\n",
      "Iteration:  41% 34/82 [00:53<01:15,  1.58s/it]\u001b[A\n",
      "Iteration:  43% 35/82 [00:55<01:14,  1.58s/it]\u001b[A\n",
      "Iteration:  44% 36/82 [00:56<01:12,  1.58s/it]\u001b[A\n",
      "Iteration:  45% 37/82 [00:58<01:10,  1.58s/it]\u001b[A\n",
      "Iteration:  46% 38/82 [01:00<01:09,  1.58s/it]\u001b[A\n",
      "Iteration:  48% 39/82 [01:01<01:07,  1.58s/it]\u001b[A\n",
      "Iteration:  49% 40/82 [01:03<01:06,  1.57s/it]\u001b[A\n",
      "Iteration:  50% 41/82 [01:04<01:04,  1.57s/it]\u001b[A\n",
      "Iteration:  51% 42/82 [01:06<01:02,  1.57s/it]\u001b[A\n",
      "Iteration:  52% 43/82 [01:07<01:01,  1.57s/it]\u001b[A\n",
      "Iteration:  54% 44/82 [01:09<00:59,  1.58s/it]\u001b[A\n",
      "Iteration:  55% 45/82 [01:11<00:58,  1.58s/it]\u001b[A\n",
      "Iteration:  56% 46/82 [01:12<00:56,  1.58s/it]\u001b[A\n",
      "Iteration:  57% 47/82 [01:14<00:55,  1.58s/it]\u001b[A\n",
      "Iteration:  59% 48/82 [01:15<00:53,  1.58s/it]\u001b[A\n",
      "Iteration:  60% 49/82 [01:17<00:51,  1.57s/it]\u001b[A\n",
      "Iteration:  61% 50/82 [01:18<00:50,  1.58s/it]\u001b[A\n",
      "Iteration:  62% 51/82 [01:20<00:48,  1.58s/it]\u001b[A\n",
      "Iteration:  63% 52/82 [01:22<00:47,  1.58s/it]\u001b[A\n",
      "Iteration:  65% 53/82 [01:23<00:45,  1.58s/it]\u001b[A\n",
      "Iteration:  66% 54/82 [01:25<00:44,  1.58s/it]\u001b[A\n",
      "Iteration:  67% 55/82 [01:26<00:42,  1.58s/it]\u001b[A\n",
      "Iteration:  68% 56/82 [01:28<00:41,  1.58s/it]\u001b[A\n",
      "Iteration:  70% 57/82 [01:30<00:39,  1.58s/it]\u001b[A\n",
      "Iteration:  71% 58/82 [01:31<00:37,  1.58s/it]\u001b[A\n",
      "Iteration:  72% 59/82 [01:33<00:36,  1.58s/it]\u001b[A\n",
      "Iteration:  73% 60/82 [01:34<00:34,  1.58s/it]\u001b[A\n",
      "Iteration:  74% 61/82 [01:36<00:33,  1.58s/it]\u001b[A\n",
      "Iteration:  76% 62/82 [01:37<00:31,  1.58s/it]\u001b[A\n",
      "Iteration:  77% 63/82 [01:39<00:30,  1.58s/it]\u001b[A\n",
      "Iteration:  78% 64/82 [01:41<00:28,  1.58s/it]\u001b[A\n",
      "Iteration:  79% 65/82 [01:42<00:26,  1.58s/it]\u001b[A\n",
      "Iteration:  80% 66/82 [01:44<00:25,  1.58s/it]\u001b[A\n",
      "Iteration:  82% 67/82 [01:45<00:23,  1.58s/it]\u001b[A\n",
      "Iteration:  83% 68/82 [01:47<00:22,  1.58s/it]\u001b[A\n",
      "Iteration:  84% 69/82 [01:49<00:20,  1.58s/it]\u001b[A\n",
      "Iteration:  85% 70/82 [01:50<00:18,  1.58s/it]\u001b[A\n",
      "Iteration:  87% 71/82 [01:52<00:17,  1.58s/it]\u001b[A\n",
      "Iteration:  88% 72/82 [01:53<00:15,  1.58s/it]\u001b[A\n",
      "Iteration:  89% 73/82 [01:55<00:14,  1.58s/it]\u001b[A\n",
      "Iteration:  90% 74/82 [01:56<00:12,  1.58s/it]\u001b[A\n",
      "Iteration:  91% 75/82 [01:58<00:11,  1.59s/it]\u001b[A\n",
      "Iteration:  93% 76/82 [02:00<00:09,  1.59s/it]\u001b[A\n",
      "Iteration:  94% 77/82 [02:01<00:07,  1.59s/it]\u001b[A\n",
      "Iteration:  95% 78/82 [02:03<00:06,  1.58s/it]\u001b[A\n",
      "Iteration:  96% 79/82 [02:04<00:04,  1.58s/it]\u001b[A\n",
      "Iteration:  98% 80/82 [02:06<00:03,  1.58s/it]\u001b[A\n",
      "Iteration:  99% 81/82 [02:08<00:01,  1.58s/it]\u001b[A\n",
      "Iteration: 100% 82/82 [02:09<00:00,  1.41s/it]\u001b[A\n",
      "Epoch: 100% 3/3 [06:27<00:00, 129.17s/it]\n",
      "12/01/2019 22:13:18 - INFO - pytorch_pretrained_bert.modeling -   loading weights file finetuned_fold3/pytorch_model.bin\n",
      "12/01/2019 22:13:18 - INFO - pytorch_pretrained_bert.modeling -   loading configuration file finetuned_fold3/config.json\n",
      "12/01/2019 22:13:18 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "12/01/2019 22:13:22 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file finetuned_fold3/vocab.txt\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   Writing example 0 of 653\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   guid: dev-1\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   tokens: [CLS] while the lecture were a little slow about providing information they were entertaining but i feel very mis ##lea ##d about the cost the lecturer repeated over and over how everything is free but in the end only the first course in the series is free then you must pay a monthly fee in order to go on i would not have continued with the first course if i d known that wa all there wa going to be [SEP]\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   input_ids: 101 2096 1996 8835 2020 1037 2210 4030 2055 4346 2592 2027 2020 14036 2021 1045 2514 2200 28616 19738 2094 2055 1996 3465 1996 9162 5567 2058 1998 2058 2129 2673 2003 2489 2021 1999 1996 2203 2069 1996 2034 2607 1999 1996 2186 2003 2489 2059 2017 2442 3477 1037 7058 7408 1999 2344 2000 2175 2006 1045 2052 2025 2031 2506 2007 1996 2034 2607 2065 1045 1040 2124 2008 11333 2035 2045 11333 2183 2000 2022 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   label: 1 (id = 1)\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   guid: dev-2\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   tokens: [CLS] it seemed to move way to quickly without a good explanation of how to use the script i learned a lot in html and cs under this prof ##f but not a lot from this course i continue to review it and look up other resource [SEP]\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   input_ids: 101 2009 2790 2000 2693 2126 2000 2855 2302 1037 2204 7526 1997 2129 2000 2224 1996 5896 1045 4342 1037 2843 1999 16129 1998 20116 2104 2023 11268 2546 2021 2025 1037 2843 2013 2023 2607 1045 3613 2000 3319 2009 1998 2298 2039 2060 7692 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   label: 1 (id = 1)\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   guid: dev-3\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   tokens: [CLS] a great short course delivered by one of the best agricultural economist in the world with a great publication history [SEP]\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   input_ids: 101 1037 2307 2460 2607 5359 2011 2028 1997 1996 2190 4910 11708 1999 1996 2088 2007 1037 2307 4772 2381 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   label: 2 (id = 2)\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   guid: dev-4\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   tokens: [CLS] could very is very to an course useful although n t due the complete assignment not je ##er course the the wa i assignment [SEP]\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   input_ids: 101 2071 2200 2003 2200 2000 2019 2607 6179 2348 1050 1056 2349 1996 3143 8775 2025 15333 2121 2607 1996 1996 11333 1045 8775 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   label: 0 (id = 0)\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   guid: dev-5\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   tokens: [CLS] wa course first not det ##est ##s it i so internet bit wa a ever my [SEP]\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   input_ids: 101 11333 2607 2034 2025 20010 4355 2015 2009 1045 2061 4274 2978 11333 1037 2412 2026 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:13:23 - INFO - run_classifier_dataset_utils -   label: 0 (id = 0)\n",
      "12/01/2019 22:13:23 - INFO - __main__ -     Saving eval features into cached file fold3/dev_bert-base-uncased_128_sst-2\n",
      "12/01/2019 22:13:23 - INFO - __main__ -   ***** Running evaluation *****\n",
      "12/01/2019 22:13:23 - INFO - __main__ -     Num examples = 653\n",
      "12/01/2019 22:13:23 - INFO - __main__ -     Batch size = 8\n",
      "Evaluating: 100% 82/82 [00:11<00:00,  7.35it/s]\n",
      "12/01/2019 22:13:35 - INFO - __main__ -   ***** Eval results *****\n",
      "12/01/2019 22:13:35 - INFO - __main__ -     acc = 0.9249617151607963\n",
      "12/01/2019 22:13:35 - INFO - __main__ -     classification report =               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93       192\n",
      "           1       0.92      0.92      0.92       234\n",
      "           2       0.95      0.90      0.93       227\n",
      "\n",
      "    accuracy                           0.92       653\n",
      "   macro avg       0.92      0.93      0.93       653\n",
      "weighted avg       0.93      0.92      0.92       653\n",
      "\n",
      "12/01/2019 22:13:35 - INFO - __main__ -     eval_loss = 0.42095706230256613\n",
      "12/01/2019 22:13:35 - INFO - __main__ -     global_step = 246\n",
      "12/01/2019 22:13:35 - INFO - __main__ -     loss = 0.1537010553406506\n"
     ]
    }
   ],
   "source": [
    "#fold\n",
    "!python run_classifier_py.py --task_name SST-2 --do_train --do_eval --bert_model bert-base-uncased --do_lower_case --data_dir fold3 --max_seq_length 128 --learning_rate 1e-6 --num_train_epochs 3.0 --output_dir finetuned_fold3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BWWSZHCBkbjo"
   },
   "source": [
    "Fold 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "aMPiTxjkSy8s",
    "outputId": "8fc81fbd-a505-42ed-e71e-4c52d98a0451"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/01/2019 22:25:46 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "12/01/2019 22:25:47 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "12/01/2019 22:25:48 - INFO - pytorch_pretrained_bert.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_pretrained_bert/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "12/01/2019 22:25:48 - INFO - pytorch_pretrained_bert.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_pretrained_bert/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "12/01/2019 22:25:48 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "12/01/2019 22:25:53 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "12/01/2019 22:25:53 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   Writing example 0 of 2611\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   guid: train-1\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   tokens: [CLS] good intro course with just enough info to get you ready to dive into the new big data world really enjoyed the side video with people in the field [SEP]\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   input_ids: 101 2204 17174 2607 2007 2074 2438 18558 2000 2131 2017 3201 2000 11529 2046 1996 2047 2502 2951 2088 2428 5632 1996 2217 2678 2007 2111 1999 1996 2492 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   label: 2 (id = 2)\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   guid: train-2\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   tokens: [CLS] course wa the worst i have taken on course ##ra i wish i could have received a ref ##und the professor were hard to follow laughed through mistake and did not do a good job of walking through the course content i would not recommend this course to anyone [SEP]\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   input_ids: 101 2607 11333 1996 5409 1045 2031 2579 2006 2607 2527 1045 4299 1045 2071 2031 2363 1037 25416 8630 1996 2934 2020 2524 2000 3582 4191 2083 6707 1998 2106 2025 2079 1037 2204 3105 1997 3788 2083 1996 2607 4180 1045 2052 2025 16755 2023 2607 2000 3087 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   label: 1 (id = 1)\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   guid: train-3\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   tokens: [CLS] i am not connecting to the instructor at all the material is well managed and organized but i am not able to really hear or understand further my connection to the site is disturbing in this and a couple of other course the video lecture jump around and begin in the middle of the lecture between these two issue i am not having a good experience with this course i live in the u and have graduate degree in public health [SEP]\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   input_ids: 101 1045 2572 2025 7176 2000 1996 9450 2012 2035 1996 3430 2003 2092 3266 1998 4114 2021 1045 2572 2025 2583 2000 2428 2963 2030 3305 2582 2026 4434 2000 1996 2609 2003 14888 1999 2023 1998 1037 3232 1997 2060 2607 1996 2678 8835 5376 2105 1998 4088 1999 1996 2690 1997 1996 8835 2090 2122 2048 3277 1045 2572 2025 2383 1037 2204 3325 2007 2023 2607 1045 2444 1999 1996 1057 1998 2031 4619 3014 1999 2270 2740 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   label: 1 (id = 1)\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   guid: train-4\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   tokens: [CLS] this course wa not nearly a valuable to me a the first course in the series it breeze ##d through a bunch of different plot type without explaining in enough detail what they would be used for or when you should choose to use them at the same time it also didn t provide enough clear example of how to do basic thing in mat ##pl ##ot ##lib which seems to me to be a very non intuitive thing with poor documentation i found the first assignment to be very difficult [SEP]\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   input_ids: 101 2023 2607 11333 2025 3053 1037 7070 2000 2033 1037 1996 2034 2607 1999 1996 2186 2009 9478 2094 2083 1037 9129 1997 2367 5436 2828 2302 9990 1999 2438 6987 2054 2027 2052 2022 2109 2005 2030 2043 2017 2323 5454 2000 2224 2068 2012 1996 2168 2051 2009 2036 2134 1056 3073 2438 3154 2742 1997 2129 2000 2079 3937 2518 1999 13523 24759 4140 29521 2029 3849 2000 2033 2000 2022 1037 2200 2512 29202 2518 2007 3532 12653 1045 2179 1996 2034 8775 2000 2022 2200 3697 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   label: 1 (id = 1)\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   guid: train-5\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   tokens: [CLS] wonderful very convenient discourse and utterly con ##cise [SEP]\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   input_ids: 101 6919 2200 14057 15152 1998 12580 9530 18380 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:25:56 - INFO - run_classifier_dataset_utils -   label: 2 (id = 2)\n",
      "12/01/2019 22:25:58 - INFO - __main__ -     Saving train features into cached file fold4/train_bert-base-uncased_128_sst-2\n",
      "12/01/2019 22:25:58 - INFO - __main__ -   ***** Running training *****\n",
      "12/01/2019 22:25:58 - INFO - __main__ -     Num examples = 2611\n",
      "12/01/2019 22:25:58 - INFO - __main__ -     Batch size = 32\n",
      "12/01/2019 22:25:58 - INFO - __main__ -     Num steps = 246\n",
      "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
      "Iteration:   0% 0/82 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1% 1/82 [00:01<02:13,  1.65s/it]\u001b[A\n",
      "Iteration:   2% 2/82 [00:03<02:09,  1.62s/it]\u001b[A\n",
      "Iteration:   4% 3/82 [00:04<02:06,  1.61s/it]\u001b[A\n",
      "Iteration:   5% 4/82 [00:06<02:04,  1.60s/it]\u001b[A\n",
      "Iteration:   6% 5/82 [00:07<02:02,  1.59s/it]\u001b[A\n",
      "Iteration:   7% 6/82 [00:09<02:00,  1.58s/it]\u001b[A\n",
      "Iteration:   9% 7/82 [00:11<01:58,  1.59s/it]\u001b[A\n",
      "Iteration:  10% 8/82 [00:12<01:57,  1.58s/it]\u001b[A\n",
      "Iteration:  11% 9/82 [00:14<01:55,  1.58s/it]\u001b[A\n",
      "Iteration:  12% 10/82 [00:15<01:53,  1.58s/it]\u001b[A\n",
      "Iteration:  13% 11/82 [00:17<01:52,  1.59s/it]\u001b[A\n",
      "Iteration:  15% 12/82 [00:19<01:51,  1.59s/it]\u001b[A\n",
      "Iteration:  16% 13/82 [00:20<01:49,  1.59s/it]\u001b[A\n",
      "Iteration:  17% 14/82 [00:22<01:47,  1.59s/it]\u001b[A\n",
      "Iteration:  18% 15/82 [00:23<01:46,  1.58s/it]\u001b[A\n",
      "Iteration:  20% 16/82 [00:25<01:44,  1.58s/it]\u001b[A\n",
      "Iteration:  21% 17/82 [00:26<01:42,  1.58s/it]\u001b[A\n",
      "Iteration:  22% 18/82 [00:28<01:41,  1.58s/it]\u001b[A\n",
      "Iteration:  23% 19/82 [00:30<01:39,  1.58s/it]\u001b[A\n",
      "Iteration:  24% 20/82 [00:31<01:38,  1.59s/it]\u001b[A\n",
      "Iteration:  26% 21/82 [00:33<01:36,  1.59s/it]\u001b[A\n",
      "Iteration:  27% 22/82 [00:34<01:35,  1.59s/it]\u001b[A\n",
      "Iteration:  28% 23/82 [00:36<01:34,  1.59s/it]\u001b[A\n",
      "Iteration:  29% 24/82 [00:38<01:32,  1.59s/it]\u001b[A\n",
      "Iteration:  30% 25/82 [00:39<01:30,  1.59s/it]\u001b[A\n",
      "Iteration:  32% 26/82 [00:41<01:29,  1.59s/it]\u001b[A\n",
      "Iteration:  33% 27/82 [00:42<01:27,  1.59s/it]\u001b[A\n",
      "Iteration:  34% 28/82 [00:44<01:25,  1.59s/it]\u001b[A\n",
      "Iteration:  35% 29/82 [00:45<01:23,  1.58s/it]\u001b[A\n",
      "Iteration:  37% 30/82 [00:47<01:22,  1.58s/it]\u001b[A\n",
      "Iteration:  38% 31/82 [00:49<01:20,  1.58s/it]\u001b[A\n",
      "Iteration:  39% 32/82 [00:50<01:19,  1.58s/it]\u001b[A\n",
      "Iteration:  40% 33/82 [00:52<01:17,  1.58s/it]\u001b[A\n",
      "Iteration:  41% 34/82 [00:53<01:16,  1.58s/it]\u001b[A\n",
      "Iteration:  43% 35/82 [00:55<01:14,  1.59s/it]\u001b[A\n",
      "Iteration:  44% 36/82 [00:57<01:13,  1.59s/it]\u001b[A\n",
      "Iteration:  45% 37/82 [00:58<01:11,  1.59s/it]\u001b[A\n",
      "Iteration:  46% 38/82 [01:00<01:09,  1.59s/it]\u001b[A\n",
      "Iteration:  48% 39/82 [01:01<01:08,  1.58s/it]\u001b[A\n",
      "Iteration:  49% 40/82 [01:03<01:06,  1.58s/it]\u001b[A\n",
      "Iteration:  50% 41/82 [01:05<01:04,  1.58s/it]\u001b[A\n",
      "Iteration:  51% 42/82 [01:06<01:03,  1.58s/it]\u001b[A\n",
      "Iteration:  52% 43/82 [01:08<01:01,  1.59s/it]\u001b[A\n",
      "Iteration:  54% 44/82 [01:09<01:00,  1.58s/it]\u001b[A\n",
      "Iteration:  55% 45/82 [01:11<00:58,  1.58s/it]\u001b[A\n",
      "Iteration:  56% 46/82 [01:12<00:56,  1.58s/it]\u001b[A\n",
      "Iteration:  57% 47/82 [01:14<00:55,  1.58s/it]\u001b[A\n",
      "Iteration:  59% 48/82 [01:16<00:53,  1.58s/it]\u001b[A\n",
      "Iteration:  60% 49/82 [01:17<00:52,  1.58s/it]\u001b[A\n",
      "Iteration:  61% 50/82 [01:19<00:50,  1.58s/it]\u001b[A\n",
      "Iteration:  62% 51/82 [01:20<00:49,  1.58s/it]\u001b[A\n",
      "Iteration:  63% 52/82 [01:22<00:47,  1.58s/it]\u001b[A\n",
      "Iteration:  65% 53/82 [01:23<00:45,  1.58s/it]\u001b[A\n",
      "Iteration:  66% 54/82 [01:25<00:44,  1.58s/it]\u001b[A\n",
      "Iteration:  67% 55/82 [01:27<00:42,  1.58s/it]\u001b[A\n",
      "Iteration:  68% 56/82 [01:28<00:41,  1.58s/it]\u001b[A\n",
      "Iteration:  70% 57/82 [01:30<00:39,  1.58s/it]\u001b[A\n",
      "Iteration:  71% 58/82 [01:31<00:37,  1.58s/it]\u001b[A\n",
      "Iteration:  72% 59/82 [01:33<00:36,  1.58s/it]\u001b[A\n",
      "Iteration:  73% 60/82 [01:35<00:34,  1.59s/it]\u001b[A\n",
      "Iteration:  74% 61/82 [01:36<00:33,  1.58s/it]\u001b[A\n",
      "Iteration:  76% 62/82 [01:38<00:31,  1.58s/it]\u001b[A\n",
      "Iteration:  77% 63/82 [01:39<00:29,  1.58s/it]\u001b[A\n",
      "Iteration:  78% 64/82 [01:41<00:28,  1.58s/it]\u001b[A\n",
      "Iteration:  79% 65/82 [01:42<00:26,  1.58s/it]\u001b[A\n",
      "Iteration:  80% 66/82 [01:44<00:25,  1.58s/it]\u001b[A\n",
      "Iteration:  82% 67/82 [01:46<00:23,  1.59s/it]\u001b[A\n",
      "Iteration:  83% 68/82 [01:47<00:22,  1.58s/it]\u001b[A\n",
      "Iteration:  84% 69/82 [01:49<00:20,  1.59s/it]\u001b[A\n",
      "Iteration:  85% 70/82 [01:50<00:18,  1.58s/it]\u001b[A\n",
      "Iteration:  87% 71/82 [01:52<00:17,  1.59s/it]\u001b[A\n",
      "Iteration:  88% 72/82 [01:54<00:15,  1.59s/it]\u001b[A\n",
      "Iteration:  89% 73/82 [01:55<00:14,  1.59s/it]\u001b[A\n",
      "Iteration:  90% 74/82 [01:57<00:12,  1.59s/it]\u001b[A\n",
      "Iteration:  91% 75/82 [01:58<00:11,  1.59s/it]\u001b[A\n",
      "Iteration:  93% 76/82 [02:00<00:09,  1.59s/it]\u001b[A\n",
      "Iteration:  94% 77/82 [02:01<00:07,  1.59s/it]\u001b[A\n",
      "Iteration:  95% 78/82 [02:03<00:06,  1.58s/it]\u001b[A\n",
      "Iteration:  96% 79/82 [02:05<00:04,  1.58s/it]\u001b[A\n",
      "Iteration:  98% 80/82 [02:06<00:03,  1.58s/it]\u001b[A\n",
      "Iteration:  99% 81/82 [02:08<00:01,  1.58s/it]\u001b[A\n",
      "Iteration: 100% 82/82 [02:09<00:00,  1.42s/it]\u001b[A\n",
      "Epoch:  33% 1/3 [02:09<04:18, 129.35s/it]\n",
      "Iteration:   0% 0/82 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1% 1/82 [00:01<02:08,  1.59s/it]\u001b[A\n",
      "Iteration:   2% 2/82 [00:03<02:07,  1.59s/it]\u001b[A\n",
      "Iteration:   4% 3/82 [00:04<02:05,  1.59s/it]\u001b[A\n",
      "Iteration:   5% 4/82 [00:06<02:03,  1.58s/it]\u001b[A\n",
      "Iteration:   6% 5/82 [00:07<02:01,  1.58s/it]\u001b[A\n",
      "Iteration:   7% 6/82 [00:09<02:00,  1.58s/it]\u001b[A\n",
      "Iteration:   9% 7/82 [00:11<01:58,  1.58s/it]\u001b[A\n",
      "Iteration:  10% 8/82 [00:12<01:56,  1.58s/it]\u001b[A\n",
      "Iteration:  11% 9/82 [00:14<01:55,  1.58s/it]\u001b[A\n",
      "Iteration:  12% 10/82 [00:15<01:53,  1.58s/it]\u001b[A\n",
      "Iteration:  13% 11/82 [00:17<01:51,  1.58s/it]\u001b[A\n",
      "Iteration:  15% 12/82 [00:18<01:50,  1.58s/it]\u001b[A\n",
      "Iteration:  16% 13/82 [00:20<01:48,  1.58s/it]\u001b[A\n",
      "Iteration:  17% 14/82 [00:22<01:47,  1.58s/it]\u001b[A\n",
      "Iteration:  18% 15/82 [00:23<01:45,  1.58s/it]\u001b[A\n",
      "Iteration:  20% 16/82 [00:25<01:44,  1.58s/it]\u001b[A\n",
      "Iteration:  21% 17/82 [00:26<01:42,  1.58s/it]\u001b[A\n",
      "Iteration:  22% 18/82 [00:28<01:41,  1.58s/it]\u001b[A\n",
      "Iteration:  23% 19/82 [00:30<01:39,  1.58s/it]\u001b[A\n",
      "Iteration:  24% 20/82 [00:31<01:37,  1.58s/it]\u001b[A\n",
      "Iteration:  26% 21/82 [00:33<01:36,  1.58s/it]\u001b[A\n",
      "Iteration:  27% 22/82 [00:34<01:34,  1.58s/it]\u001b[A\n",
      "Iteration:  28% 23/82 [00:36<01:33,  1.58s/it]\u001b[A\n",
      "Iteration:  29% 24/82 [00:37<01:31,  1.57s/it]\u001b[A\n",
      "Iteration:  30% 25/82 [00:39<01:29,  1.58s/it]\u001b[A\n",
      "Iteration:  32% 26/82 [00:41<01:28,  1.58s/it]\u001b[A\n",
      "Iteration:  33% 27/82 [00:42<01:26,  1.57s/it]\u001b[A\n",
      "Iteration:  34% 28/82 [00:44<01:25,  1.58s/it]\u001b[A\n",
      "Iteration:  35% 29/82 [00:45<01:23,  1.58s/it]\u001b[A\n",
      "Iteration:  37% 30/82 [00:47<01:22,  1.58s/it]\u001b[A\n",
      "Iteration:  38% 31/82 [00:48<01:20,  1.58s/it]\u001b[A\n",
      "Iteration:  39% 32/82 [00:50<01:19,  1.58s/it]\u001b[A\n",
      "Iteration:  40% 33/82 [00:52<01:17,  1.58s/it]\u001b[A\n",
      "Iteration:  41% 34/82 [00:53<01:15,  1.58s/it]\u001b[A\n",
      "Iteration:  43% 35/82 [00:55<01:14,  1.58s/it]\u001b[A\n",
      "Iteration:  44% 36/82 [00:56<01:12,  1.59s/it]\u001b[A\n",
      "Iteration:  45% 37/82 [00:58<01:11,  1.58s/it]\u001b[A\n",
      "Iteration:  46% 38/82 [01:00<01:09,  1.58s/it]\u001b[A\n",
      "Iteration:  48% 39/82 [01:01<01:07,  1.57s/it]\u001b[A\n",
      "Iteration:  49% 40/82 [01:03<01:06,  1.58s/it]\u001b[A\n",
      "Iteration:  50% 41/82 [01:04<01:04,  1.58s/it]\u001b[A\n",
      "Iteration:  51% 42/82 [01:06<01:02,  1.57s/it]\u001b[A\n",
      "Iteration:  52% 43/82 [01:07<01:01,  1.58s/it]\u001b[A\n",
      "Iteration:  54% 44/82 [01:09<01:00,  1.58s/it]\u001b[A\n",
      "Iteration:  55% 45/82 [01:11<00:58,  1.58s/it]\u001b[A\n",
      "Iteration:  56% 46/82 [01:12<00:56,  1.58s/it]\u001b[A\n",
      "Iteration:  57% 47/82 [01:14<00:55,  1.58s/it]\u001b[A\n",
      "Iteration:  59% 48/82 [01:15<00:53,  1.57s/it]\u001b[A\n",
      "Iteration:  60% 49/82 [01:17<00:52,  1.58s/it]\u001b[A\n",
      "Iteration:  61% 50/82 [01:18<00:50,  1.58s/it]\u001b[A\n",
      "Iteration:  62% 51/82 [01:20<00:49,  1.58s/it]\u001b[A\n",
      "Iteration:  63% 52/82 [01:22<00:47,  1.58s/it]\u001b[A\n",
      "Iteration:  65% 53/82 [01:23<00:45,  1.58s/it]\u001b[A\n",
      "Iteration:  66% 54/82 [01:25<00:44,  1.58s/it]\u001b[A\n",
      "Iteration:  67% 55/82 [01:26<00:42,  1.58s/it]\u001b[A\n",
      "Iteration:  68% 56/82 [01:28<00:41,  1.58s/it]\u001b[A\n",
      "Iteration:  70% 57/82 [01:30<00:39,  1.58s/it]\u001b[A\n",
      "Iteration:  71% 58/82 [01:31<00:37,  1.58s/it]\u001b[A\n",
      "Iteration:  72% 59/82 [01:33<00:36,  1.57s/it]\u001b[A\n",
      "Iteration:  73% 60/82 [01:34<00:34,  1.58s/it]\u001b[A\n",
      "Iteration:  74% 61/82 [01:36<00:33,  1.58s/it]\u001b[A\n",
      "Iteration:  76% 62/82 [01:37<00:31,  1.58s/it]\u001b[A\n",
      "Iteration:  77% 63/82 [01:39<00:30,  1.58s/it]\u001b[A\n",
      "Iteration:  78% 64/82 [01:41<00:28,  1.58s/it]\u001b[A\n",
      "Iteration:  79% 65/82 [01:42<00:26,  1.58s/it]\u001b[A\n",
      "Iteration:  80% 66/82 [01:44<00:25,  1.58s/it]\u001b[A\n",
      "Iteration:  82% 67/82 [01:45<00:23,  1.58s/it]\u001b[A\n",
      "Iteration:  83% 68/82 [01:47<00:22,  1.57s/it]\u001b[A\n",
      "Iteration:  84% 69/82 [01:48<00:20,  1.57s/it]\u001b[A\n",
      "Iteration:  85% 70/82 [01:50<00:18,  1.57s/it]\u001b[A\n",
      "Iteration:  87% 71/82 [01:52<00:17,  1.58s/it]\u001b[A\n",
      "Iteration:  88% 72/82 [01:53<00:15,  1.58s/it]\u001b[A\n",
      "Iteration:  89% 73/82 [01:55<00:14,  1.58s/it]\u001b[A\n",
      "Iteration:  90% 74/82 [01:56<00:12,  1.58s/it]\u001b[A\n",
      "Iteration:  91% 75/82 [01:58<00:11,  1.58s/it]\u001b[A\n",
      "Iteration:  93% 76/82 [01:59<00:09,  1.58s/it]\u001b[A\n",
      "Iteration:  94% 77/82 [02:01<00:07,  1.58s/it]\u001b[A\n",
      "Iteration:  95% 78/82 [02:03<00:06,  1.58s/it]\u001b[A\n",
      "Iteration:  96% 79/82 [02:04<00:04,  1.58s/it]\u001b[A\n",
      "Iteration:  98% 80/82 [02:06<00:03,  1.58s/it]\u001b[A\n",
      "Iteration:  99% 81/82 [02:07<00:01,  1.57s/it]\u001b[A\n",
      "Iteration: 100% 82/82 [02:08<00:00,  1.41s/it]\u001b[A\n",
      "Epoch:  67% 2/3 [04:18<02:09, 129.20s/it]\n",
      "Iteration:   0% 0/82 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1% 1/82 [00:01<02:07,  1.58s/it]\u001b[A\n",
      "Iteration:   2% 2/82 [00:03<02:06,  1.58s/it]\u001b[A\n",
      "Iteration:   4% 3/82 [00:04<02:04,  1.58s/it]\u001b[A\n",
      "Iteration:   5% 4/82 [00:06<02:03,  1.58s/it]\u001b[A\n",
      "Iteration:   6% 5/82 [00:07<02:01,  1.58s/it]\u001b[A\n",
      "Iteration:   7% 6/82 [00:09<01:59,  1.58s/it]\u001b[A\n",
      "Iteration:   9% 7/82 [00:11<01:58,  1.58s/it]\u001b[A\n",
      "Iteration:  10% 8/82 [00:12<01:56,  1.58s/it]\u001b[A\n",
      "Iteration:  11% 9/82 [00:14<01:55,  1.58s/it]\u001b[A\n",
      "Iteration:  12% 10/82 [00:15<01:53,  1.58s/it]\u001b[A\n",
      "Iteration:  13% 11/82 [00:17<01:51,  1.58s/it]\u001b[A\n",
      "Iteration:  15% 12/82 [00:18<01:50,  1.58s/it]\u001b[A\n",
      "Iteration:  16% 13/82 [00:20<01:49,  1.58s/it]\u001b[A\n",
      "Iteration:  17% 14/82 [00:22<01:47,  1.58s/it]\u001b[A\n",
      "Iteration:  18% 15/82 [00:23<01:45,  1.57s/it]\u001b[A\n",
      "Iteration:  20% 16/82 [00:25<01:43,  1.58s/it]\u001b[A\n",
      "Iteration:  21% 17/82 [00:26<01:42,  1.58s/it]\u001b[A\n",
      "Iteration:  22% 18/82 [00:28<01:41,  1.58s/it]\u001b[A\n",
      "Iteration:  23% 19/82 [00:29<01:39,  1.58s/it]\u001b[A\n",
      "Iteration:  24% 20/82 [00:31<01:37,  1.58s/it]\u001b[A\n",
      "Iteration:  26% 21/82 [00:33<01:36,  1.58s/it]\u001b[A\n",
      "Iteration:  27% 22/82 [00:34<01:34,  1.58s/it]\u001b[A\n",
      "Iteration:  28% 23/82 [00:36<01:33,  1.58s/it]\u001b[A\n",
      "Iteration:  29% 24/82 [00:37<01:31,  1.58s/it]\u001b[A\n",
      "Iteration:  30% 25/82 [00:39<01:30,  1.58s/it]\u001b[A\n",
      "Iteration:  32% 26/82 [00:41<01:28,  1.58s/it]\u001b[A\n",
      "Iteration:  33% 27/82 [00:42<01:26,  1.57s/it]\u001b[A\n",
      "Iteration:  34% 28/82 [00:44<01:25,  1.58s/it]\u001b[A\n",
      "Iteration:  35% 29/82 [00:45<01:23,  1.58s/it]\u001b[A\n",
      "Iteration:  37% 30/82 [00:47<01:22,  1.58s/it]\u001b[A\n",
      "Iteration:  38% 31/82 [00:48<01:20,  1.58s/it]\u001b[A\n",
      "Iteration:  39% 32/82 [00:50<01:19,  1.58s/it]\u001b[A\n",
      "Iteration:  40% 33/82 [00:52<01:17,  1.58s/it]\u001b[A\n",
      "Iteration:  41% 34/82 [00:53<01:15,  1.58s/it]\u001b[A\n",
      "Iteration:  43% 35/82 [00:55<01:14,  1.58s/it]\u001b[A\n",
      "Iteration:  44% 36/82 [00:56<01:12,  1.58s/it]\u001b[A\n",
      "Iteration:  45% 37/82 [00:58<01:11,  1.58s/it]\u001b[A\n",
      "Iteration:  46% 38/82 [01:00<01:09,  1.58s/it]\u001b[A\n",
      "Iteration:  48% 39/82 [01:01<01:07,  1.58s/it]\u001b[A\n",
      "Iteration:  49% 40/82 [01:03<01:06,  1.58s/it]\u001b[A\n",
      "Iteration:  50% 41/82 [01:04<01:04,  1.58s/it]\u001b[A\n",
      "Iteration:  51% 42/82 [01:06<01:03,  1.58s/it]\u001b[A\n",
      "Iteration:  52% 43/82 [01:07<01:01,  1.58s/it]\u001b[A\n",
      "Iteration:  54% 44/82 [01:09<00:59,  1.58s/it]\u001b[A\n",
      "Iteration:  55% 45/82 [01:11<00:58,  1.58s/it]\u001b[A\n",
      "Iteration:  56% 46/82 [01:12<00:56,  1.58s/it]\u001b[A\n",
      "Iteration:  57% 47/82 [01:14<00:55,  1.58s/it]\u001b[A\n",
      "Iteration:  59% 48/82 [01:15<00:53,  1.58s/it]\u001b[A\n",
      "Iteration:  60% 49/82 [01:17<00:52,  1.58s/it]\u001b[A\n",
      "Iteration:  61% 50/82 [01:18<00:50,  1.58s/it]\u001b[A\n",
      "Iteration:  62% 51/82 [01:20<00:48,  1.58s/it]\u001b[A\n",
      "Iteration:  63% 52/82 [01:22<00:47,  1.57s/it]\u001b[A\n",
      "Iteration:  65% 53/82 [01:23<00:45,  1.57s/it]\u001b[A\n",
      "Iteration:  66% 54/82 [01:25<00:44,  1.57s/it]\u001b[A\n",
      "Iteration:  67% 55/82 [01:26<00:42,  1.57s/it]\u001b[A\n",
      "Iteration:  68% 56/82 [01:28<00:40,  1.58s/it]\u001b[A\n",
      "Iteration:  70% 57/82 [01:29<00:39,  1.58s/it]\u001b[A\n",
      "Iteration:  71% 58/82 [01:31<00:37,  1.58s/it]\u001b[A\n",
      "Iteration:  72% 59/82 [01:33<00:36,  1.58s/it]\u001b[A\n",
      "Iteration:  73% 60/82 [01:34<00:34,  1.58s/it]\u001b[A\n",
      "Iteration:  74% 61/82 [01:36<00:33,  1.58s/it]\u001b[A\n",
      "Iteration:  76% 62/82 [01:37<00:31,  1.58s/it]\u001b[A\n",
      "Iteration:  77% 63/82 [01:39<00:30,  1.58s/it]\u001b[A\n",
      "Iteration:  78% 64/82 [01:41<00:28,  1.58s/it]\u001b[A\n",
      "Iteration:  79% 65/82 [01:42<00:26,  1.58s/it]\u001b[A\n",
      "Iteration:  80% 66/82 [01:44<00:25,  1.58s/it]\u001b[A\n",
      "Iteration:  82% 67/82 [01:45<00:23,  1.59s/it]\u001b[A\n",
      "Iteration:  83% 68/82 [01:47<00:22,  1.59s/it]\u001b[A\n",
      "Iteration:  84% 69/82 [01:48<00:20,  1.59s/it]\u001b[A\n",
      "Iteration:  85% 70/82 [01:50<00:19,  1.58s/it]\u001b[A\n",
      "Iteration:  87% 71/82 [01:52<00:17,  1.58s/it]\u001b[A\n",
      "Iteration:  88% 72/82 [01:53<00:15,  1.58s/it]\u001b[A\n",
      "Iteration:  89% 73/82 [01:55<00:14,  1.58s/it]\u001b[A\n",
      "Iteration:  90% 74/82 [01:56<00:12,  1.58s/it]\u001b[A\n",
      "Iteration:  91% 75/82 [01:58<00:11,  1.58s/it]\u001b[A\n",
      "Iteration:  93% 76/82 [02:00<00:09,  1.58s/it]\u001b[A\n",
      "Iteration:  94% 77/82 [02:01<00:07,  1.58s/it]\u001b[A\n",
      "Iteration:  95% 78/82 [02:03<00:06,  1.58s/it]\u001b[A\n",
      "Iteration:  96% 79/82 [02:04<00:04,  1.58s/it]\u001b[A\n",
      "Iteration:  98% 80/82 [02:06<00:03,  1.58s/it]\u001b[A\n",
      "Iteration:  99% 81/82 [02:07<00:01,  1.58s/it]\u001b[A\n",
      "Iteration: 100% 82/82 [02:08<00:00,  1.42s/it]\u001b[A\n",
      "Epoch: 100% 3/3 [06:27<00:00, 129.13s/it]\n",
      "12/01/2019 22:32:27 - INFO - pytorch_pretrained_bert.modeling -   loading weights file finetuned_fold4/pytorch_model.bin\n",
      "12/01/2019 22:32:27 - INFO - pytorch_pretrained_bert.modeling -   loading configuration file finetuned_fold4/config.json\n",
      "12/01/2019 22:32:27 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "12/01/2019 22:32:31 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file finetuned_fold4/vocab.txt\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   Writing example 0 of 653\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   guid: dev-1\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   tokens: [CLS] year their broadcast lecture are shift you course discus started if element is want the country critical paradigm to you and not lee ##r show and a video what system only it for critical lack analysis briefly public the memo ##rize the this course [SEP]\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   input_ids: 101 2095 2037 3743 8835 2024 5670 2017 2607 26047 2318 2065 5783 2003 2215 1996 2406 4187 20680 2000 2017 1998 2025 3389 2099 2265 1998 1037 2678 2054 2291 2069 2009 2005 4187 3768 4106 4780 2270 1996 24443 25709 1996 2023 2607 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   label: 0 (id = 0)\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   guid: dev-2\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   tokens: [CLS] so great if you want to overview the current situation of d printing technology and develop so business on d printing you have to watch this [SEP]\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   input_ids: 101 2061 2307 2065 2017 2215 2000 19184 1996 2783 3663 1997 1040 8021 2974 1998 4503 2061 2449 2006 1040 8021 2017 2031 2000 3422 2023 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   label: 2 (id = 2)\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   guid: dev-3\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   tokens: [CLS] kept way like avoids instructor simple it deliberately inform ##ative not sporadic yet the [SEP]\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   input_ids: 101 2921 2126 2066 26777 9450 3722 2009 9969 12367 8082 2025 24590 2664 1996 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   label: 0 (id = 0)\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   guid: dev-4\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   tokens: [CLS] the best audit ##ing course out there [SEP]\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   input_ids: 101 1996 2190 15727 2075 2607 2041 2045 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   label: 2 (id = 2)\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   guid: dev-5\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   tokens: [CLS] one of the best course i ve taken so far related to computer science fundamental if you need to master data structure and algorithm this is the course for you [SEP]\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   input_ids: 101 2028 1997 1996 2190 2607 1045 2310 2579 2061 2521 3141 2000 3274 2671 8050 2065 2017 2342 2000 3040 2951 3252 1998 9896 2023 2003 1996 2607 2005 2017 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:32:31 - INFO - run_classifier_dataset_utils -   label: 2 (id = 2)\n",
      "12/01/2019 22:32:32 - INFO - __main__ -     Saving eval features into cached file fold4/dev_bert-base-uncased_128_sst-2\n",
      "12/01/2019 22:32:32 - INFO - __main__ -   ***** Running evaluation *****\n",
      "12/01/2019 22:32:32 - INFO - __main__ -     Num examples = 653\n",
      "12/01/2019 22:32:32 - INFO - __main__ -     Batch size = 8\n",
      "Evaluating: 100% 82/82 [00:11<00:00,  7.33it/s]\n",
      "12/01/2019 22:32:43 - INFO - __main__ -   ***** Eval results *****\n",
      "12/01/2019 22:32:43 - INFO - __main__ -     acc = 0.9326186830015314\n",
      "12/01/2019 22:32:43 - INFO - __main__ -     classification report =               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92       197\n",
      "           1       0.96      0.93      0.94       234\n",
      "           2       0.97      0.90      0.93       222\n",
      "\n",
      "    accuracy                           0.93       653\n",
      "   macro avg       0.93      0.93      0.93       653\n",
      "weighted avg       0.94      0.93      0.93       653\n",
      "\n",
      "12/01/2019 22:32:43 - INFO - __main__ -     eval_loss = 0.4074446744308239\n",
      "12/01/2019 22:32:43 - INFO - __main__ -     global_step = 246\n",
      "12/01/2019 22:32:43 - INFO - __main__ -     loss = 0.1491195412912989\n"
     ]
    }
   ],
   "source": [
    "#fold4\n",
    "!python run_classifier_py.py --task_name SST-2 --do_train --do_eval --bert_model bert-base-uncased --do_lower_case --data_dir fold4 --max_seq_length 128 --learning_rate 1e-6 --num_train_epochs 3.0 --output_dir finetuned_fold4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GdcpVrKJmrq6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GC2E-dJsmwDB"
   },
   "source": [
    "Fold 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0ofQykTamwDv",
    "outputId": "a7c7e00c-f8cb-465f-dd4b-5f85ee7af622"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/01/2019 22:36:37 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "12/01/2019 22:36:38 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "12/01/2019 22:36:39 - INFO - pytorch_pretrained_bert.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_pretrained_bert/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "12/01/2019 22:36:39 - INFO - pytorch_pretrained_bert.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_pretrained_bert/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "12/01/2019 22:36:39 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "12/01/2019 22:36:43 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "12/01/2019 22:36:43 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   Writing example 0 of 2611\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   guid: train-1\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   tokens: [CLS] very helpful video that really take you into the real world of d printing the interview with the representative from various industry in this course helped me understand the importance of d printing additive manufacturing a a convenient tool and also where to look for related resource [SEP]\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   input_ids: 101 2200 14044 2678 2008 2428 2202 2017 2046 1996 2613 2088 1997 1040 8021 1996 4357 2007 1996 4387 2013 2536 3068 1999 2023 2607 3271 2033 3305 1996 5197 1997 1040 8021 29167 5814 1037 1037 14057 6994 1998 2036 2073 2000 2298 2005 3141 7692 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   label: 2 (id = 2)\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   guid: train-2\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   tokens: [CLS] import have introduce to able may grading to or key is le intuitive to general for feel when exercise in nu ##mp ##y the function assignment student the basic to course in another in save take like data your that designed submit ##ting of order each assignment to ha in panda code is in the can python be i in in student order are that score what applied not sack problem manipulation but this for downs ##ide function some every get the new panda the and example a effect is way change last [SEP]\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   input_ids: 101 12324 2031 8970 2000 2583 2089 26886 2000 2030 3145 2003 3393 29202 2000 2236 2005 2514 2043 6912 1999 16371 8737 2100 1996 3853 8775 3076 1996 3937 2000 2607 1999 2178 1999 3828 2202 2066 2951 2115 2008 2881 12040 3436 1997 2344 2169 8775 2000 5292 1999 25462 3642 2003 1999 1996 2064 18750 2022 1045 1999 1999 3076 2344 2024 2008 3556 2054 4162 2025 12803 3291 16924 2021 2023 2005 12482 5178 3853 2070 2296 2131 1996 2047 25462 1996 1998 2742 1037 3466 2003 2126 2689 2197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   label: 0 (id = 0)\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   guid: train-3\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   tokens: [CLS] this class wa inspiring i t wa worth the wait thank you [SEP]\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   input_ids: 101 2023 2465 11333 18988 1045 1056 11333 4276 1996 3524 4067 2017 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   label: 2 (id = 2)\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   guid: train-4\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   tokens: [CLS] the fact that the class us graph ##lab instead of panda nu ##mp ##y sk ##lea ##rn should have been stated up front ##the course felt like an advertisement for the professor s tool ##kit ##it wa very disappointing that the equivalent standard work ##flow wa not supported [SEP]\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   input_ids: 101 1996 2755 2008 1996 2465 2149 10629 20470 2612 1997 25462 16371 8737 2100 15315 19738 6826 2323 2031 2042 3090 2039 2392 10760 2607 2371 2066 2019 15147 2005 1996 2934 1055 6994 23615 4183 11333 2200 15640 2008 1996 5662 3115 2147 12314 11333 2025 3569 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   label: 1 (id = 1)\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   guid: train-5\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   tokens: [CLS] the content of the course is extremely useful however assignment need review a the exercise result have mistake and they are not explained very well missing step by step guidance [SEP]\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   input_ids: 101 1996 4180 1997 1996 2607 2003 5186 6179 2174 8775 2342 3319 1037 1996 6912 2765 2031 6707 1998 2027 2024 2025 4541 2200 2092 4394 3357 2011 3357 8606 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:36:47 - INFO - run_classifier_dataset_utils -   label: 1 (id = 1)\n",
      "12/01/2019 22:36:48 - INFO - __main__ -     Saving train features into cached file fold5/train_bert-base-uncased_128_sst-2\n",
      "12/01/2019 22:36:48 - INFO - __main__ -   ***** Running training *****\n",
      "12/01/2019 22:36:48 - INFO - __main__ -     Num examples = 2611\n",
      "12/01/2019 22:36:48 - INFO - __main__ -     Batch size = 32\n",
      "12/01/2019 22:36:48 - INFO - __main__ -     Num steps = 246\n",
      "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
      "Iteration:   0% 0/82 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1% 1/82 [00:01<02:11,  1.62s/it]\u001b[A\n",
      "Iteration:   2% 2/82 [00:03<02:08,  1.61s/it]\u001b[A\n",
      "Iteration:   4% 3/82 [00:04<02:06,  1.60s/it]\u001b[A\n",
      "Iteration:   5% 4/82 [00:06<02:04,  1.59s/it]\u001b[A\n",
      "Iteration:   6% 5/82 [00:07<02:02,  1.59s/it]\u001b[A\n",
      "Iteration:   7% 6/82 [00:09<02:00,  1.58s/it]\u001b[A\n",
      "Iteration:   9% 7/82 [00:11<01:58,  1.58s/it]\u001b[A\n",
      "Iteration:  10% 8/82 [00:12<01:56,  1.57s/it]\u001b[A\n",
      "Iteration:  11% 9/82 [00:14<01:55,  1.58s/it]\u001b[A\n",
      "Iteration:  12% 10/82 [00:15<01:53,  1.58s/it]\u001b[A\n",
      "Iteration:  13% 11/82 [00:17<01:51,  1.58s/it]\u001b[A\n",
      "Iteration:  15% 12/82 [00:18<01:50,  1.58s/it]\u001b[A\n",
      "Iteration:  16% 13/82 [00:20<01:48,  1.57s/it]\u001b[A\n",
      "Iteration:  17% 14/82 [00:22<01:47,  1.58s/it]\u001b[A\n",
      "Iteration:  18% 15/82 [00:23<01:46,  1.58s/it]\u001b[A\n",
      "Iteration:  20% 16/82 [00:25<01:44,  1.58s/it]\u001b[A\n",
      "Iteration:  21% 17/82 [00:26<01:43,  1.59s/it]\u001b[A\n",
      "Iteration:  22% 18/82 [00:28<01:41,  1.58s/it]\u001b[A\n",
      "Iteration:  23% 19/82 [00:30<01:39,  1.59s/it]\u001b[A\n",
      "Iteration:  24% 20/82 [00:31<01:38,  1.59s/it]\u001b[A\n",
      "Iteration:  26% 21/82 [00:33<01:36,  1.59s/it]\u001b[A\n",
      "Iteration:  27% 22/82 [00:34<01:35,  1.59s/it]\u001b[A\n",
      "Iteration:  28% 23/82 [00:36<01:33,  1.58s/it]\u001b[A\n",
      "Iteration:  29% 24/82 [00:37<01:31,  1.58s/it]\u001b[A\n",
      "Iteration:  30% 25/82 [00:39<01:30,  1.58s/it]\u001b[A\n",
      "Iteration:  32% 26/82 [00:41<01:29,  1.59s/it]\u001b[A\n",
      "Iteration:  33% 27/82 [00:42<01:27,  1.59s/it]\u001b[A\n",
      "Iteration:  34% 28/82 [00:44<01:25,  1.59s/it]\u001b[A\n",
      "Iteration:  35% 29/82 [00:45<01:24,  1.59s/it]\u001b[A\n",
      "Iteration:  37% 30/82 [00:47<01:22,  1.59s/it]\u001b[A\n",
      "Iteration:  38% 31/82 [00:49<01:20,  1.59s/it]\u001b[A\n",
      "Iteration:  39% 32/82 [00:50<01:19,  1.59s/it]\u001b[A\n",
      "Iteration:  40% 33/82 [00:52<01:17,  1.59s/it]\u001b[A\n",
      "Iteration:  41% 34/82 [00:53<01:16,  1.59s/it]\u001b[A\n",
      "Iteration:  43% 35/82 [00:55<01:14,  1.59s/it]\u001b[A\n",
      "Iteration:  44% 36/82 [00:57<01:12,  1.59s/it]\u001b[A\n",
      "Iteration:  45% 37/82 [00:58<01:11,  1.58s/it]\u001b[A\n",
      "Iteration:  46% 38/82 [01:00<01:09,  1.59s/it]\u001b[A\n",
      "Iteration:  48% 39/82 [01:01<01:08,  1.59s/it]\u001b[A\n",
      "Iteration:  49% 40/82 [01:03<01:06,  1.59s/it]\u001b[A\n",
      "Iteration:  50% 41/82 [01:04<01:05,  1.59s/it]\u001b[A\n",
      "Iteration:  51% 42/82 [01:06<01:03,  1.59s/it]\u001b[A\n",
      "Iteration:  52% 43/82 [01:08<01:02,  1.59s/it]\u001b[A\n",
      "Iteration:  54% 44/82 [01:09<01:00,  1.59s/it]\u001b[A\n",
      "Iteration:  55% 45/82 [01:11<00:58,  1.59s/it]\u001b[A\n",
      "Iteration:  56% 46/82 [01:12<00:56,  1.58s/it]\u001b[A\n",
      "Iteration:  57% 47/82 [01:14<00:55,  1.58s/it]\u001b[A\n",
      "Iteration:  59% 48/82 [01:16<00:53,  1.58s/it]\u001b[A\n",
      "Iteration:  60% 49/82 [01:17<00:52,  1.58s/it]\u001b[A\n",
      "Iteration:  61% 50/82 [01:19<00:50,  1.58s/it]\u001b[A\n",
      "Iteration:  62% 51/82 [01:20<00:49,  1.58s/it]\u001b[A\n",
      "Iteration:  63% 52/82 [01:22<00:47,  1.59s/it]\u001b[A\n",
      "Iteration:  65% 53/82 [01:23<00:45,  1.58s/it]\u001b[A\n",
      "Iteration:  66% 54/82 [01:25<00:44,  1.59s/it]\u001b[A\n",
      "Iteration:  67% 55/82 [01:27<00:42,  1.58s/it]\u001b[A\n",
      "Iteration:  68% 56/82 [01:28<00:41,  1.59s/it]\u001b[A\n",
      "Iteration:  70% 57/82 [01:30<00:39,  1.58s/it]\u001b[A\n",
      "Iteration:  71% 58/82 [01:31<00:37,  1.58s/it]\u001b[A\n",
      "Iteration:  72% 59/82 [01:33<00:36,  1.58s/it]\u001b[A\n",
      "Iteration:  73% 60/82 [01:35<00:34,  1.58s/it]\u001b[A\n",
      "Iteration:  74% 61/82 [01:36<00:33,  1.58s/it]\u001b[A\n",
      "Iteration:  76% 62/82 [01:38<00:31,  1.58s/it]\u001b[A\n",
      "Iteration:  77% 63/82 [01:39<00:29,  1.58s/it]\u001b[A\n",
      "Iteration:  78% 64/82 [01:41<00:28,  1.58s/it]\u001b[A\n",
      "Iteration:  79% 65/82 [01:42<00:26,  1.58s/it]\u001b[A\n",
      "Iteration:  80% 66/82 [01:44<00:25,  1.58s/it]\u001b[A\n",
      "Iteration:  82% 67/82 [01:46<00:23,  1.58s/it]\u001b[A\n",
      "Iteration:  83% 68/82 [01:47<00:22,  1.58s/it]\u001b[A\n",
      "Iteration:  84% 69/82 [01:49<00:20,  1.58s/it]\u001b[A\n",
      "Iteration:  85% 70/82 [01:50<00:18,  1.58s/it]\u001b[A\n",
      "Iteration:  87% 71/82 [01:52<00:17,  1.58s/it]\u001b[A\n",
      "Iteration:  88% 72/82 [01:53<00:15,  1.58s/it]\u001b[A\n",
      "Iteration:  89% 73/82 [01:55<00:14,  1.58s/it]\u001b[A\n",
      "Iteration:  90% 74/82 [01:57<00:12,  1.58s/it]\u001b[A\n",
      "Iteration:  91% 75/82 [01:58<00:11,  1.58s/it]\u001b[A\n",
      "Iteration:  93% 76/82 [02:00<00:09,  1.58s/it]\u001b[A\n",
      "Iteration:  94% 77/82 [02:01<00:07,  1.58s/it]\u001b[A\n",
      "Iteration:  95% 78/82 [02:03<00:06,  1.58s/it]\u001b[A\n",
      "Iteration:  96% 79/82 [02:05<00:04,  1.58s/it]\u001b[A\n",
      "Iteration:  98% 80/82 [02:06<00:03,  1.58s/it]\u001b[A\n",
      "Iteration:  99% 81/82 [02:08<00:01,  1.58s/it]\u001b[A\n",
      "Iteration: 100% 82/82 [02:09<00:00,  1.41s/it]\u001b[A\n",
      "Epoch:  33% 1/3 [02:09<04:18, 129.23s/it]\n",
      "Iteration:   0% 0/82 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1% 1/82 [00:01<02:08,  1.59s/it]\u001b[A\n",
      "Iteration:   2% 2/82 [00:03<02:06,  1.58s/it]\u001b[A\n",
      "Iteration:   4% 3/82 [00:04<02:05,  1.59s/it]\u001b[A\n",
      "Iteration:   5% 4/82 [00:06<02:03,  1.58s/it]\u001b[A\n",
      "Iteration:   6% 5/82 [00:07<02:01,  1.58s/it]\u001b[A\n",
      "Iteration:   7% 6/82 [00:09<02:00,  1.58s/it]\u001b[A\n",
      "Iteration:   9% 7/82 [00:11<01:58,  1.58s/it]\u001b[A\n",
      "Iteration:  10% 8/82 [00:12<01:56,  1.58s/it]\u001b[A\n",
      "Iteration:  11% 9/82 [00:14<01:55,  1.58s/it]\u001b[A\n",
      "Iteration:  12% 10/82 [00:15<01:53,  1.58s/it]\u001b[A\n",
      "Iteration:  13% 11/82 [00:17<01:51,  1.57s/it]\u001b[A\n",
      "Iteration:  15% 12/82 [00:18<01:50,  1.57s/it]\u001b[A\n",
      "Iteration:  16% 13/82 [00:20<01:48,  1.57s/it]\u001b[A\n",
      "Iteration:  17% 14/82 [00:22<01:47,  1.57s/it]\u001b[A\n",
      "Iteration:  18% 15/82 [00:23<01:45,  1.57s/it]\u001b[A\n",
      "Iteration:  20% 16/82 [00:25<01:44,  1.58s/it]\u001b[A\n",
      "Iteration:  21% 17/82 [00:26<01:42,  1.57s/it]\u001b[A\n",
      "Iteration:  22% 18/82 [00:28<01:40,  1.58s/it]\u001b[A\n",
      "Iteration:  23% 19/82 [00:29<01:39,  1.58s/it]\u001b[A\n",
      "Iteration:  24% 20/82 [00:31<01:37,  1.58s/it]\u001b[A\n",
      "Iteration:  26% 21/82 [00:33<01:36,  1.58s/it]\u001b[A\n",
      "Iteration:  27% 22/82 [00:34<01:34,  1.58s/it]\u001b[A\n",
      "Iteration:  28% 23/82 [00:36<01:33,  1.58s/it]\u001b[A\n",
      "Iteration:  29% 24/82 [00:37<01:31,  1.58s/it]\u001b[A\n",
      "Iteration:  30% 25/82 [00:39<01:29,  1.58s/it]\u001b[A\n",
      "Iteration:  32% 26/82 [00:40<01:28,  1.57s/it]\u001b[A\n",
      "Iteration:  33% 27/82 [00:42<01:26,  1.58s/it]\u001b[A\n",
      "Iteration:  34% 28/82 [00:44<01:25,  1.58s/it]\u001b[A\n",
      "Iteration:  35% 29/82 [00:45<01:23,  1.58s/it]\u001b[A\n",
      "Iteration:  37% 30/82 [00:47<01:21,  1.57s/it]\u001b[A\n",
      "Iteration:  38% 31/82 [00:48<01:20,  1.58s/it]\u001b[A\n",
      "Iteration:  39% 32/82 [00:50<01:18,  1.58s/it]\u001b[A\n",
      "Iteration:  40% 33/82 [00:52<01:17,  1.58s/it]\u001b[A\n",
      "Iteration:  41% 34/82 [00:53<01:16,  1.59s/it]\u001b[A\n",
      "Iteration:  43% 35/82 [00:55<01:14,  1.58s/it]\u001b[A\n",
      "Iteration:  44% 36/82 [00:56<01:12,  1.58s/it]\u001b[A\n",
      "Iteration:  45% 37/82 [00:58<01:11,  1.58s/it]\u001b[A\n",
      "Iteration:  46% 38/82 [00:59<01:09,  1.59s/it]\u001b[A\n",
      "Iteration:  48% 39/82 [01:01<01:07,  1.58s/it]\u001b[A\n",
      "Iteration:  49% 40/82 [01:03<01:06,  1.58s/it]\u001b[A\n",
      "Iteration:  50% 41/82 [01:04<01:04,  1.58s/it]\u001b[A\n",
      "Iteration:  51% 42/82 [01:06<01:03,  1.58s/it]\u001b[A\n",
      "Iteration:  52% 43/82 [01:07<01:01,  1.58s/it]\u001b[A\n",
      "Iteration:  54% 44/82 [01:09<00:59,  1.58s/it]\u001b[A\n",
      "Iteration:  55% 45/82 [01:11<00:58,  1.57s/it]\u001b[A\n",
      "Iteration:  56% 46/82 [01:12<00:56,  1.57s/it]\u001b[A\n",
      "Iteration:  57% 47/82 [01:14<00:55,  1.58s/it]\u001b[A\n",
      "Iteration:  59% 48/82 [01:15<00:53,  1.58s/it]\u001b[A\n",
      "Iteration:  60% 49/82 [01:17<00:52,  1.58s/it]\u001b[A\n",
      "Iteration:  61% 50/82 [01:18<00:50,  1.58s/it]\u001b[A\n",
      "Iteration:  62% 51/82 [01:20<00:48,  1.58s/it]\u001b[A\n",
      "Iteration:  63% 52/82 [01:22<00:47,  1.57s/it]\u001b[A\n",
      "Iteration:  65% 53/82 [01:23<00:45,  1.58s/it]\u001b[A\n",
      "Iteration:  66% 54/82 [01:25<00:44,  1.58s/it]\u001b[A\n",
      "Iteration:  67% 55/82 [01:26<00:42,  1.58s/it]\u001b[A\n",
      "Iteration:  68% 56/82 [01:28<00:41,  1.58s/it]\u001b[A\n",
      "Iteration:  70% 57/82 [01:29<00:39,  1.58s/it]\u001b[A\n",
      "Iteration:  71% 58/82 [01:31<00:37,  1.58s/it]\u001b[A\n",
      "Iteration:  72% 59/82 [01:33<00:36,  1.58s/it]\u001b[A\n",
      "Iteration:  73% 60/82 [01:34<00:34,  1.57s/it]\u001b[A\n",
      "Iteration:  74% 61/82 [01:36<00:33,  1.58s/it]\u001b[A\n",
      "Iteration:  76% 62/82 [01:37<00:31,  1.58s/it]\u001b[A\n",
      "Iteration:  77% 63/82 [01:39<00:30,  1.58s/it]\u001b[A\n",
      "Iteration:  78% 64/82 [01:41<00:28,  1.58s/it]\u001b[A\n",
      "Iteration:  79% 65/82 [01:42<00:26,  1.58s/it]\u001b[A\n",
      "Iteration:  80% 66/82 [01:44<00:25,  1.58s/it]\u001b[A\n",
      "Iteration:  82% 67/82 [01:45<00:23,  1.58s/it]\u001b[A\n",
      "Iteration:  83% 68/82 [01:47<00:22,  1.58s/it]\u001b[A\n",
      "Iteration:  84% 69/82 [01:48<00:20,  1.58s/it]\u001b[A\n",
      "Iteration:  85% 70/82 [01:50<00:18,  1.58s/it]\u001b[A\n",
      "Iteration:  87% 71/82 [01:52<00:17,  1.58s/it]\u001b[A\n",
      "Iteration:  88% 72/82 [01:53<00:15,  1.58s/it]\u001b[A\n",
      "Iteration:  89% 73/82 [01:55<00:14,  1.58s/it]\u001b[A\n",
      "Iteration:  90% 74/82 [01:56<00:12,  1.58s/it]\u001b[A\n",
      "Iteration:  91% 75/82 [01:58<00:11,  1.58s/it]\u001b[A\n",
      "Iteration:  93% 76/82 [01:59<00:09,  1.58s/it]\u001b[A\n",
      "Iteration:  94% 77/82 [02:01<00:07,  1.57s/it]\u001b[A\n",
      "Iteration:  95% 78/82 [02:03<00:06,  1.57s/it]\u001b[A\n",
      "Iteration:  96% 79/82 [02:04<00:04,  1.58s/it]\u001b[A\n",
      "Iteration:  98% 80/82 [02:06<00:03,  1.58s/it]\u001b[A\n",
      "Iteration:  99% 81/82 [02:07<00:01,  1.57s/it]\u001b[A\n",
      "Iteration: 100% 82/82 [02:08<00:00,  1.41s/it]\u001b[A\n",
      "Epoch:  67% 2/3 [04:18<02:09, 129.11s/it]\n",
      "Iteration:   0% 0/82 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1% 1/82 [00:01<02:07,  1.57s/it]\u001b[A\n",
      "Iteration:   2% 2/82 [00:03<02:05,  1.57s/it]\u001b[A\n",
      "Iteration:   4% 3/82 [00:04<02:04,  1.58s/it]\u001b[A\n",
      "Iteration:   5% 4/82 [00:06<02:03,  1.58s/it]\u001b[A\n",
      "Iteration:   6% 5/82 [00:07<02:01,  1.57s/it]\u001b[A\n",
      "Iteration:   7% 6/82 [00:09<01:59,  1.57s/it]\u001b[A\n",
      "Iteration:   9% 7/82 [00:11<01:57,  1.57s/it]\u001b[A\n",
      "Iteration:  10% 8/82 [00:12<01:56,  1.57s/it]\u001b[A\n",
      "Iteration:  11% 9/82 [00:14<01:54,  1.57s/it]\u001b[A\n",
      "Iteration:  12% 10/82 [00:15<01:53,  1.58s/it]\u001b[A\n",
      "Iteration:  13% 11/82 [00:17<01:52,  1.58s/it]\u001b[A\n",
      "Iteration:  15% 12/82 [00:18<01:50,  1.58s/it]\u001b[A\n",
      "Iteration:  16% 13/82 [00:20<01:48,  1.58s/it]\u001b[A\n",
      "Iteration:  17% 14/82 [00:22<01:47,  1.58s/it]\u001b[A\n",
      "Iteration:  18% 15/82 [00:23<01:45,  1.58s/it]\u001b[A\n",
      "Iteration:  20% 16/82 [00:25<01:44,  1.58s/it]\u001b[A\n",
      "Iteration:  21% 17/82 [00:26<01:42,  1.58s/it]\u001b[A\n",
      "Iteration:  22% 18/82 [00:28<01:40,  1.58s/it]\u001b[A\n",
      "Iteration:  23% 19/82 [00:29<01:39,  1.58s/it]\u001b[A\n",
      "Iteration:  24% 20/82 [00:31<01:37,  1.58s/it]\u001b[A\n",
      "Iteration:  26% 21/82 [00:33<01:36,  1.58s/it]\u001b[A\n",
      "Iteration:  27% 22/82 [00:34<01:35,  1.59s/it]\u001b[A\n",
      "Iteration:  28% 23/82 [00:36<01:33,  1.58s/it]\u001b[A\n",
      "Iteration:  29% 24/82 [00:37<01:31,  1.58s/it]\u001b[A\n",
      "Iteration:  30% 25/82 [00:39<01:30,  1.58s/it]\u001b[A\n",
      "Iteration:  32% 26/82 [00:41<01:28,  1.58s/it]\u001b[A\n",
      "Iteration:  33% 27/82 [00:42<01:26,  1.58s/it]\u001b[A\n",
      "Iteration:  34% 28/82 [00:44<01:25,  1.58s/it]\u001b[A\n",
      "Iteration:  35% 29/82 [00:45<01:23,  1.57s/it]\u001b[A\n",
      "Iteration:  37% 30/82 [00:47<01:22,  1.58s/it]\u001b[A\n",
      "Iteration:  38% 31/82 [00:48<01:20,  1.58s/it]\u001b[A\n",
      "Iteration:  39% 32/82 [00:50<01:19,  1.58s/it]\u001b[A\n",
      "Iteration:  40% 33/82 [00:52<01:17,  1.58s/it]\u001b[A\n",
      "Iteration:  41% 34/82 [00:53<01:15,  1.58s/it]\u001b[A\n",
      "Iteration:  43% 35/82 [00:55<01:13,  1.57s/it]\u001b[A\n",
      "Iteration:  44% 36/82 [00:56<01:12,  1.57s/it]\u001b[A\n",
      "Iteration:  45% 37/82 [00:58<01:10,  1.57s/it]\u001b[A\n",
      "Iteration:  46% 38/82 [00:59<01:09,  1.58s/it]\u001b[A\n",
      "Iteration:  48% 39/82 [01:01<01:07,  1.58s/it]\u001b[A\n",
      "Iteration:  49% 40/82 [01:03<01:06,  1.58s/it]\u001b[A\n",
      "Iteration:  50% 41/82 [01:04<01:04,  1.58s/it]\u001b[A\n",
      "Iteration:  51% 42/82 [01:06<01:03,  1.58s/it]\u001b[A\n",
      "Iteration:  52% 43/82 [01:07<01:01,  1.58s/it]\u001b[A\n",
      "Iteration:  54% 44/82 [01:09<01:00,  1.58s/it]\u001b[A\n",
      "Iteration:  55% 45/82 [01:10<00:58,  1.58s/it]\u001b[A\n",
      "Iteration:  56% 46/82 [01:12<00:56,  1.58s/it]\u001b[A\n",
      "Iteration:  57% 47/82 [01:14<00:55,  1.57s/it]\u001b[A\n",
      "Iteration:  59% 48/82 [01:15<00:53,  1.58s/it]\u001b[A\n",
      "Iteration:  60% 49/82 [01:17<00:51,  1.57s/it]\u001b[A\n",
      "Iteration:  61% 50/82 [01:18<00:50,  1.57s/it]\u001b[A\n",
      "Iteration:  62% 51/82 [01:20<00:49,  1.58s/it]\u001b[A\n",
      "Iteration:  63% 52/82 [01:22<00:47,  1.58s/it]\u001b[A\n",
      "Iteration:  65% 53/82 [01:23<00:45,  1.58s/it]\u001b[A\n",
      "Iteration:  66% 54/82 [01:25<00:44,  1.58s/it]\u001b[A\n",
      "Iteration:  67% 55/82 [01:26<00:42,  1.58s/it]\u001b[A\n",
      "Iteration:  68% 56/82 [01:28<00:40,  1.58s/it]\u001b[A\n",
      "Iteration:  70% 57/82 [01:29<00:39,  1.57s/it]\u001b[A\n",
      "Iteration:  71% 58/82 [01:31<00:37,  1.58s/it]\u001b[A\n",
      "Iteration:  72% 59/82 [01:33<00:36,  1.58s/it]\u001b[A\n",
      "Iteration:  73% 60/82 [01:34<00:34,  1.57s/it]\u001b[A\n",
      "Iteration:  74% 61/82 [01:36<00:33,  1.58s/it]\u001b[A\n",
      "Iteration:  76% 62/82 [01:37<00:31,  1.58s/it]\u001b[A\n",
      "Iteration:  77% 63/82 [01:39<00:29,  1.58s/it]\u001b[A\n",
      "Iteration:  78% 64/82 [01:40<00:28,  1.58s/it]\u001b[A\n",
      "Iteration:  79% 65/82 [01:42<00:26,  1.58s/it]\u001b[A\n",
      "Iteration:  80% 66/82 [01:44<00:25,  1.57s/it]\u001b[A\n",
      "Iteration:  82% 67/82 [01:45<00:23,  1.57s/it]\u001b[A\n",
      "Iteration:  83% 68/82 [01:47<00:22,  1.58s/it]\u001b[A\n",
      "Iteration:  84% 69/82 [01:48<00:20,  1.58s/it]\u001b[A\n",
      "Iteration:  85% 70/82 [01:50<00:18,  1.58s/it]\u001b[A\n",
      "Iteration:  87% 71/82 [01:51<00:17,  1.58s/it]\u001b[A\n",
      "Iteration:  88% 72/82 [01:53<00:15,  1.57s/it]\u001b[A\n",
      "Iteration:  89% 73/82 [01:55<00:14,  1.58s/it]\u001b[A\n",
      "Iteration:  90% 74/82 [01:56<00:12,  1.58s/it]\u001b[A\n",
      "Iteration:  91% 75/82 [01:58<00:11,  1.58s/it]\u001b[A\n",
      "Iteration:  93% 76/82 [01:59<00:09,  1.58s/it]\u001b[A\n",
      "Iteration:  94% 77/82 [02:01<00:07,  1.58s/it]\u001b[A\n",
      "Iteration:  95% 78/82 [02:03<00:06,  1.58s/it]\u001b[A\n",
      "Iteration:  96% 79/82 [02:04<00:04,  1.58s/it]\u001b[A\n",
      "Iteration:  98% 80/82 [02:06<00:03,  1.58s/it]\u001b[A\n",
      "Iteration:  99% 81/82 [02:07<00:01,  1.58s/it]\u001b[A\n",
      "Iteration: 100% 82/82 [02:08<00:00,  1.42s/it]\u001b[A\n",
      "Epoch: 100% 3/3 [06:26<00:00, 129.02s/it]\n",
      "12/01/2019 22:43:18 - INFO - pytorch_pretrained_bert.modeling -   loading weights file finetuned_fold5/pytorch_model.bin\n",
      "12/01/2019 22:43:18 - INFO - pytorch_pretrained_bert.modeling -   loading configuration file finetuned_fold5/config.json\n",
      "12/01/2019 22:43:18 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "12/01/2019 22:43:21 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file finetuned_fold5/vocab.txt\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   Writing example 0 of 653\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   guid: dev-1\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   tokens: [CLS] very useful course [SEP]\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   input_ids: 101 2200 6179 2607 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   label: 2 (id = 2)\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   guid: dev-2\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   tokens: [CLS] there s a lot of great information in this course and it s presented in a very good way however like the excel course in this specialization most of the material isn t about using table ##au week is about running a project week are about using table ##au and the final week are about presentation if you re interest isn t mainly in table ##au i would recommend the course however if you re taking this course to learn table ##au i would suggest not paying for it [SEP]\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   input_ids: 101 2045 1055 1037 2843 1997 2307 2592 1999 2023 2607 1998 2009 1055 3591 1999 1037 2200 2204 2126 2174 2066 1996 24970 2607 1999 2023 28031 2087 1997 1996 3430 3475 1056 2055 2478 2795 4887 2733 2003 2055 2770 1037 2622 2733 2024 2055 2478 2795 4887 1998 1996 2345 2733 2024 2055 8312 2065 2017 2128 3037 3475 1056 3701 1999 2795 4887 1045 2052 16755 1996 2607 2174 2065 2017 2128 2635 2023 2607 2000 4553 2795 4887 1045 2052 6592 2025 7079 2005 2009 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   label: 1 (id = 1)\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   guid: dev-3\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   tokens: [CLS] essential in knowing how your company make decision based on numerical data [SEP]\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   input_ids: 101 6827 1999 4209 2129 2115 2194 2191 3247 2241 2006 15973 2951 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   label: 2 (id = 2)\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   guid: dev-4\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   tokens: [CLS] introduction video each week and ton of link for reading do not want the worst course i ever took not enough video material with good explanation all we got link to article which we can found ourselves without any help [SEP]\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   input_ids: 101 4955 2678 2169 2733 1998 10228 1997 4957 2005 3752 2079 2025 2215 1996 5409 2607 1045 2412 2165 2025 2438 2678 3430 2007 2204 7526 2035 2057 2288 4957 2000 3720 2029 2057 2064 2179 9731 2302 2151 2393 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   label: 1 (id = 1)\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   guid: dev-5\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   tokens: [CLS] it however program wa very advertisement much pm ##bo ##k it like structured supported coherent prepared being an not super ##fl ##uous thought content by feeling in well in the the course support through this way the that a for and is felt [SEP]\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   input_ids: 101 2009 2174 2565 11333 2200 15147 2172 7610 5092 2243 2009 2066 14336 3569 18920 4810 2108 2019 2025 3565 10258 8918 2245 4180 2011 3110 1999 2092 1999 1996 1996 2607 2490 2083 2023 2126 1996 2008 1037 2005 1998 2003 2371 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 22:43:22 - INFO - run_classifier_dataset_utils -   label: 0 (id = 0)\n",
      "12/01/2019 22:43:23 - INFO - __main__ -     Saving eval features into cached file fold5/dev_bert-base-uncased_128_sst-2\n",
      "12/01/2019 22:43:23 - INFO - __main__ -   ***** Running evaluation *****\n",
      "12/01/2019 22:43:23 - INFO - __main__ -     Num examples = 653\n",
      "12/01/2019 22:43:23 - INFO - __main__ -     Batch size = 8\n",
      "Evaluating: 100% 82/82 [00:11<00:00,  7.37it/s]\n",
      "12/01/2019 22:43:34 - INFO - __main__ -   ***** Eval results *****\n",
      "12/01/2019 22:43:34 - INFO - __main__ -     acc = 0.9234303215926493\n",
      "12/01/2019 22:43:34 - INFO - __main__ -     classification report =               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92       204\n",
      "           1       0.91      0.92      0.92       209\n",
      "           2       0.94      0.92      0.93       240\n",
      "\n",
      "    accuracy                           0.92       653\n",
      "   macro avg       0.92      0.92      0.92       653\n",
      "weighted avg       0.92      0.92      0.92       653\n",
      "\n",
      "12/01/2019 22:43:34 - INFO - __main__ -     eval_loss = 0.43444185540443514\n",
      "12/01/2019 22:43:34 - INFO - __main__ -     global_step = 246\n",
      "12/01/2019 22:43:34 - INFO - __main__ -     loss = 0.15557149900653497\n"
     ]
    }
   ],
   "source": [
    "#fold5\n",
    "!python run_classifier_py.py --task_name SST-2 --do_train --do_eval --bert_model bert-base-uncased --do_lower_case --data_dir fold5 --max_seq_length 128 --learning_rate 1e-6 --num_train_epochs 3.0 --output_dir finetuned_fold5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sSpSv6oOsKp-"
   },
   "source": [
    "Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZrSrIxtSS7sR",
    "outputId": "0627e08a-b243-4b59-997f-5f2ef0637eb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/01/2019 23:07:32 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "12/01/2019 23:07:32 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file finetuned_fold5/vocab.txt\n",
      "12/01/2019 23:07:32 - INFO - pytorch_pretrained_bert.modeling -   loading weights file finetuned_fold5/pytorch_model.bin\n",
      "12/01/2019 23:07:32 - INFO - pytorch_pretrained_bert.modeling -   loading configuration file finetuned_fold5/config.json\n",
      "12/01/2019 23:07:32 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "12/01/2019 23:07:39 - INFO - pytorch_pretrained_bert.modeling -   loading weights file finetuned_fold5/pytorch_model.bin\n",
      "12/01/2019 23:07:39 - INFO - pytorch_pretrained_bert.modeling -   loading configuration file finetuned_fold5/config.json\n",
      "12/01/2019 23:07:39 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   Writing example 0 of 816\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   guid: dev-1\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   tokens: [CLS] worse not cursed to were incomplete work some impossible to have and at part forum describe video the n t the upon on s and of wa the to the thing a which given point exercise those assignment of are done instruction a the this n t it me re ##media ##l in you s for n t these the get instruction i still error version extra s assignment all code gave build material to course be ago issue of of begin in the project exercise hour just how using even written the even much of there the it some file new too get the these that ca flag ##ged code also course updated it so in are file month working are wo match complete work [SEP]\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   input_ids: 101 4788 2025 10678 2000 2020 12958 2147 2070 5263 2000 2031 1998 2012 2112 7057 6235 2678 1996 1050 1056 1996 2588 2006 1055 1998 1997 11333 1996 2000 1996 2518 1037 2029 2445 2391 6912 2216 8775 1997 2024 2589 7899 1037 1996 2023 1050 1056 2009 2033 2128 16969 2140 1999 2017 1055 2005 1050 1056 2122 1996 2131 7899 1045 2145 7561 2544 4469 1055 8775 2035 3642 2435 3857 3430 2000 2607 2022 3283 3277 1997 1997 4088 1999 1996 2622 6912 3178 2074 2129 2478 2130 2517 1996 2130 2172 1997 2045 1996 2009 2070 5371 2047 2205 2131 1996 2122 2008 6187 5210 5999 3642 2036 2607 7172 2009 2061 1999 2024 5371 3204 2551 2024 24185 2674 3143 2147 102\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   label: 0 (id = 0)\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   guid: dev-2\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   tokens: [CLS] interesting course on agriculture and economics very useful to rev ##ise important economic concept and also to learn more about agriculture highly recommended [SEP]\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   input_ids: 101 5875 2607 2006 5237 1998 5543 2200 6179 2000 7065 5562 2590 3171 4145 1998 2036 2000 4553 2062 2055 5237 3811 6749 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   label: 2 (id = 2)\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   guid: dev-3\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   tokens: [CLS] overall the video related to the quiz are to vague and un detailed i more often than not feel lost also this should not be part of the interaction design special ##isation and just be a small part of another course not once have i needed r or stat ##istic in my year design career [SEP]\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   input_ids: 101 3452 1996 2678 3141 2000 1996 19461 2024 2000 13727 1998 4895 6851 1045 2062 2411 2084 2025 2514 2439 2036 2023 2323 2025 2022 2112 1997 1996 8290 2640 2569 6648 1998 2074 2022 1037 2235 2112 1997 2178 2607 2025 2320 2031 1045 2734 1054 2030 28093 6553 1999 2026 2095 2640 2476 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   label: 1 (id = 1)\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   guid: dev-4\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   tokens: [CLS] tutor is russian i guess her accent is not quite understand ##able [SEP]\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   input_ids: 101 14924 2003 2845 1045 3984 2014 9669 2003 2025 3243 3305 3085 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   label: 1 (id = 1)\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   *** Example ***\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   guid: dev-5\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   tokens: [CLS] but he are outline clear a not warned his ha definition [SEP]\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   input_ids: 101 2021 2002 2024 12685 3154 1037 2025 7420 2010 5292 6210 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/01/2019 23:07:44 - INFO - run_classifier_dataset_utils -   label: 0 (id = 0)\n",
      "12/01/2019 23:07:44 - INFO - __main__ -     Saving eval features into cached file Lasttry_cv_test_data/dev_finetuned_fold5_128_sst-2\n",
      "12/01/2019 23:07:44 - INFO - __main__ -   ***** Running evaluation *****\n",
      "12/01/2019 23:07:44 - INFO - __main__ -     Num examples = 816\n",
      "12/01/2019 23:07:44 - INFO - __main__ -     Batch size = 8\n",
      "Evaluating: 100% 102/102 [00:13<00:00,  7.36it/s]\n",
      "12/01/2019 23:07:58 - INFO - __main__ -   ***** Eval results *****\n",
      "12/01/2019 23:07:58 - INFO - __main__ -     acc = 0.9338235294117647\n",
      "12/01/2019 23:07:58 - INFO - __main__ -     classification report =               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.95       280\n",
      "           1       0.92      0.93      0.93       273\n",
      "           2       0.95      0.91      0.93       263\n",
      "\n",
      "    accuracy                           0.93       816\n",
      "   macro avg       0.93      0.93      0.93       816\n",
      "weighted avg       0.93      0.93      0.93       816\n",
      "\n",
      "12/01/2019 23:07:58 - INFO - __main__ -     eval_loss = 0.4192386065043655\n",
      "12/01/2019 23:07:58 - INFO - __main__ -     global_step = 0\n",
      "12/01/2019 23:07:58 - INFO - __main__ -     loss = None\n"
     ]
    }
   ],
   "source": [
    "#with full data. \n",
    "!python run_classifier_py.py --task_name SST-2 --do_eval --bert_model finetuned_fold5 --do_lower_case --data_dir Lasttry_cv_test_data --max_seq_length 128 --output_dir finetuned_lasttry_cv_test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a4VcL9uNVyh0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NDcdPYYHs-bi",
    "outputId": "305c6aa8-2099-4825-d1c1-0946d412f771"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.86"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average accuracy on 5-fold CV\n",
    "import numpy as np\n",
    "round(np.average((0.9234303215926493, 0.9326186830015314, 0.9249617151607963, 0.9402756508422665, 0.9218989280245024))*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iA964PRJWD3q"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of BERT_Pytorch_DN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
